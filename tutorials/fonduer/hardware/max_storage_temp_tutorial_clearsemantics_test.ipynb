{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Extracting Maximum Storage Temperatures for Transistors from PDF Datasheets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will walk through the process of using `Fonduer` to identify mentions of transistory polarity (i.e. `NPN` or `PNP`) in a corpus of transistor datasheets from Digikey.com. The tutorial is broken into several parts, each covering a stage in the pipeline:\n",
    "\n",
    "1. Initialiation\n",
    "2. Candidate Extraction\n",
    "3. Annotating Evaluation Data\n",
    "4. Featurization and Training\n",
    "5. Evaluation\n",
    "\n",
    "## Part 1: Initialization\n",
    "\n",
    "We first preprocess our input documents using `Fonduer` utilities, which transform each richly formatted document into the `Fonduer` unified data model, which captures the variability and multimodality of richly formatted data. We also extract standard linguistic features from each context which will be used later in featurization using [CoreNLP](http://stanfordnlp.github.io/CoreNLP/).\n",
    "\n",
    "This preprocessed data is saved to a database. Connection strings can be specified by setting the `SNORKELDB` environment variable. If no database is specified, then SQLite at `./snorkel.db` is created by default.\n",
    "\n",
    "We initialize several variables for convenience that define what the database should be called, what level of parallelization the `Fonduer` pipeline can run with, and the number of documents in our test and training sets. In the code below, we use PostgreSQL as our database backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sen/anaconda2/lib/python2.7/site-packages/matplotlib/__init__.py:1085: UserWarning: Duplicate key in file \"/Users/sen/.matplotlib/matplotlibrc\", line #467\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "PARALLEL = 4 # assuming a quad-core machine\n",
    "ATTRIBUTE = \"stg_temp_max\"\n",
    "\n",
    "os.environ['SNORKELDBNAME'] = ATTRIBUTE\n",
    "os.environ['SNORKELDB'] = 'postgres://localhost:5432/' + os.environ['SNORKELDBNAME']\n",
    "        \n",
    "sys.path.append(os.environ['SNORKELHOME'] + '/tutorials/fonduer/hardware/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing a `SnorkelSession`\n",
    "\n",
    "We first initialize a `SnorkelSession`, which manages the connection to the database automatically, and enables us to save intermediate results. This code also reinitializes each of the databases to ensure that we are starting fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "256\n",
      "SNORKELDB = postgres://localhost:5432/stg_temp_max\n",
      "SNORKELDBNAME = stg_temp_max\n"
     ]
    }
   ],
   "source": [
    "print os.system(\"dropdb \" + os.environ['SNORKELDBNAME'])\n",
    "print os.system(\"createdb \" + os.environ['SNORKELDBNAME'])\n",
    "print \"SNORKELDB = %s\" % os.environ['SNORKELDB']\n",
    "print \"SNORKELDBNAME = %s\" % os.environ['SNORKELDBNAME']\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Parsing and Loading the Corpus\n",
    "\n",
    "Next, we preprocess and load the corpus of datasheets. Each datasheet has a PDF and HTML representation. Both representations are used in conjunction to create a robust unified data model with textual, structural, tabular, and visual modality information. Note that since each document is independent of each other, we can parse the documents in parallel. **Note that parallel execution will not work with SQLite, the default database engine**. We depend on PostgreSQL for this functionality.\n",
    "\n",
    "#### Configuring an `HTMLPreprocessor`\n",
    "We start by setting the paths to where our documents are stored, and defining a `HTMLPreprocessor` to read in the documents found in the specified paths. `max_docs` specified the number of documents to parse. For the sake of this tutorial, we only look at 50 documents.\n",
    "\n",
    "*Note that you need to have run `download_data.sh` before executing these next steps or you won't have the documents needed for the tutorial.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.parser import HTMLPreprocessor, OmniParser\n",
    "\n",
    "docs_path = os.environ['SNORKELHOME'] + '/tutorials/fonduer/hardware/data/html/'\n",
    "pdf_path = os.environ['SNORKELHOME'] + '/tutorials/fonduer/hardware/data/pdf/'\n",
    "\n",
    "doc_preprocessor = HTMLPreprocessor(docs_path, max_docs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuring an `OmniParser`\n",
    "Next, we configure an `OmniParser`, which serves as our `CorpusParser` for PDF documents. We use NLP preprocessing tools to split our documents into phrases, tokens, and provide annotations such as part-of-speech tags and dependency parse structures for these phrases. In addition, we can specify which modality information to include in the unified data model for each document. Below, we enable all modality information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 5.06 s, sys: 202 ms, total: 5.26 s\n",
      "Wall time: 1min 41s\n"
     ]
    }
   ],
   "source": [
    "corpus_parser = OmniParser(structural=True, lingual=True, visual=True, pdf_path=pdf_path)\n",
    "%time corpus_parser.apply(doc_preprocessor, parallelism=PARALLEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use simple database queries (written in the syntax of [SQLAlchemy](http://www.sqlalchemy.org/), which `Fonduer` uses) to check how many documents and sentences were parsed, or even check how many phrases and tables are contained in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 50\n",
      "Phrases: 20189\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import Document, Phrase\n",
    "\n",
    "print \"Documents:\", session.query(Document).count()\n",
    "print \"Phrases:\", session.query(Phrase).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Candidate Extraction\n",
    "\n",
    "The next step is to extract **candidates** from our corpus. A `candidate` is the objects for which we want to make predictions. In this case, the candidates are pairs of transistor part numbers and their corresponding polarities as found in their datasheets. Our task is to predict which pairs are true in the associated document.\n",
    "\n",
    "### Defining a `Candidate` schema\n",
    "We first define the **schema** of the relation we want to extract. This must be a subclass of `Candidate`, and we define it using a helper function. Here, we define a binary relation which connects two `Span` objects of text. This function also created the database table if it does not already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "Part_Attr = candidate_subclass('Part_Attr', ['part','attr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing a `CandidateExtractor`\n",
    "\n",
    "Next, we write **matchers** to define which spans of text in the corpus are instances of each entity. In our case, we need to write a matcher that defines a transistor part number and a matcher to define a valid polarity value.\n",
    "\n",
    "Matchers can leverage a variety of information from regular expressions, to dictionaries, to user-defined functions. Furthermore, different techniques can be combined to form higher quality matchers. In general, matchers should seek to be as precise as possible while maintaining complete recall.\n",
    "\n",
    "#### Writing an advanced transistor part matcher\n",
    "\n",
    "Here, we show how transistor part numbers can leverage [naming conventions](https://en.wikipedia.org/wiki/Transistor#Part_numbering_standards.2Fspecifications) as regular expressions, and use a dictionary of known part numbers, and use user-defined functions together. First, we create a regular expression matcher for standard transistor naming conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.matchers import *\n",
    "\n",
    "### Transistor Naming Conventions as Regular Expressions ###\n",
    "eeca_rgx = '([ABC][A-Z][WXYZ]?[0-9]{3,5}(?:[A-Z]){0,5}[0-9]?[A-Z]?(?:-[A-Z0-9]{1,7})?(?:[-][A-Z0-9]{1,2})?(?:\\/DG)?)'\n",
    "jedec_rgx = '(2N\\d{3,4}[A-Z]{0,5}[0-9]?[A-Z]?)'\n",
    "jis_rgx = '(2S[ABCDEFGHJKMQRSTVZ]{1}[\\d]{2,4})'\n",
    "others_rgx = '((?:NSVBC|SMBT|MJ|MJE|MPS|MRF|RCA|TIP|ZTX|ZT|ZXT|TIS|TIPL|DTC|MMBT|SMMBT|PZT|FZT|STD|BUV|PBSS|KSC|CXT|FCX|CMPT){1}[\\d]{2,4}[A-Z]{0,5}(?:-[A-Z0-9]{0,6})?(?:[-][A-Z0-9]{0,1})?)'\n",
    "\n",
    "part_rgx = '|'.join([eeca_rgx, jedec_rgx, jis_rgx, others_rgx])\n",
    "part_rgx_matcher = RegexMatchSpan(rgx=part_rgx, longest_match_only=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can create a matcher from a dictionary of known part numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def get_digikey_parts_set(path):\n",
    "    \"\"\"\n",
    "    Reads in the digikey part dictionary and yeilds each part.\n",
    "    \"\"\"\n",
    "    all_parts = set()\n",
    "    with open(path, \"r\") as csvinput:\n",
    "        reader = csv.reader(csvinput)\n",
    "        for line in reader:\n",
    "            (part, url) = line\n",
    "            all_parts.add(part)\n",
    "    return all_parts\n",
    "            \n",
    "### Dictionary of known transistor parts ###\n",
    "dict_path = os.environ['SNORKELHOME'] + '/tutorials/fonduer/hardware/data/digikey_part_dictionary.csv'\n",
    "part_dict_matcher = DictionaryMatch(d=get_digikey_parts_set(dict_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use user-defined functions to further improve our matchers. For example, here we use patterns in the document filenames as a signal for whether a span of text in a document is a valid transistor part number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def common_prefix_length_diff(str1, str2):\n",
    "    for i in range(min(len(str1), len(str2))):\n",
    "        if str1[i] != str2[i]:\n",
    "            return min(len(str1), len(str2)) - i\n",
    "    return 0\n",
    "\n",
    "def part_file_name_conditions(attr):\n",
    "    file_name = attr.sentence.document.name\n",
    "    if len(file_name.split('_')) != 2: return False\n",
    "    if attr.get_span()[0] == '-': return False\n",
    "    name = attr.get_span().replace('-', '')\n",
    "    return any(char.isdigit() for char in name) and any(char.isalpha() for char in name) and common_prefix_length_diff(file_name.split('_')[1], name) <= 2\n",
    "\n",
    "add_rgx = '^[A-Z0-9\\-]{5,15}$'\n",
    "\n",
    "part_file_name_lambda_matcher = LambdaFunctionMatch(func=part_file_name_conditions)\n",
    "part_file_name_matcher = Intersect(RegexMatchSpan(rgx=add_rgx, longest_match_only=True), part_file_name_lambda_matcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can union all of these matchers together to form our final part matcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "part_matcher = Union(part_rgx_matcher, part_dict_matcher, part_file_name_matcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing a simple polarity  matcher\n",
    "\n",
    "In contrast, our polarity matcher can be a very simple regular expression since polarity values are either \"NPN\" or \"PNP\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attr_matcher = RegexMatchSpan(rgx=r'(?:[1][5-9]|20)[05]', longest_match_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two matchers define each entity in our relation schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a relation's `ContextSpaces`\n",
    "\n",
    "Next, in order to define the \"space\" of all candidates that are even considered from the document, we need to define a `ContextSpace` for each component of the relation we wish to extract.\n",
    "\n",
    "In the case of transistor part numbers, the `ContextSpace` can be quite complex due to the need to handle implicit part numbers that are implied in text like \"BC546A/B/C...BC548A/B/C\", which refers to 9 unique part numbers. In addition, to handle these, we consider all n-grams up to 3 words long.\n",
    "\n",
    "In contrast, the `ContextSpace` for polarity values is simple: we only need to look at every word in a document individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hardware_spaces import OmniNgramsPart, OmniNgramsTemp\n",
    "    \n",
    "part_ngrams = OmniNgramsPart(parts_by_doc=None, n_max=3)\n",
    "attr_ngrams = OmniNgramsTemp(n_max=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining candidate `Throttlers`\n",
    "\n",
    "Next, we need to define **throttlers**, which allow us to further prune excess candidates and avoid unnecessarily materializing invalid candidates. Trottlers, like matchers, act as hard filters, and should be created to have high precsion while maintaining complete recall, if possible.\n",
    "\n",
    "Here, we create a throttler that discards candidates if they are in the same table, but the part and polarity are not vertically or horizontally aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.contrib.fonduer.lf_helpers import *\n",
    "import re\n",
    "\n",
    "def stg_temp_throttler((part, attr)):\n",
    "    return True\n",
    "\n",
    "throttler = stg_temp_throttler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the Corpus into Test and Train\n",
    "\n",
    "We'll split the documents 40/5/5 into train/dev/test splits. Note that here we do this in a non-random order to preverse the consistency in the tutorial, and we reference the splits by 0/1/2 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.models import Document\n",
    "\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "ld   = len(docs)\n",
    "\n",
    "train_docs = set()\n",
    "dev_docs   = set()\n",
    "test_docs  = set()\n",
    "splits = (0.8, 0.9)\n",
    "for i,doc in enumerate(docs):\n",
    "    if i < splits[0] * ld:\n",
    "        train_docs.add(doc)\n",
    "    elif i < splits[1] * ld:\n",
    "        dev_docs.add(doc)\n",
    "    else:\n",
    "        test_docs.add(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the `CandidateExtractor`\n",
    "\n",
    "Now, we have all the component necessary to perform candidate extraction. We have defined the \"space\" of things to consider for each candidate, provided matchers that signal when a valid mention is seen, and a throttler to prunes away excess candidates. We now can define the `CandidateExtractor` with the contexts to extract from, the matchers, and the throttler to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "CPU times: user 12.7 s, sys: 587 ms, total: 13.2 s\n",
      "Wall time: 15 s\n"
     ]
    }
   ],
   "source": [
    "from snorkel.candidates import CandidateExtractor\n",
    "\n",
    "\n",
    "candidate_extractor = CandidateExtractor(Part_Attr, \n",
    "                        [part_ngrams, attr_ngrams], \n",
    "                        [part_matcher, attr_matcher], \n",
    "                        throttler=throttler)\n",
    "\n",
    "%time candidate_extractor.apply(train_docs, split=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we specified that these `Candidates` belong to the training set by specifying `split=0`; recall that we're referring to train/dev/test as splits 0/1/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 2055\n"
     ]
    }
   ],
   "source": [
    "train_cands = session.query(Part_Attr).filter(Part_Attr.split == 0).all()\n",
    "print \"Number of candidates:\", len(train_cands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeating for development and test splits\n",
    "Finally, we rerun the same operation for the other two document divisions: dev and test. For each, we simply load the `Corpus` object and run them through the `CandidateExtractor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "Number of candidates: 109\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "Number of candidates: 384\n",
      "CPU times: user 3.58 s, sys: 158 ms, total: 3.74 s\n",
      "Wall time: 4.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, docs in enumerate([dev_docs, test_docs]):\n",
    "    candidate_extractor.apply(docs, split=i+1)\n",
    "    print \"Number of candidates:\", session.query(Part_Attr).filter(Part_Attr.split == i+1).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Loading Evaluation Labels\n",
    "\n",
    "Although one of the main purposes of Snorkel is to enable training of state-of-the-art machine learning models without the burden of hand-labeling training data, it is still critical to have a small amount of labeled data to help us develop & evaluate our application.\n",
    "\n",
    "In particular, we will generally need two small labeled sets:\n",
    "* A development set, which can be a subset of our training set, which we use to help guide us when writing labeling functions (see next part of the tutorial)\n",
    "* A test set which we evaluate our final application performance against. Note that for fair evaluation, you should get someone not involved in development of your application to label the test set, so that it is blind!\n",
    "\n",
    "Let's load in the dev and test sets that we'll be labeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "dev_cands = session.query(Part_Attr).filter(Part_Attr.split == 1).all()\n",
    "print len(dev_cands)\n",
    "\n",
    "test_cands = session.query(Part_Attr).filter(Part_Attr.split == 2).all()\n",
    "print len(test_cands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Loading External Evaluation Labels\n",
    "\n",
    "We have already annotated the dev and test set for this tutorial, and we'll now load it using an externally-defined helper function.\n",
    "\n",
    "Loading and saving external \"gold\" labels can be a bit messy, but is often a critical part of development, especially when gold labels are expensive and/or time-consuming to obtain. Snorkel stores all labels that are manually annotated in a **stable** format (called StableLabels), which is somewhat independent from the rest of Snorkel's data model, does not get deleted when you delete the candidates, corpus, or any other objects, and can be recovered even if the rest of the data changes or is deleted.\n",
    "\n",
    "Our general procedure with external labels is to load them into the StableLabel table, then use Snorkel's helpers to load them into the main data model from there. If interested in example implementation details, please see the script we now load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2548 candidate labels\n",
      "[========================================] 100%\n",
      "AnnotatorLabels created: 629\n"
     ]
    }
   ],
   "source": [
    "from hardware_utils import load_hardware_labels\n",
    "\n",
    "gold_file = os.environ['SNORKELHOME'] + '/tutorials/fonduer/hardware/data/hardware_tutorial_gold.csv'\n",
    "load_hardware_labels(session, Part_Attr, gold_file, ATTRIBUTE ,annotator_name='gold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Featurization and modeling noisy training labels\n",
    "\n",
    "In this part of the tutorial, we will write **labeling functions** which express various heuristics, patterns, and [weak supervision](http://hazyresearch.github.io/snorkel/blog/weak_supervision.html) strategies to label our data.\n",
    "\n",
    "In the wild, hand-labeled training data is rare and expensive. A common scenario is to have access to tons of unlabeled training data, and have some idea of how to label them programmatically. For example:\n",
    "* We may be able to think of text patterns that would indicate a part and polarity mention are related, for example the word \"polarity\" appearing between them.\n",
    "* We may have access to an external knowledge base that lists some pairs of parts and polarities, and can use these to noisily label some of our mention pairs.\n",
    "Our labeling functions will capture these types of strategies. We know that these labeling functions will not be perfect, and some may be quite low-quality, so we will model their accuracies with a generative model, which `Fonduer` will help us easily apply.\n",
    "\n",
    "Using data programming, we can then train machine learning models to learn which features are the most important in classifying candidates.\n",
    "\n",
    "### Multimodal featurization of `Candidates`\n",
    "\n",
    "Unlike dealing with plain unstructured text, `Fonduer` deals with richly formatted data, and consequently featurizes each candidate with a baseline library of multimodal features. There are two methods that can be used to extract features: `Fonduer`'s optimized PostgreSQL batch extractor, or the default `Snorkel` extractor. We show both options:\n",
    "\n",
    "#### Extract with `Fonduer`'s optimized Postgres Batch Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "Copying part_attr_feature to postgres\n",
      "COPY 2055\n",
      "\n",
      "CPU times: user 5.47 s, sys: 368 ms, total: 5.84 s\n",
      "Wall time: 1min 58s\n",
      "(2055, 8621)\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "Copying part_attr_feature to postgres\n",
      "COPY 109\n",
      "\n",
      "CPU times: user 255 ms, sys: 39.1 ms, total: 294 ms\n",
      "Wall time: 19.9 s\n",
      "(109, 9324)\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "Copying part_attr_feature to postgres\n",
      "COPY 384\n",
      "\n",
      "CPU times: user 1.6 s, sys: 88.8 ms, total: 1.68 s\n",
      "Wall time: 20.4 s\n",
      "(384, 9837)\n"
     ]
    }
   ],
   "source": [
    "from snorkel.contrib.fonduer.async_annotations import BatchFeatureAnnotator\n",
    "\n",
    "featurizer = BatchFeatureAnnotator(Part_Attr)\n",
    "%time F_train = featurizer.apply(split=0, parallelism=PARALLEL)\n",
    "print F_train.shape\n",
    "%time F_dev = featurizer.apply(split=1, parallelism=PARALLEL)\n",
    "print F_dev.shape\n",
    "%time F_test = featurizer.apply(split=2, parallelism=PARALLEL)\n",
    "print F_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract with default `Snorkel` extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from snorkel.annotations import FeatureAnnotator\n",
    "# from snorkel.contrib.fonduer.features.features import get_all_feats\n",
    "\n",
    "# featurizer = FeatureAnnotator(f=get_all_feats)\n",
    "# %time F_train = featurizer.apply(split=0, parallelism=PARALLEL)\n",
    "# %time F_dev = featurizer.apply(split=1, parallelism=PARALLEL)\n",
    "# %time F_test = featurizer.apply(split=2, parallelism=PARALLEL)\n",
    "# F_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a development set\n",
    "In our setting here, we will use the phrase \"development set\" to refer to a set of examples (here, a subset of our training set) which we label by hand and use to help us develop and refine labeling functions. Unlike the test set, which we do not look at and use for final evaluation, we can inspect the development set while writing labeling functions.\n",
    "\n",
    "In our case, we already loaded labels for a development set (split 1), so we can load them again now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<109x1 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 42 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "L_gold_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Labeling Functions\n",
    "\n",
    "In Snorkel, our primary interface through which we provide training signal to the end extraction model we are training is by writing labeling functions (**LFs**) (as opposed to hand-labeling massive training sets). We'll go through some examples for our spouse extraction task below.\n",
    "\n",
    "A labeling function isn't anything special. It's just a Python function that accepts a `Candidate` as the input argument and returns `1` if it says the Candidate should be marked as true, `-1` if it says the `Candidate` should be marked as false, and `0` if it doesn't know how to vote and abstains. In practice, many labeling functions are unipolar: it labels only 1s and 0s, or it labels only -1s and 0s.\n",
    "\n",
    "Recall that our goal is to ultimately train a high-performance classification model that predicts which of our Candidates are true mentions of spouse relations. It turns out that we can do this by writing potentially low-quality labeling functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.contrib.fonduer.lf_helpers import *\n",
    "import re\n",
    "\n",
    "def LF_storage_row(c):\n",
    "    return 1 if 'storage' in get_row_ngrams(c.attr) else 0\n",
    "\n",
    "def LF_operating_row(c):\n",
    "    return 1 if 'operating' in get_row_ngrams(c.attr) else 0\n",
    "\n",
    "def LF_temperature_row(c):\n",
    "    return 1 if 'temperature' in get_row_ngrams(c.attr) else 0\n",
    "\n",
    "def LF_tstg_row(c):\n",
    "    return 1 if overlap(\n",
    "        ['tstg','stg','ts'], \n",
    "        list(get_row_ngrams(c.attr))) else 0\n",
    "\n",
    "def LF_not_temp_relevant(c):\n",
    "    return -1 if not overlap(\n",
    "        ['storage','temperature','tstg','stg', 'ts'],\n",
    "        list(get_aligned_ngrams(c.attr))) else 0\n",
    "\n",
    "def LF_temp_outside_table(c):\n",
    "    return -1 if not c.attr.is_tabular() is None else 0\n",
    "\n",
    "def LF_too_many_numbers_row(c):\n",
    "    num_numbers = list(get_row_ngrams(c.attr, attrib=\"ner_tags\")).count('number')\n",
    "    return -1 if num_numbers >= 3 else 0\n",
    "\n",
    "def LF_collector_aligned(c):\n",
    "    return -1 if overlap(\n",
    "        ['collector', 'collector-current', 'collector-base', 'collector-emitter'],\n",
    "        list(get_aligned_ngrams(c.attr))) else 0\n",
    "\n",
    "def LF_current_aligned(c):\n",
    "    ngrams = get_aligned_ngrams(c.attr)\n",
    "    return -1 if overlap(\n",
    "        ['current', 'dc', 'ic'],\n",
    "        list(get_aligned_ngrams(c.attr))) else 0\n",
    "\n",
    "def LF_voltage_row_temp(c):\n",
    "    ngrams = get_aligned_ngrams(c.attr)\n",
    "    return -1 if overlap(\n",
    "        ['voltage', 'cbo', 'ceo', 'ebo', 'v'],\n",
    "        list(get_aligned_ngrams(c.attr))) else 0\n",
    "\n",
    "def LF_voltage_row_part(c):\n",
    "    ngrams = get_aligned_ngrams(c.part)\n",
    "    return -1 if overlap(\n",
    "        ['voltage', 'cbo', 'ceo', 'ebo', 'v'],\n",
    "        list(get_aligned_ngrams(c.attr))) else 0\n",
    "\n",
    "def LF_typ_row(c):\n",
    "    return -1 if overlap(\n",
    "        ['typ', 'typ.'],\n",
    "        list(get_row_ngrams(c.attr))) else 0\n",
    "\n",
    "def LF_test_condition_aligned(c):\n",
    "    return -1 if overlap(\n",
    "        ['test', 'condition'],\n",
    "        list(get_aligned_ngrams(c.attr))) else 0\n",
    "\n",
    "def LF_complement_left_row(c):\n",
    "    return -1 if (\n",
    "        overlap(['complement','complementary'], \n",
    "        chain.from_iterable([get_row_ngrams(c.part), get_left_ngrams(c.part, window=10)]))) else 0\n",
    "\n",
    "def LF_temp_on_high_page_num(c):\n",
    "    return -1 if c.attr.get_attrib_tokens('page')[0] > 2 else 0\n",
    "\n",
    "def LF_to_left(c):\n",
    "    return 1 if 'to' in get_left_ngrams(c.attr, window=2) else 0\n",
    "\n",
    "def LF_negative_number_left(c):\n",
    "    return 1 if any([re.match(r'-\\s*\\d+', ngram) for ngram in get_left_ngrams(c.attr, window=4)]) else 0\n",
    "\n",
    "\n",
    "\n",
    "stg_temp_lfs = [\n",
    "    LF_storage_row,\n",
    "    LF_operating_row,\n",
    "    LF_temperature_row,\n",
    "    LF_tstg_row,\n",
    "    LF_not_temp_relevant,\n",
    "    LF_temp_outside_table,\n",
    "    LF_too_many_numbers_row,\n",
    "    LF_collector_aligned,\n",
    "    LF_current_aligned,\n",
    "    LF_voltage_row_temp,\n",
    "    LF_voltage_row_part,\n",
    "    LF_typ_row,\n",
    "    LF_test_condition_aligned,\n",
    "    LF_complement_left_row,\n",
    "    LF_temp_on_high_page_num,\n",
    "    LF_to_left,\n",
    "    LF_negative_number_left\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the Labeling Functions\n",
    "\n",
    "Next, we need to actually run the LFs over all of our training candidates, producing a set of `Labels` and `LabelKeys` (just the names of the LFs) in the database. We'll do this using the `LabelAnnotator` class, a `UDF` which we will again run with `UDFRunner`. Note that this will delete any existing `Labels` and `LabelKeys` for this candidate set. Also note that we are using `Fonduer`'s optimized batch label annotator, which runs in parallel and depends on having Postgres as the backend database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "Copying part_attr_label to postgres\n",
      "COPY 2055\n",
      "\n",
      "CPU times: user 193 ms, sys: 43.3 ms, total: 236 ms\n",
      "Wall time: 44.2 s\n",
      "(2055, 17)\n",
      "Running UDF...\n",
      "Copying part_attr_label_updates to postgres\n",
      "COPY 2055\n",
      "\n",
      "CPU times: user 309 ms, sys: 71.3 ms, total: 381 ms\n",
      "Wall time: 43.1 s\n",
      "(2055, 17)\n",
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process BatchAnnotatorUDF-28:\n",
      "Traceback (most recent call last):\n",
      "Process BatchAnnotatorUDF-29:\n",
      "  File \"/Users/sen/anaconda2/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sen/anaconda2/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "Process BatchAnnotatorUDF-30:\n",
      "    self.run()\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sen/Documents/Sen/Stanford/Deepdive/snorkel/snorkel/udf.py\", line 154, in run\n",
      "  File \"/Users/sen/Documents/Sen/Stanford/Deepdive/snorkel/snorkel/udf.py\", line 154, in run\n",
      "  File \"/Users/sen/anaconda2/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    for y in self.apply(x, **self.apply_kwargs):\n",
      "    for y in self.apply(x, **self.apply_kwargs):\n",
      "    self.run()\n",
      "Process BatchAnnotatorUDF-31:\n",
      "  File \"/Users/sen/Documents/Sen/Stanford/Deepdive/snorkel/snorkel/contrib/fonduer/async_annotations.py\", line 199, in apply\n",
      "  File \"/Users/sen/Documents/Sen/Stanford/Deepdive/snorkel/snorkel/contrib/fonduer/async_annotations.py\", line 199, in apply\n",
      "  File \"/Users/sen/Documents/Sen/Stanford/Deepdive/snorkel/snorkel/udf.py\", line 154, in run\n",
      "Traceback (most recent call last):\n",
      "    for y in self.apply(x, **self.apply_kwargs):\n",
      "    keys, values = zip(*[(k,v) for k,v in self.anno_generator(candidate) if v != 0])\n",
      "    keys, values = zip(*[(k,v) for k,v in self.anno_generator(candidate) if v != 0])\n",
      "  File \"/Users/sen/anaconda2/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/Users/sen/Documents/Sen/Stanford/Deepdive/snorkel/snorkel/contrib/fonduer/async_annotations.py\", line 199, in apply\n",
      "ValueError: need more than 0 values to unpack\n",
      "ValueError: need more than 0 values to unpack\n",
      "    self.run()\n",
      "    keys, values = zip(*[(k,v) for k,v in self.anno_generator(candidate) if v != 0])\n",
      "  File \"/Users/sen/Documents/Sen/Stanford/Deepdive/snorkel/snorkel/udf.py\", line 154, in run\n",
      "ValueError: need more than 0 values to unpack\n",
      "    for y in self.apply(x, **self.apply_kwargs):\n",
      "  File \"/Users/sen/Documents/Sen/Stanford/Deepdive/snorkel/snorkel/contrib/fonduer/async_annotations.py\", line 199, in apply\n",
      "    keys, values = zip(*[(k,v) for k,v in self.anno_generator(candidate) if v != 0])\n",
      "ValueError: need more than 0 values to unpack\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying part_attr_label_updates to postgres\n",
      "COPY 0\n",
      "\n",
      "CPU times: user 408 ms, sys: 88.3 ms, total: 497 ms\n",
      "Wall time: 639 ms\n",
      "(2055, 17)\n"
     ]
    }
   ],
   "source": [
    "from snorkel.contrib.fonduer.async_annotations import BatchLabelAnnotator\n",
    "\n",
    "labeler = BatchLabelAnnotator(Part_Attr, lfs = stg_temp_lfs)\n",
    "%time L_train = labeler.apply(split=0, parallelism=4)\n",
    "print L_train.shape\n",
    "L_train1 = L_train\n",
    "\n",
    "# Update existing labels, assume the lfs are updated\n",
    "stg_temp_lfs2 = stg_temp_lfs\n",
    "labeler = BatchLabelAnnotator(Part_Attr, lfs = stg_temp_lfs2)\n",
    "%time L_train = labeler.apply(split=0, clear=False, update_existing=True, parallelism=4)\n",
    "print L_train.shape\n",
    "L_train2 = L_train\n",
    "\n",
    "# Update existing labels, assume the lfs are updated\n",
    "def LF_test(c):\n",
    "    return 1 if 'to' in get_left_ngrams(c.attr, window=2) else 0\n",
    "stg_temp_lfs3 = [LF_test]\n",
    "labeler = BatchLabelAnnotator(Part_Attr, lfs = stg_temp_lfs3)\n",
    "%time L_train = labeler.apply(split=0, clear=False, update_existing=True, parallelism=4)\n",
    "print L_train.shape\n",
    "L_train3 = L_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part_Attr(Span(\"BC817-16\", sentence=109098, chars=[0,7], words=[0,0]), ImplicitSpan(\"150\", sentence=108908, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC818\", sentence=108841, chars=[20,24], words=[1,1]), ImplicitSpan(\"150\", sentence=108908, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC818\", sentence=84677, chars=[36,40], words=[5,5]), ImplicitSpan(\"150\", sentence=108908, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC817\", sentence=108990, chars=[9,13], words=[4,4]), ImplicitSpan(\"150\", sentence=108908, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC817\", sentence=108841, chars=[0,4], words=[0,0]), ImplicitSpan(\"150\", sentence=108908, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC817\", sentence=84677, chars=[26,30], words=[3,3]), ImplicitSpan(\"150\", sentence=108908, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"2N3906RL1G\", sentence=97010, chars=[0,9], words=[0,0]), ImplicitSpan(\"150\", sentence=96534, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"2N3906\", sentence=76060, chars=[0,5], words=[0,0]), ImplicitSpan(\"150\", sentence=96534, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"2N3906RLRAG\", sentence=97031, chars=[0,10], words=[0,0]), ImplicitSpan(\"150\", sentence=96534, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"2N3906\", sentence=76115, words=[0,0], position=[0]), ImplicitSpan(\"150\", sentence=96534, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"2N3906RL1\", sentence=97001, chars=[0,8], words=[0,0]), ImplicitSpan(\"150\", sentence=96534, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC546A\", sentence=80476, chars=[0,5], words=[0,0]), ImplicitSpan(\"150\", sentence=111926, words=[3,3], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC547\", sentence=111950, words=[2,4], position=[1]), ImplicitSpan(\"150\", sentence=111926, words=[3,3], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC547\", sentence=111950, words=[3,5], position=[0]), ImplicitSpan(\"150\", sentence=111926, words=[3,3], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC549C\", sentence=80557, words=[0,2], position=[0]), ImplicitSpan(\"150\", sentence=111926, words=[3,3], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC547\", sentence=111830, words=[0,2], position=[0]), ImplicitSpan(\"150\", sentence=111926, words=[3,3], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"2N4123\", sentence=80418, words=[0,0], position=[0]), ImplicitSpan(\"150\", sentence=105898, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"2N4123\", sentence=80462, words=[0,2], position=[0]), ImplicitSpan(\"150\", sentence=105898, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"2N4124\", sentence=81514, words=[0,2], position=[0]), ImplicitSpan(\"150\", sentence=106475, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"MMBT4124\", sentence=81553, chars=[25,32], words=[1,1]), ImplicitSpan(\"150\", sentence=106475, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"CEN-U45\", sentence=81645, chars=[26,32], words=[3,3]), ImplicitSpan(\"150\", sentence=113289, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"CEN-U45\", sentence=81645, chars=[26,32], words=[3,3]), ImplicitSpan(\"150\", sentence=113113, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC549\", sentence=83651, words=[2,4], position=[1]), ImplicitSpan(\"150\", sentence=107880, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC548\", sentence=83611, words=[4,6], position=[1]), ImplicitSpan(\"150\", sentence=107880, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC558\", sentence=83654, words=[6,8], position=[0]), ImplicitSpan(\"150\", sentence=107880, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC546\", sentence=83611, words=[0,2], position=[1]), ImplicitSpan(\"150\", sentence=107880, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC549\", sentence=83611, words=[5,7], position=[0]), ImplicitSpan(\"150\", sentence=107880, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC560\", sentence=83654, chars=[46,50], words=[11,11]), ImplicitSpan(\"150\", sentence=107880, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC547\", sentence=83611, words=[1,3], position=[1]), ImplicitSpan(\"150\", sentence=107880, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC559\", sentence=83654, words=[7,9], position=[1]), ImplicitSpan(\"150\", sentence=107880, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC550\", sentence=83629, chars=[32,36], words=[8,8]), ImplicitSpan(\"150\", sentence=107880, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC549\", sentence=83611, words=[6,8], position=[1]), ImplicitSpan(\"150\", sentence=107880, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC559\", sentence=83654, words=[8,10], position=[0]), ImplicitSpan(\"150\", sentence=107880, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC547\", sentence=83611, words=[2,4], position=[1]), ImplicitSpan(\"150\", sentence=107880, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC548\", sentence=83611, words=[3,5], position=[1]), ImplicitSpan(\"150\", sentence=107880, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC558\", sentence=83654, words=[5,7], position=[1]), ImplicitSpan(\"150\", sentence=107880, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"CMPT5401E\", sentence=77475, chars=[26,34], words=[3,3]), ImplicitSpan(\"150\", sentence=77491, words=[15,15], position=[0]))\n",
      "Part_Attr(Span(\"CMPT5401E\", sentence=99990, chars=[0,8], words=[0,0]), ImplicitSpan(\"150\", sentence=77491, words=[15,15], position=[0]))\n",
      "Part_Attr(Span(\"CMPT5551E\", sentence=77482, chars=[21,29], words=[2,2]), ImplicitSpan(\"150\", sentence=77491, words=[15,15], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"2N6426\", sentence=88611, words=[0,2], position=[0]), ImplicitSpan(\"150\", sentence=115460, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"2N6426\", sentence=88600, chars=[0,5], words=[0,0]), ImplicitSpan(\"150\", sentence=115460, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC817\", sentence=83218, chars=[20,24], words=[2,2]), ImplicitSpan(\"150\", sentence=114552, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC818-16\", sentence=83209, chars=[31,38], words=[3,3]), ImplicitSpan(\"150\", sentence=114552, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC807\", sentence=83218, chars=[37,41], words=[4,4]), ImplicitSpan(\"150\", sentence=114552, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC817-16\", sentence=83209, chars=[0,7], words=[0,0]), ImplicitSpan(\"150\", sentence=114552, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC817\", sentence=83206, chars=[0,4], words=[0,0]), ImplicitSpan(\"150\", sentence=114552, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC818\", sentence=83227, chars=[6,10], words=[1,1]), ImplicitSpan(\"150\", sentence=114552, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC808\", sentence=83221, chars=[17,21], words=[2,2]), ImplicitSpan(\"150\", sentence=114552, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC81840\", sentence=83182, chars=[8,14], words=[2,2]), ImplicitSpan(\"150\", sentence=114552, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC818\", sentence=83206, chars=[32,36], words=[3,3]), ImplicitSpan(\"150\", sentence=114552, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC807\", sentence=77256, chars=[14,18], words=[2,2]), ImplicitSpan(\"150\", sentence=99536, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC817\", sentence=77240, words=[0,2], position=[1]), ImplicitSpan(\"150\", sentence=99536, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC818\", sentence=77249, chars=[8,12], words=[2,2]), ImplicitSpan(\"150\", sentence=99536, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC808\", sentence=77256, chars=[22,26], words=[4,4]), ImplicitSpan(\"150\", sentence=99536, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC818-40L\", sentence=89440, words=[0,2], position=[0]), ImplicitSpan(\"150\", sentence=116335, words=[3,3], position=[0]))\n",
      "Part_Attr(Span(\"BC818-40LT1\", sentence=89430, chars=[0,10], words=[0,0]), ImplicitSpan(\"150\", sentence=116335, words=[3,3], position=[0]))\n",
      "Part_Attr(Span(\"FCX491ATA\", sentence=100555, chars=[0,8], words=[0,0]), ImplicitSpan(\"150\", sentence=100718, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"FCX491AQTA\", sentence=100573, chars=[0,9], words=[0,0]), ImplicitSpan(\"150\", sentence=100718, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"FCX491A\", sentence=77601, chars=[0,6], words=[0,0]), ImplicitSpan(\"150\", sentence=100718, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"FCX591A\", sentence=77622, chars=[24,30], words=[4,4]), ImplicitSpan(\"150\", sentence=100718, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"AP02002\", sentence=77714, chars=[11,17], words=[2,2]), ImplicitSpan(\"150\", sentence=100718, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"2DD2652-7\", sentence=114104, chars=[0,8], words=[0,0]), ImplicitSpan(\"150\", sentence=114031, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"2DD2652\", sentence=82047, chars=[0,6], words=[0,0]), ImplicitSpan(\"150\", sentence=114031, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"TIP152\", sentence=108230, chars=[7,12], words=[1,1]), ImplicitSpan(\"150\", sentence=84292, words=[3,3], position=[0]))\n",
      "Part_Attr(Span(\"TIP152\", sentence=108230, chars=[7,12], words=[1,1]), ImplicitSpan(\"150\", sentence=108188, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"TIP152\", sentence=108230, chars=[7,12], words=[1,1]), ImplicitSpan(\"150\", sentence=84289, words=[3,3], position=[0]))\n",
      "Part_Attr(Span(\"TIP152\", sentence=108230, chars=[7,12], words=[1,1]), ImplicitSpan(\"150\", sentence=108196, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"TIP152\", sentence=84569, chars=[14,19], words=[2,2]), ImplicitSpan(\"150\", sentence=84292, words=[3,3], position=[0]))\n",
      "Part_Attr(Span(\"TIP152\", sentence=84569, chars=[14,19], words=[2,2]), ImplicitSpan(\"150\", sentence=108188, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"TIP152\", sentence=84569, chars=[14,19], words=[2,2]), ImplicitSpan(\"150\", sentence=84289, words=[3,3], position=[0]))\n",
      "Part_Attr(Span(\"TIP152\", sentence=84569, chars=[14,19], words=[2,2]), ImplicitSpan(\"150\", sentence=108196, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BY205-600\", sentence=84428, chars=[0,8], words=[0,0]), ImplicitSpan(\"150\", sentence=84292, words=[3,3], position=[0]))\n",
      "Part_Attr(Span(\"BY205-600\", sentence=84428, chars=[0,8], words=[0,0]), ImplicitSpan(\"150\", sentence=108188, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BY205-600\", sentence=84428, chars=[0,8], words=[0,0]), ImplicitSpan(\"150\", sentence=84289, words=[3,3], position=[0]))\n",
      "Part_Attr(Span(\"BY205-600\", sentence=84428, chars=[0,8], words=[0,0]), ImplicitSpan(\"150\", sentence=108196, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"TIP150\", sentence=108100, chars=[0,5], words=[0,0]), ImplicitSpan(\"150\", sentence=84292, words=[3,3], position=[0]))\n",
      "Part_Attr(Span(\"TIP150\", sentence=108100, chars=[0,5], words=[0,0]), ImplicitSpan(\"150\", sentence=108188, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"TIP150\", sentence=108100, chars=[0,5], words=[0,0]), ImplicitSpan(\"150\", sentence=84289, words=[3,3], position=[0]))\n",
      "Part_Attr(Span(\"TIP150\", sentence=108100, chars=[0,5], words=[0,0]), ImplicitSpan(\"150\", sentence=108196, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"2N4124\", sentence=77942, chars=[0,5], words=[0,0]), ImplicitSpan(\"150\", sentence=102053, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"2N4124\", sentence=77954, chars=[160,165], words=[6,6]), ImplicitSpan(\"150\", sentence=102053, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC547B\", sentence=100403, words=[11,13], position=[1]), ImplicitSpan(\"150\", sentence=100248, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC546A\", sentence=100430, words=[10,12], position=[1]), ImplicitSpan(\"150\", sentence=100248, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC548\", sentence=100388, words=[16,18], position=[1]), ImplicitSpan(\"150\", sentence=100248, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC546B\", sentence=100403, words=[9,11], position=[1]), ImplicitSpan(\"150\", sentence=100248, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC546\", sentence=77586, words=[3,5], position=[1]), ImplicitSpan(\"150\", sentence=100248, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC546-48ABC\", sentence=77581, chars=[0,10], words=[0,0]), ImplicitSpan(\"150\", sentence=100248, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC547\", sentence=77586, words=[4,6], position=[1]), ImplicitSpan(\"150\", sentence=100248, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC548\", sentence=100388, words=[17,19], position=[0]), ImplicitSpan(\"150\", sentence=100248, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC547A\", sentence=100430, words=[12,14], position=[0]), ImplicitSpan(\"150\", sentence=100248, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC546\", sentence=77586, words=[2,4], position=[0]), ImplicitSpan(\"150\", sentence=100248, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC547B\", sentence=100403, words=[10,12], position=[1]), ImplicitSpan(\"150\", sentence=100248, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"FZT651TA\", sentence=109329, chars=[0,7], words=[0,0]), ImplicitSpan(\"150\", sentence=109518, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"FZT651\", sentence=84707, chars=[0,5], words=[0,0]), ImplicitSpan(\"150\", sentence=109518, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"FZT751\", sentence=109272, chars=[24,29], words=[4,4]), ImplicitSpan(\"150\", sentence=109518, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"AP02002\", sentence=85229, chars=[11,17], words=[2,2]), ImplicitSpan(\"150\", sentence=109518, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC182\", sentence=79468, chars=[0,4], words=[0,0]), ImplicitSpan(\"150\", sentence=110086, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC182\", sentence=79490, words=[0,2], position=[0]), ImplicitSpan(\"150\", sentence=110086, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"CXT4033\", sentence=81831, chars=[26,32], words=[3,3]), ImplicitSpan(\"150\", sentence=113465, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC327\", sentence=82894, chars=[14,18], words=[2,2]), ImplicitSpan(\"150\", sentence=107206, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC337\", sentence=82852, chars=[0,4], words=[0,0]), ImplicitSpan(\"150\", sentence=107206, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC328\", sentence=82894, chars=[22,26], words=[4,4]), ImplicitSpan(\"150\", sentence=107206, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC338\", sentence=82852, chars=[6,10], words=[1,1]), ImplicitSpan(\"150\", sentence=107206, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC338\", sentence=82873, chars=[8,12], words=[2,2]), ImplicitSpan(\"150\", sentence=107206, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC33716BU\", sentence=107058, chars=[0,8], words=[0,0]), ImplicitSpan(\"150\", sentence=107206, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC33716TFR\", sentence=107073, chars=[0,9], words=[0,0]), ImplicitSpan(\"150\", sentence=107206, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC848\", sentence=108546, chars=[0,4], words=[0,0]), ImplicitSpan(\"150\", sentence=108617, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC846A\", sentence=108763, chars=[0,5], words=[0,0]), ImplicitSpan(\"150\", sentence=108617, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC846\", sentence=84627, words=[3,5], position=[0]), ImplicitSpan(\"150\", sentence=108617, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC848\", sentence=108661, chars=[9,13], words=[4,4]), ImplicitSpan(\"150\", sentence=108617, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC848\", sentence=84629, chars=[4,8], words=[1,1]), ImplicitSpan(\"150\", sentence=108617, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC846\", sentence=84627, words=[2,4], position=[0]), ImplicitSpan(\"150\", sentence=108617, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"MMBT6427\", sentence=97543, chars=[1,8], words=[1,1]), ImplicitSpan(\"150\", sentence=97523, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"MMBT6427\", sentence=76525, chars=[25,32], words=[1,1]), ImplicitSpan(\"150\", sentence=97523, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"MMBT6427\", sentence=76513, chars=[9,16], words=[2,2]), ImplicitSpan(\"150\", sentence=97523, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"2N6427\", sentence=76513, chars=[0,5], words=[0,0]), ImplicitSpan(\"150\", sentence=97523, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"MPSA14\", sentence=76534, chars=[4,9], words=[1,1]), ImplicitSpan(\"150\", sentence=97523, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"MMBT6427\", sentence=76524, chars=[11,18], words=[2,2]), ImplicitSpan(\"150\", sentence=97523, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC337\", sentence=115858, chars=[82,86], words=[11,11]), ImplicitSpan(\"150\", sentence=115801, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC337\", sentence=88969, chars=[0,4], words=[0,0]), ImplicitSpan(\"150\", sentence=115801, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC337\", sentence=88979, words=[0,2], position=[1]), ImplicitSpan(\"150\", sentence=115801, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC337-25\", sentence=88979, words=[2,4], position=[0]), ImplicitSpan(\"150\", sentence=115801, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC337-25\", sentence=88979, words=[1,3], position=[0]), ImplicitSpan(\"150\", sentence=115801, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"2N3905\", sentence=81990, chars=[26,31], words=[3,3]), ImplicitSpan(\"150\", sentence=113661, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"2N3904\", sentence=81996, chars=[39,44], words=[6,6]), ImplicitSpan(\"150\", sentence=113661, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"2N3903\", sentence=81996, chars=[28,33], words=[4,4]), ImplicitSpan(\"150\", sentence=113661, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"2N3905\", sentence=81981, chars=[0,5], words=[0,0]), ImplicitSpan(\"150\", sentence=113661, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC549\", sentence=89230, words=[5,7], position=[0]), ImplicitSpan(\"150\", sentence=116175, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC547\", sentence=89230, words=[1,3], position=[1]), ImplicitSpan(\"150\", sentence=116175, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC559\", sentence=89244, words=[7,9], position=[1]), ImplicitSpan(\"150\", sentence=116175, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC549\", sentence=89230, words=[6,8], position=[1]), ImplicitSpan(\"150\", sentence=116175, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC559\", sentence=89244, words=[8,10], position=[0]), ImplicitSpan(\"150\", sentence=116175, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC547\", sentence=89230, words=[2,4], position=[1]), ImplicitSpan(\"150\", sentence=116175, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC550\", sentence=89236, chars=[32,36], words=[8,8]), ImplicitSpan(\"150\", sentence=116175, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC548\", sentence=89230, words=[3,5], position=[1]), ImplicitSpan(\"150\", sentence=116175, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC558\", sentence=89244, words=[5,7], position=[1]), ImplicitSpan(\"150\", sentence=116175, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC560\", sentence=89244, chars=[46,50], words=[11,11]), ImplicitSpan(\"150\", sentence=116175, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC548\", sentence=89230, words=[4,6], position=[1]), ImplicitSpan(\"150\", sentence=116175, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC549\", sentence=89243, words=[2,4], position=[1]), ImplicitSpan(\"150\", sentence=116175, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC558\", sentence=89244, words=[6,8], position=[0]), ImplicitSpan(\"150\", sentence=116175, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC546\", sentence=89230, words=[0,2], position=[1]), ImplicitSpan(\"150\", sentence=116175, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BD246B\", sentence=112883, words=[0,0], position=[0]), ImplicitSpan(\"150\", sentence=81336, words=[3,3], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BD246B\", sentence=112883, words=[0,0], position=[0]), ImplicitSpan(\"150\", sentence=112785, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BD246B\", sentence=112883, words=[0,0], position=[0]), ImplicitSpan(\"150\", sentence=81333, words=[3,3], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BD246B\", sentence=112883, words=[0,0], position=[0]), ImplicitSpan(\"150\", sentence=112777, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BD246C\", sentence=112817, chars=[14,19], words=[2,2]), ImplicitSpan(\"150\", sentence=81336, words=[3,3], position=[0]))\n",
      "Part_Attr(Span(\"BD246C\", sentence=112817, chars=[14,19], words=[2,2]), ImplicitSpan(\"150\", sentence=112785, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BD246C\", sentence=112817, chars=[14,19], words=[2,2]), ImplicitSpan(\"150\", sentence=81333, words=[3,3], position=[0]))\n",
      "Part_Attr(Span(\"BD246C\", sentence=112817, chars=[14,19], words=[2,2]), ImplicitSpan(\"150\", sentence=112777, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BD246A\", sentence=112683, chars=[0,5], words=[0,0]), ImplicitSpan(\"150\", sentence=81336, words=[3,3], position=[0]))\n",
      "Part_Attr(Span(\"BD246A\", sentence=112683, chars=[0,5], words=[0,0]), ImplicitSpan(\"150\", sentence=112785, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BD246A\", sentence=112683, chars=[0,5], words=[0,0]), ImplicitSpan(\"150\", sentence=81333, words=[3,3], position=[0]))\n",
      "Part_Attr(Span(\"BD246A\", sentence=112683, chars=[0,5], words=[0,0]), ImplicitSpan(\"150\", sentence=112777, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BD246C\", sentence=81540, chars=[20,25], words=[3,3]), ImplicitSpan(\"150\", sentence=81336, words=[3,3], position=[0]))\n",
      "Part_Attr(Span(\"BD246C\", sentence=81540, chars=[20,25], words=[3,3]), ImplicitSpan(\"150\", sentence=112785, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BD246C\", sentence=81540, chars=[20,25], words=[3,3]), ImplicitSpan(\"150\", sentence=81333, words=[3,3], position=[0]))\n",
      "Part_Attr(Span(\"BD246C\", sentence=81540, chars=[20,25], words=[3,3]), ImplicitSpan(\"150\", sentence=112777, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BD246\", sentence=112677, chars=[0,4], words=[0,0]), ImplicitSpan(\"150\", sentence=81336, words=[3,3], position=[0]))\n",
      "Part_Attr(Span(\"BD246\", sentence=112677, chars=[0,4], words=[0,0]), ImplicitSpan(\"150\", sentence=112785, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BD246\", sentence=112677, chars=[0,4], words=[0,0]), ImplicitSpan(\"150\", sentence=81333, words=[3,3], position=[0]))\n",
      "Part_Attr(Span(\"BD246\", sentence=112677, chars=[0,4], words=[0,0]), ImplicitSpan(\"150\", sentence=112777, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BD246A\", sentence=81540, chars=[6,11], words=[1,1]), ImplicitSpan(\"150\", sentence=81336, words=[3,3], position=[0]))\n",
      "Part_Attr(Span(\"BD246A\", sentence=81540, chars=[6,11], words=[1,1]), ImplicitSpan(\"150\", sentence=112785, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BD246A\", sentence=81540, chars=[6,11], words=[1,1]), ImplicitSpan(\"150\", sentence=81333, words=[3,3], position=[0]))\n",
      "Part_Attr(Span(\"BD246A\", sentence=81540, chars=[6,11], words=[1,1]), ImplicitSpan(\"150\", sentence=112777, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BD246\", sentence=112881, words=[0,0], position=[0]), ImplicitSpan(\"150\", sentence=81336, words=[3,3], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BD246\", sentence=112881, words=[0,0], position=[0]), ImplicitSpan(\"150\", sentence=112785, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BD246\", sentence=112881, words=[0,0], position=[0]), ImplicitSpan(\"150\", sentence=81333, words=[3,3], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BD246\", sentence=112881, words=[0,0], position=[0]), ImplicitSpan(\"150\", sentence=112777, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BD246B\", sentence=112683, chars=[7,12], words=[1,1]), ImplicitSpan(\"150\", sentence=81336, words=[3,3], position=[0]))\n",
      "Part_Attr(Span(\"BD246B\", sentence=112683, chars=[7,12], words=[1,1]), ImplicitSpan(\"150\", sentence=112785, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BD246B\", sentence=112683, chars=[7,12], words=[1,1]), ImplicitSpan(\"150\", sentence=81333, words=[3,3], position=[0]))\n",
      "Part_Attr(Span(\"BD246B\", sentence=112683, chars=[7,12], words=[1,1]), ImplicitSpan(\"150\", sentence=112777, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BD246B\", sentence=81540, chars=[13,18], words=[2,2]), ImplicitSpan(\"150\", sentence=81336, words=[3,3], position=[0]))\n",
      "Part_Attr(Span(\"BD246B\", sentence=81540, chars=[13,18], words=[2,2]), ImplicitSpan(\"150\", sentence=112785, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BD246B\", sentence=81540, chars=[13,18], words=[2,2]), ImplicitSpan(\"150\", sentence=81333, words=[3,3], position=[0]))\n",
      "Part_Attr(Span(\"BD246B\", sentence=81540, chars=[13,18], words=[2,2]), ImplicitSpan(\"150\", sentence=112777, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BD245\", sentence=81294, chars=[40,44], words=[6,6]), ImplicitSpan(\"150\", sentence=81336, words=[3,3], position=[0]))\n",
      "Part_Attr(Span(\"BD245\", sentence=81294, chars=[40,44], words=[6,6]), ImplicitSpan(\"150\", sentence=112785, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BD245\", sentence=81294, chars=[40,44], words=[6,6]), ImplicitSpan(\"150\", sentence=81333, words=[3,3], position=[0]))\n",
      "Part_Attr(Span(\"BD245\", sentence=81294, chars=[40,44], words=[6,6]), ImplicitSpan(\"150\", sentence=112777, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC546B\", sentence=115108, words=[1,3], position=[1]), ImplicitSpan(\"150\", sentence=115016, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC546B\", sentence=115104, words=[0,0], position=[1]), ImplicitSpan(\"150\", sentence=115016, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC546B\", sentence=88242, words=[0,2], position=[1]), ImplicitSpan(\"150\", sentence=115016, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC548B\", sentence=88242, words=[7,9], position=[1]), ImplicitSpan(\"150\", sentence=115016, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC548B\", sentence=88242, words=[8,10], position=[0]), ImplicitSpan(\"150\", sentence=115016, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC547A\", sentence=115109, words=[9,11], position=[0]), ImplicitSpan(\"150\", sentence=115016, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC546\", sentence=88219, chars=[0,4], words=[0,0]), ImplicitSpan(\"150\", sentence=115016, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC547A\", sentence=115109, words=[10,12], position=[1]), ImplicitSpan(\"150\", sentence=115016, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC547A\", sentence=88242, words=[2,4], position=[1]), ImplicitSpan(\"150\", sentence=115016, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"BC547A\", sentence=115103, chars=[85,90], words=[11,11]), ImplicitSpan(\"150\", sentence=115016, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC546B\", sentence=115108, words=[0,2], position=[0]), ImplicitSpan(\"150\", sentence=115016, words=[2,2], position=[0]))\n",
      "Part_Attr(ImplicitSpan(\"BC547A\", sentence=88242, words=[1,3], position=[0]), ImplicitSpan(\"150\", sentence=115016, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"2N3906TAR\", sentence=108933, chars=[0,8], words=[0,0]), ImplicitSpan(\"150\", sentence=109073, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"MMBT3906\", sentence=78572, chars=[33,40], words=[1,1]), ImplicitSpan(\"150\", sentence=109073, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"PZT3906\", sentence=78572, chars=[67,73], words=[2,2]), ImplicitSpan(\"150\", sentence=109073, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"2N3906BU\", sentence=108903, chars=[0,7], words=[0,0]), ImplicitSpan(\"150\", sentence=109073, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"MMBT3906\", sentence=78560, chars=[9,16], words=[2,2]), ImplicitSpan(\"150\", sentence=109073, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"MMBT3906\", sentence=78537, chars=[7,14], words=[1,1]), ImplicitSpan(\"150\", sentence=109073, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"PZT3906\", sentence=78560, chars=[20,26], words=[4,4]), ImplicitSpan(\"150\", sentence=109073, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"2N3906\", sentence=78537, chars=[0,5], words=[0,0]), ImplicitSpan(\"150\", sentence=109073, words=[2,2], position=[0]))\n",
      "Part_Attr(Span(\"PZT3906\", sentence=78537, chars=[16,22], words=[2,2]), ImplicitSpan(\"150\", sentence=109073, words=[2,2], position=[0]))\n"
     ]
    }
   ],
   "source": [
    "for c in train_cands:\n",
    "    if LF_test(c) != 0:\n",
    "        print c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process BatchAnnotatorUDF-32:\n",
      "Traceback (most recent call last):\n",
      "Process BatchAnnotatorUDF-33:\n",
      "Process BatchAnnotatorUDF-34:\n",
      "  File \"/Users/sen/anaconda2/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "    self.run()\n",
      "  File \"/Users/sen/anaconda2/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/Users/sen/anaconda2/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/Users/sen/Documents/Sen/Stanford/Deepdive/snorkel/snorkel/udf.py\", line 154, in run\n",
      "    self.run()\n",
      "    for y in self.apply(x, **self.apply_kwargs):\n",
      "    self.run()\n",
      "  File \"/Users/sen/Documents/Sen/Stanford/Deepdive/snorkel/snorkel/udf.py\", line 154, in run\n",
      "  File \"/Users/sen/Documents/Sen/Stanford/Deepdive/snorkel/snorkel/contrib/fonduer/async_annotations.py\", line 199, in apply\n",
      "  File \"/Users/sen/Documents/Sen/Stanford/Deepdive/snorkel/snorkel/udf.py\", line 154, in run\n",
      "    for y in self.apply(x, **self.apply_kwargs):\n",
      "Process BatchAnnotatorUDF-35:\n",
      "    keys, values = zip(*[(k,v) for k,v in self.anno_generator(candidate) if v != 0])\n",
      "    for y in self.apply(x, **self.apply_kwargs):\n",
      "  File \"/Users/sen/Documents/Sen/Stanford/Deepdive/snorkel/snorkel/contrib/fonduer/async_annotations.py\", line 199, in apply\n",
      "ValueError: need more than 0 values to unpack\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sen/Documents/Sen/Stanford/Deepdive/snorkel/snorkel/contrib/fonduer/async_annotations.py\", line 199, in apply\n",
      "    keys, values = zip(*[(k,v) for k,v in self.anno_generator(candidate) if v != 0])\n",
      "  File \"/Users/sen/anaconda2/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    keys, values = zip(*[(k,v) for k,v in self.anno_generator(candidate) if v != 0])\n",
      "ValueError: need more than 0 values to unpack\n",
      "    self.run()\n",
      "ValueError: need more than 0 values to unpack\n",
      "  File \"/Users/sen/Documents/Sen/Stanford/Deepdive/snorkel/snorkel/udf.py\", line 154, in run\n",
      "    for y in self.apply(x, **self.apply_kwargs):\n",
      "  File \"/Users/sen/Documents/Sen/Stanford/Deepdive/snorkel/snorkel/contrib/fonduer/async_annotations.py\", line 199, in apply\n",
      "    keys, values = zip(*[(k,v) for k,v in self.anno_generator(candidate) if v != 0])\n",
      "ValueError: need more than 0 values to unpack\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying part_attr_label_updates to postgres\n",
      "COPY 0\n",
      "\n",
      "CPU times: user 317 ms, sys: 74.9 ms, total: 392 ms\n",
      "Wall time: 548 ms\n",
      "(2055, 17)\n"
     ]
    }
   ],
   "source": [
    "def LF_test(c):\n",
    "    return 1 if 'to' in get_left_ngrams(c.attr, window=2) else 0\n",
    "stg_temp_lfs3 = [LF_test]\n",
    "labeler = BatchLabelAnnotator(Part_Attr, lfs = stg_temp_lfs3)\n",
    "%time L_train = labeler.apply(split=0, clear=False, update_existing=True, parallelism=4)\n",
    "print L_train.shape\n",
    "L_train3 = L_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2055x17 sparse matrix of type '<type 'numpy.float32'>'\n",
       "\twith 8569 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t-1.0\n",
      "  (0, 7)\t-1.0\n",
      "  (0, 11)\t-1.0\n",
      "  (1, 1)\t-1.0\n",
      "  (1, 3)\t-1.0\n",
      "  (1, 7)\t-1.0\n",
      "  (1, 11)\t-1.0\n",
      "  (1, 13)\t-1.0\n",
      "  (1, 15)\t-1.0\n",
      "  (2, 5)\t1.0\n",
      "  (2, 8)\t1.0\n",
      "  (2, 9)\t1.0\n",
      "  (2, 11)\t-1.0\n",
      "  (2, 12)\t1.0\n",
      "  (2, 14)\t1.0\n",
      "  (2, 16)\t1.0\n",
      "  (3, 0)\t-1.0\n",
      "  (3, 3)\t-1.0\n",
      "  (3, 10)\t-1.0\n",
      "  (3, 11)\t-1.0\n",
      "  (3, 13)\t-1.0\n",
      "  (3, 15)\t-1.0\n",
      "  (4, 3)\t-1.0\n",
      "  (4, 7)\t-1.0\n",
      "  (4, 11)\t-1.0\n",
      "  :\t:\n",
      "  (2050, 11)\t-1.0\n",
      "  (2050, 12)\t1.0\n",
      "  (2050, 13)\t-1.0\n",
      "  (2050, 14)\t1.0\n",
      "  (2050, 15)\t-1.0\n",
      "  (2051, 0)\t-1.0\n",
      "  (2051, 3)\t-1.0\n",
      "  (2051, 7)\t-1.0\n",
      "  (2051, 11)\t-1.0\n",
      "  (2051, 13)\t-1.0\n",
      "  (2051, 15)\t-1.0\n",
      "  (2052, 8)\t1.0\n",
      "  (2052, 11)\t-1.0\n",
      "  (2052, 13)\t-1.0\n",
      "  (2052, 14)\t1.0\n",
      "  (2052, 15)\t-1.0\n",
      "  (2053, 7)\t-1.0\n",
      "  (2053, 11)\t-1.0\n",
      "  (2054, 8)\t1.0\n",
      "  (2054, 9)\t1.0\n",
      "  (2054, 11)\t-1.0\n",
      "  (2054, 12)\t1.0\n",
      "  (2054, 13)\t-1.0\n",
      "  (2054, 14)\t1.0\n",
      "  (2054, 15)\t-1.0\n"
     ]
    }
   ],
   "source": [
    "print L_train3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from snorkel.annotations import LabelAnnotator\n",
    "\n",
    "# labeler = LabelAnnotator(f=stg_temp_lfs)\n",
    "# %time L_train = labeler.apply(split=0, parallelism=PARALLEL)\n",
    "# L_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the returned matrix is a special subclass of the scipy.sparse.csr_matrix class, with some special features which we demonstrate below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Part_Attr(Span(\"BC817-16\", sentence=109098, chars=[0,7], words=[0,0]), ImplicitSpan(\"200\", sentence=108885, words=[0,0], position=[0]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_train.get_candidate(session, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view statistics about the resulting label matrix.\n",
    "* **Coverage** is the fraction of candidates that the labeling function emits a non-zero label for.\n",
    "* **Overlap** is the fraction candidates that the labeling function emits a non-zero label for and that another labeling function emits a non-zero label for.\n",
    "* **Conflict** is the fraction candidates that the labeling function emits a non-zero label for and that another labeling function emits a conflicting non-zero label for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.96 ms, sys: 1.37 ms, total: 6.33 ms\n",
      "Wall time: 5.37 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conflicts</th>\n",
       "      <th>coverage</th>\n",
       "      <th>j</th>\n",
       "      <th>overlaps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_collector_aligned</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042822</td>\n",
       "      <td>0</td>\n",
       "      <td>0.042822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_too_many_numbers_row</th>\n",
       "      <td>0.044769</td>\n",
       "      <td>0.515815</td>\n",
       "      <td>1</td>\n",
       "      <td>0.515815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_temp_on_high_page_num</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210219</td>\n",
       "      <td>2</td>\n",
       "      <td>0.210219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_current_aligned</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.364477</td>\n",
       "      <td>3</td>\n",
       "      <td>0.364477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_complement_left_row</th>\n",
       "      <td>0.019951</td>\n",
       "      <td>0.068127</td>\n",
       "      <td>4</td>\n",
       "      <td>0.068127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_operating_row</th>\n",
       "      <td>0.056934</td>\n",
       "      <td>0.056934</td>\n",
       "      <td>5</td>\n",
       "      <td>0.056934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_typ_row</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002920</td>\n",
       "      <td>6</td>\n",
       "      <td>0.002920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_not_temp_relevant</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.823844</td>\n",
       "      <td>7</td>\n",
       "      <td>0.823844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_temperature_row</th>\n",
       "      <td>0.163504</td>\n",
       "      <td>0.163504</td>\n",
       "      <td>8</td>\n",
       "      <td>0.163504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_tstg_row</th>\n",
       "      <td>0.109976</td>\n",
       "      <td>0.109976</td>\n",
       "      <td>9</td>\n",
       "      <td>0.109976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_test_condition_aligned</th>\n",
       "      <td>0.000973</td>\n",
       "      <td>0.012165</td>\n",
       "      <td>10</td>\n",
       "      <td>0.012165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_temp_outside_table</th>\n",
       "      <td>0.164964</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_storage_row</th>\n",
       "      <td>0.108516</td>\n",
       "      <td>0.108516</td>\n",
       "      <td>12</td>\n",
       "      <td>0.108516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_voltage_row_temp</th>\n",
       "      <td>0.033090</td>\n",
       "      <td>0.237956</td>\n",
       "      <td>13</td>\n",
       "      <td>0.237956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_negative_number_left</th>\n",
       "      <td>0.116302</td>\n",
       "      <td>0.116302</td>\n",
       "      <td>14</td>\n",
       "      <td>0.116302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_voltage_row_part</th>\n",
       "      <td>0.033090</td>\n",
       "      <td>0.237956</td>\n",
       "      <td>15</td>\n",
       "      <td>0.237956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_to_left</th>\n",
       "      <td>0.098297</td>\n",
       "      <td>0.098297</td>\n",
       "      <td>16</td>\n",
       "      <td>0.098297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           conflicts  coverage   j  overlaps\n",
       "LF_collector_aligned        0.000000  0.042822   0  0.042822\n",
       "LF_too_many_numbers_row     0.044769  0.515815   1  0.515815\n",
       "LF_temp_on_high_page_num    0.000000  0.210219   2  0.210219\n",
       "LF_current_aligned          0.000000  0.364477   3  0.364477\n",
       "LF_complement_left_row      0.019951  0.068127   4  0.068127\n",
       "LF_operating_row            0.056934  0.056934   5  0.056934\n",
       "LF_typ_row                  0.000000  0.002920   6  0.002920\n",
       "LF_not_temp_relevant        0.000000  0.823844   7  0.823844\n",
       "LF_temperature_row          0.163504  0.163504   8  0.163504\n",
       "LF_tstg_row                 0.109976  0.109976   9  0.109976\n",
       "LF_test_condition_aligned   0.000973  0.012165  10  0.012165\n",
       "LF_temp_outside_table       0.164964  1.000000  11  1.000000\n",
       "LF_storage_row              0.108516  0.108516  12  0.108516\n",
       "LF_voltage_row_temp         0.033090  0.237956  13  0.237956\n",
       "LF_negative_number_left     0.116302  0.116302  14  0.116302\n",
       "LF_voltage_row_part         0.033090  0.237956  15  0.237956\n",
       "LF_to_left                  0.098297  0.098297  16  0.098297"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time L_train.lf_stats_legacy()\n",
    "# %time L_train.lf_stats(session, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Generative Model\n",
    "\n",
    "Now, we'll train a model of the LFs to estimate their accuracies. Once the model is trained, we can combine the outputs of the LFs into a single, noise-aware training label set for our extractor. Intuitively, we'll model the LFs by observing how they overlap and conflict with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sen/anaconda2/lib/python2.7/site-packages/matplotlib/__init__.py:1401: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.6 s, sys: 104 ms, total: 38.7 s\n",
      "Wall time: 38.7 s\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "\n",
    "gen_model = GenerativeModel()\n",
    "%time gen_model.train(L_train, epochs=500, decay=0.95, step_size=0.1/L_train.shape[0], reg_param=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the generative model to the training candidates to get the noise-aware training label set. We'll refer to these as the training marginals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_marginals = gen_model.marginals(L_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at the distribution of the training marginals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12560c810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_marginals, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the learned accuracy parameters as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.53336266,  0.70146052,  0.59032579,  0.65092272,  0.53893432,\n",
       "        0.51197252,  0.51734911,  0.82547258,  0.4863562 ,  0.50277308,\n",
       "        0.5221416 ,  0.84754375,  0.49998261,  0.60225486,  0.49884864,\n",
       "        0.60282089,  0.50637445])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_model.weights.lf_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Using the Model to Iterate on Labeling Functions\n",
    "\n",
    "Now that we have learned the generative model, we can stop here and use this to potentially debug and/or improve our labeling function set. First, we apply the LFs to our development set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need more than 0 values to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-5d0a5591b404>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mL_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabeler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_existing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/sen/Documents/Sen/Stanford/Deepdive/snorkel/snorkel/contrib/fonduer/async_annotations.pyc\u001b[0m in \u001b[0;36mapply_existing\u001b[0;34m(self, split, key_group, **kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_existing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;34m\"\"\"Alias for apply that emphasizes we are using an existing AnnotatorKey set.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_group\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace_key_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sen/Documents/Sen/Stanford/Deepdive/snorkel/snorkel/contrib/fonduer/async_annotations.pyc\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, split, key_group, replace_key_set, update_existing, storage, ignore_keys, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0msegment_file_blob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegment_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_segment_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mremove_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegment_file_blob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBatchAnnotator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;31m# Insert and update keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sen/Documents/Sen/Stanford/Deepdive/snorkel/snorkel/udf.pyc\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, xs, clear, parallelism, progress_bar, count, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Running UDF...\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparallelism\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mparallelism\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_st\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallelism\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sen/Documents/Sen/Stanford/Deepdive/snorkel/snorkel/udf.pyc\u001b[0m in \u001b[0;36mapply_st\u001b[0;34m(self, xs, progress_bar, count, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;31m# Apply UDF and add results to the session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0;31m# Uf UDF has a reduce step, this will take care of the insert; else add to session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sen/Documents/Sen/Stanford/Deepdive/snorkel/snorkel/contrib/fonduer/async_annotations.pyc\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, batch_range, table_name, split, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;31m# Runs the actual extraction function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manno_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray_tsv_escape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray_tsv_escape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need more than 0 values to unpack"
     ]
    }
   ],
   "source": [
    "L_dev = labeler.apply_existing(split=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we get the score of the generative model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = gen_model.score(session, L_dev, L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L_dev.lf_stats(session, L_gold_dev, gen_model.weights.lf_accuracy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Generative Model Performance\n",
    "\n",
    "At this point, we should be getting an F1 score of around 0.6 to 0.7 on the development set, which is pretty good! However, we should be very careful in interpreting this. Since we developed our labeling functions using this development set as a guide, and our generative model is composed of these labeling functions, we expect it to score very well here!\n",
    "\n",
    "In fact, it is probably somewhat overfit to this set. However this is fine, since in the next tutorial, we'll train a more powerful end extraction model which will generalize beyond the development set, and which we will evaluate on a blind test set (i.e. one we never looked at during development).\n",
    "\n",
    "## Part 5: Training our End Extraction Model\n",
    "\n",
    "Now, we'll use the noisy training labels we generated in the last part to train our end extraction model. For this tutorial, we will be training a simple - but fairly effective - logistic regression model. More generally, however, Snorkel plugs in with many ML libraries including [TensorFlow](https://www.tensorflow.org/), making it easy to use almost any state-of-the-art model as the end extractor!\n",
    "\n",
    "### Training the Discriminative Model\n",
    "We use the training marginals to train a discriminative model that classifies each Candidate as a true or false mention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import SparseLogisticRegression\n",
    "\n",
    "disc_model = SparseLogisticRegression()\n",
    "%time disc_model.train(F_train, train_marginals, n_epochs=200, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on the Test Set\n",
    "In this final section, we'll get the score we've been after: the performance of the extraction model on the blind test set (split 2). First, we load the test set labels and gold candidates from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Now, we score using the discriminitive model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = disc_model.score(session, F_test, L_gold_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hardware_utils import entity_level_f1\n",
    "import os\n",
    "\n",
    "import cPickle as pickle\n",
    "pickle_file = os.environ['SNORKELHOME'] + '/tutorials/fonduer/hardware/data/parts_by_doc_dict.pkl'\n",
    "with open(pickle_file, 'r') as f:\n",
    "    parts_by_doc = pickle.load(f)\n",
    "\n",
    "# gold_file = os.environ['SNORKELHOME'] + '/tutorials/fonduer/hardware/data/hardware_tutorial_gold.csv'\n",
    "# corpus = session.query(Corpus).filter(Corpus.name == 'Hardware Dev').one()\n",
    "# %time (TP, FP, FN) = entity_level_f1(tp.union(fp), gold_file, ATTRIBUTE, test_docs, parts_by_doc=None)\n",
    "%time (TP, FP, FN) = entity_level_f1(tp.union(fp), gold_file, ATTRIBUTE, test_docs, parts_by_doc=parts_by_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fn_cand = fn_list[20]\n",
    "\n",
    "from hardware_utils import part_error_analysis\n",
    "part_error_analysis(fn_cand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
