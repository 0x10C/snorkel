{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro. to Snorkel: Extracting Spouse Relations from the News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part V: Training our End Extraction Model\n",
    "\n",
    "In this final section of the tutorial, we'll use the noisy training labels we generated in the last tutorial part to train our end extraction model.\n",
    "\n",
    "For this tutorial, we will be training a simple - but fairly effective - logistic regression model.  More generally, however, Snorkel plugs in with many ML libraries including [TensorFlow](https://www.tensorflow.org/), making it easy to use almost any state-of-the-art model as the end extractor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "# TO USE A DATABASE OTHER THAN SQLITE, USE THIS LINE\n",
    "# Note that this is necessary for parallel execution amongst other things...\n",
    "# os.environ['SNORKELDB'] = 'postgres:///snorkel-intro'\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat our definition of the `Spouse` `Candidate` subclass, and load the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "Spouse = candidate_subclass('Spouse', ['person1', 'person2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training the Discriminative Model\n",
    "We use the training marginals to train a discriminative model that classifies each `Candidate` as a true or false mention. We'll use a random hyperparameter search, evaluated on the development set labels, to find the best hyperparameters for our model. To run a hyperparameter search, we need labels for a development set. If they aren't already available, we can manually create labels using the Viewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_marginals\n",
    "\n",
    "train_marginals = load_marginals(session, split=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_cands = session.query(Spouse).filter(Spouse.split == 0).order_by(Spouse.id).all()\n",
    "dev_cands   = session.query(Spouse).filter(Spouse.split == 1).order_by(Spouse.id).all()\n",
    "test_cands  = session.query(Spouse).filter(Spouse.split == 2).order_by(Spouse.id).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "\n",
    "L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1, load_as_array=True, zero_one=True)\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold', split=2, zero_one=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(train_cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[reRNN] Dimension=100  LR=0.001\n",
      "[reRNN] Begin preprocessing\n",
      "[reRNN] Loaded 2754 candidates for evaluation\n",
      "[reRNN] Preprocessing done (6.09s)\n",
      "[reRNN] Training model\n",
      "[reRNN] #examples=17272  #epochs=50  batch size=256\n",
      "[reRNN] Epoch 0 (56.82s)\tAverage loss=0.558467\tDev F1=17.92\n",
      "[reRNN] Epoch 1 (109.72s)\tAverage loss=0.484166\tDev F1=37.27\n",
      "[reRNN] Epoch 2 (162.73s)\tAverage loss=0.473554\tDev F1=36.04\n",
      "[reRNN] Epoch 3 (215.70s)\tAverage loss=0.471496\tDev F1=37.39\n",
      "[reRNN] Epoch 4 (268.65s)\tAverage loss=0.470444\tDev F1=39.29\n",
      "[reRNN] Epoch 5 (321.58s)\tAverage loss=0.469278\tDev F1=40.25\n",
      "[reRNN] Epoch 6 (374.47s)\tAverage loss=0.468272\tDev F1=39.02\n",
      "[reRNN] Epoch 7 (427.36s)\tAverage loss=0.467444\tDev F1=39.51\n",
      "[reRNN] Epoch 8 (480.40s)\tAverage loss=0.466709\tDev F1=42.46\n",
      "[reRNN] Epoch 9 (533.63s)\tAverage loss=0.466255\tDev F1=42.13\n",
      "[reRNN] Epoch 10 (586.59s)\tAverage loss=0.466110\tDev F1=43.94\n",
      "[reRNN] Epoch 11 (639.59s)\tAverage loss=0.465791\tDev F1=43.16\n",
      "[reRNN] Epoch 12 (692.50s)\tAverage loss=0.465056\tDev F1=43.35\n",
      "[reRNN] Epoch 13 (745.54s)\tAverage loss=0.464991\tDev F1=42.80\n",
      "[reRNN] Epoch 14 (798.81s)\tAverage loss=0.464841\tDev F1=42.75\n",
      "[reRNN] Epoch 15 (851.86s)\tAverage loss=0.465160\tDev F1=45.12\n",
      "[reRNN] Epoch 16 (904.86s)\tAverage loss=0.466254\tDev F1=42.64\n",
      "[reRNN] Epoch 17 (957.98s)\tAverage loss=0.464852\tDev F1=43.36\n",
      "[reRNN] Epoch 18 (1010.97s)\tAverage loss=0.464314\tDev F1=43.82\n",
      "[reRNN] Epoch 19 (1063.86s)\tAverage loss=0.463817\tDev F1=44.49\n",
      "[reRNN] Epoch 20 (1116.92s)\tAverage loss=0.463739\tDev F1=44.63\n",
      "[reRNN] Epoch 21 (1169.90s)\tAverage loss=0.463508\tDev F1=44.84\n",
      "[reRNN] Epoch 22 (1222.85s)\tAverage loss=0.463529\tDev F1=46.33\n",
      "[reRNN] Epoch 23 (1276.17s)\tAverage loss=0.463594\tDev F1=44.78\n",
      "[reRNN] Epoch 24 (1329.04s)\tAverage loss=0.463378\tDev F1=42.80\n",
      "[reRNN] Epoch 25 (1381.99s)\tAverage loss=0.463119\tDev F1=44.03\n",
      "[reRNN] Epoch 26 (1435.03s)\tAverage loss=0.462842\tDev F1=42.97\n",
      "[reRNN] Epoch 27 (1487.99s)\tAverage loss=0.462741\tDev F1=43.06\n",
      "[reRNN] Epoch 28 (1540.93s)\tAverage loss=0.462804\tDev F1=44.40\n",
      "[reRNN] Epoch 29 (1593.89s)\tAverage loss=0.462803\tDev F1=44.44\n",
      "[reRNN] Epoch 30 (1647.70s)\tAverage loss=0.462652\tDev F1=45.29\n",
      "[reRNN] Epoch 31 (1700.91s)\tAverage loss=0.462667\tDev F1=47.44\n",
      "[reRNN] Epoch 32 (1754.16s)\tAverage loss=0.462787\tDev F1=46.44\n",
      "[reRNN] Epoch 33 (1806.98s)\tAverage loss=0.463143\tDev F1=45.54\n",
      "[reRNN] Epoch 34 (1860.00s)\tAverage loss=0.462674\tDev F1=45.64\n",
      "[reRNN] Epoch 35 (1913.04s)\tAverage loss=0.462471\tDev F1=45.74\n",
      "[reRNN] Epoch 36 (1965.93s)\tAverage loss=0.462305\tDev F1=46.44\n",
      "[reRNN] Epoch 37 (2018.88s)\tAverage loss=0.462306\tDev F1=43.50\n",
      "[reRNN] Epoch 38 (2071.98s)\tAverage loss=0.462369\tDev F1=42.46\n",
      "[reRNN] Epoch 39 (2125.16s)\tAverage loss=0.462151\tDev F1=43.78\n",
      "[reRNN] Epoch 40 (2177.99s)\tAverage loss=0.461922\tDev F1=44.78\n",
      "[reRNN] Epoch 41 (2231.09s)\tAverage loss=0.462022\tDev F1=44.92\n",
      "[reRNN] Epoch 42 (2284.06s)\tAverage loss=0.461894\tDev F1=45.37\n",
      "[reRNN] Epoch 43 (2337.16s)\tAverage loss=0.461807\tDev F1=47.47\n",
      "[reRNN] Epoch 44 (2390.18s)\tAverage loss=0.462330\tDev F1=44.58\n",
      "[reRNN] Epoch 45 (2443.19s)\tAverage loss=0.461986\tDev F1=44.78\n",
      "[reRNN] Epoch 46 (2497.08s)\tAverage loss=0.461654\tDev F1=43.39\n",
      "[reRNN] Epoch 47 (2551.36s)\tAverage loss=0.461701\tDev F1=42.68\n",
      "[reRNN] Epoch 48 (2605.35s)\tAverage loss=0.461778\tDev F1=42.06\n",
      "[reRNN] Epoch 49 (2658.38s)\tAverage loss=0.461650\tDev F1=45.81\n",
      "[reRNN] Training done (2661.13s)\n"
     ]
    }
   ],
   "source": [
    "from snorkel.contrib.rnn import reRNN\n",
    "\n",
    "train_kwargs = {\n",
    "    'lr':         0.001,\n",
    "    'dim':        100,\n",
    "    'n_epochs':   50,\n",
    "    'dropout':    0.5,\n",
    "  #  'rebalance':  0.5,\n",
    "    'print_freq': 1,\n",
    "    'max_sentence_length': 100\n",
    "}\n",
    "\n",
    "lstm = reRNN(seed=1701, n_threads=None)\n",
    "lstm.train(train_cands, train_marginals, dev_candidates=dev_cands, dev_labels=L_gold_dev, **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.461\n",
      "Neg. class accuracy: 0.958\n",
      "Precision            0.464\n",
      "Recall               0.461\n",
      "F1                   0.462\n",
      "----------------------------------------\n",
      "TP: 89 | FP: 103 | TN: 2368 | FN: 104\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tp, fp, tn, fn = lstm.score(session, test_cands, L_gold_test, b=.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[reRNN] Dimension=100  LR=0.0001\n",
      "[reRNN] Begin preprocessing\n",
      "[reRNN] Loaded 2754 candidates for evaluation\n",
      "[reRNN] Preprocessing done (9.82s)\n",
      "[reRNN] Training model\n",
      "[reRNN] #examples=17272  #epochs=50  batch size=256\n",
      "[reRNN] Epoch 0 (99.73s)\tAverage loss=0.647129\tDev F1=0.00\n",
      "[reRNN] Epoch 1 (189.94s)\tAverage loss=0.562570\tDev F1=0.00\n",
      "[reRNN] Epoch 2 (279.77s)\tAverage loss=0.548148\tDev F1=0.00\n",
      "[reRNN] Epoch 3 (487.53s)\tAverage loss=0.528396\tDev F1=0.00\n",
      "[reRNN] Epoch 4 (1167.04s)\tAverage loss=0.508069\tDev F1=13.43\n",
      "[reRNN] Epoch 5 (11983.05s)\tAverage loss=0.492076\tDev F1=29.55\n",
      "[reRNN] Epoch 6 (12076.61s)\tAverage loss=0.483190\tDev F1=31.96\n",
      "[reRNN] Epoch 7 (12166.91s)\tAverage loss=0.478926\tDev F1=33.33\n",
      "[reRNN] Epoch 8 (12256.24s)\tAverage loss=0.476748\tDev F1=33.18\n",
      "[reRNN] Epoch 9 (12345.06s)\tAverage loss=0.475008\tDev F1=33.18\n",
      "[reRNN] Epoch 10 (12436.70s)\tAverage loss=0.473900\tDev F1=33.85\n",
      "[reRNN] Epoch 11 (12529.63s)\tAverage loss=0.472954\tDev F1=34.07\n",
      "[reRNN] Epoch 12 (12624.44s)\tAverage loss=0.472208\tDev F1=33.41\n",
      "[reRNN] Epoch 13 (12714.70s)\tAverage loss=0.471938\tDev F1=33.19\n",
      "[reRNN] Epoch 14 (12805.19s)\tAverage loss=0.471292\tDev F1=33.93\n",
      "[reRNN] Epoch 15 (12895.51s)\tAverage loss=0.470840\tDev F1=32.97\n",
      "[reRNN] Epoch 16 (12963.95s)\tAverage loss=0.470478\tDev F1=32.54\n",
      "[reRNN] Epoch 17 (13021.18s)\tAverage loss=0.469999\tDev F1=32.61\n",
      "[reRNN] Epoch 18 (13074.32s)\tAverage loss=0.469885\tDev F1=34.32\n",
      "[reRNN] Epoch 19 (13145.40s)\tAverage loss=0.469513\tDev F1=34.27\n",
      "[reRNN] Epoch 20 (13237.60s)\tAverage loss=0.469442\tDev F1=34.63\n",
      "[reRNN] Epoch 21 (13327.49s)\tAverage loss=0.469111\tDev F1=34.27\n",
      "[reRNN] Epoch 22 (13416.33s)\tAverage loss=0.468777\tDev F1=34.03\n",
      "[reRNN] Epoch 23 (13505.24s)\tAverage loss=0.468653\tDev F1=34.93\n",
      "[reRNN] Epoch 24 (13595.05s)\tAverage loss=0.468704\tDev F1=34.38\n",
      "[reRNN] Epoch 25 (13690.43s)\tAverage loss=0.468504\tDev F1=35.22\n",
      "[reRNN] Epoch 26 (13786.75s)\tAverage loss=0.468354\tDev F1=34.66\n",
      "[reRNN] Epoch 27 (13879.38s)\tAverage loss=0.468223\tDev F1=35.17\n",
      "[reRNN] Epoch 28 (13977.91s)\tAverage loss=0.468118\tDev F1=34.41\n",
      "[reRNN] Epoch 29 (14073.90s)\tAverage loss=0.467852\tDev F1=34.69\n",
      "[reRNN] Epoch 30 (14170.00s)\tAverage loss=0.467727\tDev F1=34.35\n",
      "[reRNN] Epoch 31 (14266.01s)\tAverage loss=0.467506\tDev F1=34.50\n",
      "[reRNN] Epoch 32 (14362.54s)\tAverage loss=0.467348\tDev F1=34.57\n",
      "[reRNN] Epoch 33 (14456.46s)\tAverage loss=0.467293\tDev F1=33.69\n",
      "[reRNN] Epoch 34 (14551.35s)\tAverage loss=0.467261\tDev F1=34.42\n",
      "[reRNN] Epoch 35 (14644.87s)\tAverage loss=0.467030\tDev F1=33.83\n",
      "[reRNN] Epoch 36 (14737.76s)\tAverage loss=0.466835\tDev F1=33.40\n",
      "[reRNN] Epoch 37 (14835.47s)\tAverage loss=0.466766\tDev F1=34.05\n",
      "[reRNN] Epoch 38 (14930.19s)\tAverage loss=0.466837\tDev F1=33.47\n",
      "[reRNN] Epoch 39 (15024.22s)\tAverage loss=0.466560\tDev F1=33.19\n",
      "[reRNN] Epoch 40 (15118.79s)\tAverage loss=0.466405\tDev F1=33.26\n",
      "[reRNN] Epoch 41 (15212.83s)\tAverage loss=0.466361\tDev F1=33.40\n",
      "[reRNN] Epoch 42 (15306.87s)\tAverage loss=0.466282\tDev F1=32.57\n",
      "[reRNN] Epoch 43 (15401.93s)\tAverage loss=0.466188\tDev F1=33.47\n",
      "[reRNN] Epoch 44 (15496.01s)\tAverage loss=0.466315\tDev F1=33.54\n",
      "[reRNN] Epoch 45 (15589.59s)\tAverage loss=0.466132\tDev F1=33.40\n",
      "[reRNN] Epoch 46 (15683.60s)\tAverage loss=0.466050\tDev F1=33.81\n",
      "[reRNN] Epoch 47 (15775.61s)\tAverage loss=0.465966\tDev F1=33.74\n",
      "[reRNN] Epoch 48 (15865.88s)\tAverage loss=0.465939\tDev F1=33.47\n",
      "[reRNN] Epoch 49 (15937.01s)\tAverage loss=0.465860\tDev F1=32.65\n",
      "[reRNN] Training done (15939.98s)\n"
     ]
    }
   ],
   "source": [
    "from snorkel.contrib.rnn import reRNN\n",
    "\n",
    "train_kwargs = {\n",
    "    'lr':         0.0001,\n",
    "    'dim':        100,\n",
    "    'n_epochs':   50,\n",
    "    'dropout':    0.5,\n",
    "  #  'rebalance':  0.5,\n",
    "    'print_freq': 1,\n",
    "    'max_sentence_length': 100\n",
    "}\n",
    "\n",
    "lstm = reRNN(seed=1701, n_threads=3)\n",
    "lstm.train(train_cands, train_marginals, dev_candidates=dev_cands, dev_labels=L_gold_dev, **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.contrib.rnn import reRNN\n",
    "\n",
    "train_kwargs = {\n",
    "    'lr':         0.005,\n",
    "    'dim':        100,\n",
    "    'n_epochs':   10,\n",
    "    'dropout':    0.5,\n",
    "    'rebalance':  0.25,\n",
    "    'print_freq': 5\n",
    "}\n",
    "\n",
    "lstm = reRNN(seed=1701, n_threads=None)\n",
    "lstm.train(train_cands, train_marginals, dev_candidates=dev_cands, dev_labels=L_gold_dev, **train_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up and run the hyperparameter search, training our model with different hyperparamters and picking the best model configuration to keep. We'll set the random seed to maintain reproducibility.\n",
    "\n",
    "Note that we are fitting our model's parameters to the training set generated by our labeling functions, while we are picking hyperparamters with respect to score over the development set labels which we created by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.learning.utils import (\n",
    "    MentionScorer, RandomSearch, ListParameter, RangeParameter\n",
    ")\n",
    "\n",
    "# Searching over learning rate\n",
    "rate_param = RangeParameter('lr', 1e-6, 1e-2, step=1, log_base=10)\n",
    "l1_param  = RangeParameter('l1_penalty', 1e-6, 1e-2, step=1, log_base=10)\n",
    "l2_param  = RangeParameter('l2_penalty', 1e-6, 1e-2, step=1, log_base=10)\n",
    "\n",
    "searcher = RandomSearch(session, disc_model, F_train, train_marginals, [rate_param, l1_param, l2_param], n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load in our dev set labels. We will pick the optimal result from the hyperparameter search by testing against these labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the hyperparameter search / train the end extraction model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1701)\n",
    "searcher.fit(F_dev, L_gold_dev, n_epochs=50, rebalance=0.5, print_freq=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note that to train a model without tuning any hyperparameters (at your own risk) just use the `train` method of the discriminative model. For instance, to train with 20 epochs and a learning rate of 0.001, you could run:_\n",
    "```\n",
    "disc_model.train(F_train, train_marginals, n_epochs=20, lr=0.001)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can analyze the learned model by examining the weights. For example, we can print out the features with the highest weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w, _ = disc_model.get_weights()\n",
    "largest_idxs = reversed(np.argsort(np.abs(w))[-5:])\n",
    "for i in largest_idxs:\n",
    "    print('Feature: {0: <70}Weight: {1:.6f}'.format(F_train.get_key(session, i).name, w[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluating on the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last section of the tutorial, we'll get the score we've been after: the performance of the extraction model on the blind test set (`split` 2). First, we load the test set labels and gold candidates we made in Part III."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we get the precision, recall, and F1 score from the discriminative model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p, r, f1 = disc_model.score(F_test, L_gold_test)\n",
    "print(\"Prec: {0:.3f}, Recall: {1:.3f}, F1 Score: {2:.3f}\".format(p, r, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the candidates returned in sets (true positives, false positives, true negatives, false negatives) as well as a more detailed score report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = disc_model.error_analysis(session, F_test, L_gold_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if this is the final test set that you will be reporting final numbers on, to avoid biasing results you should not inspect results.  However you can run the model on your _development set_ and, as we did in the previous part with the generative labeling function model, inspect examples to do error analysis.\n",
    "\n",
    "##### More importantly, you completed the introduction to Snorkel! Give yourself a pat on the back!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
