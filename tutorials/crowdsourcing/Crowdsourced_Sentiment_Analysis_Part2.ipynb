{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Crowdsourced Sentiment Analysis with Snorkel - Training an ML Model with Snorkel for Sentiment Analysis over Unseen Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [Part I](Crowdsourced_Sentiment_Analysis_Part1.ipynb) of the tutorial we saw how `Snorkel's` generative model can be used to resolve conflicts in crowdsourced answers for a sentiment analysis task. In that part we assumed that we have crowd labels for all our tweets. \n",
    "\n",
    "In this second part, we will show how the output of `Snorkel's` generative model can be used to provide the necessary labeled data for training an LSTM that takes as input a tweet **for which no crowd labels are available** and predicts its sentiment. To emulate the above we split our dataset in two parts: one with tweets for which crowd labels are available and one with tweets for which crowd labels are hidden from Snorkel.\n",
    "\n",
    "\n",
    "The following tutorial is broken up into four parts, each covering a step in the pipeline:\n",
    "1. Load files from Part I\n",
    "2. Load and split data\n",
    "3. Train an ML model with Snorkel\n",
    "4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Files from Part I\n",
    "\n",
    "We first load certain dataframes and pickled files from Part I. These files are required in the subsequent steps. For more details on how the files were generated please check [Part I](Crowdsourced_Sentiment_Analysis_Part1.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize Spark Environment and Spark SQL\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Snorkel Crowdsourcing Demo\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load dataframes from parquet files\n",
    "worker_labels = spark.read.parquet(\"data/worker_labels.parquet\")\n",
    "gold_answers = spark.read.parquet(\"data/gold_answers.parquet\")\n",
    "\n",
    "# Load maps\n",
    "import pickle\n",
    "task2ObjMap = pickle.load( open( \"data/task2ObjMap.pkl\", \"rb\" ) )\n",
    "obj2TaskMap = pickle.load( open( \"data/obj2TaskMap.pkl\", \"rb\" ) )\n",
    "worker2LFMap = pickle.load( open( \"data/worker2LFMap.pkl\", \"rb\" ) )\n",
    "lf2WorkerMap = pickle.load( open( \"data/lf2WorkerMap.pkl\", \"rb\" ) )\n",
    "taskLabels = pickle.load( open( \"data/taskLabels.pkl\", \"rb\" ) )\n",
    "taskLabelsMap = pickle.load( open( \"data/taskLabelsMap.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Split Tweets to Test and Train Sets.\n",
    "\n",
    "In this part we show how to load the tweets to `Snorkel's` backend database and split them into a training and testing set.\n",
    "\n",
    "The first task we need to perfom is load the raw tweet bodies into Snorkel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load tweet bodies in a dataframe\n",
    "raw_crowd_answers = spark.read.format(\"csv\").option(\"header\", \"true\").csv(\"data/weather-non-agg-DFE.csv\")\n",
    "tweet_bodies = raw_crowd_answers.select(\"tweet_id\", \"tweet_body\").orderBy(\"tweet_id\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize a Snorkel session\n",
    "from snorkel import SnorkelSession\n",
    "from snorkel.models import candidate_subclass\n",
    "from snorkel.contrib.models.context import RawText\n",
    "\n",
    "session = SnorkelSession()\n",
    "\n",
    "# Define a tweet candidate\n",
    "Tweet = candidate_subclass('Tweet', ['tweet_body'], values=taskLabels)\n",
    "\n",
    "# Generate and store the tweet candidates to be classified\n",
    "\n",
    "# We split the tweets in two sets: one for which the crowd \n",
    "# labels are not available to Snorkel (test) and one for which we assume\n",
    "# crowd labels are obtained (to be used for training)\n",
    "total_tweets = tweet_bodies.count()\n",
    "\n",
    "# Take the first 10% of tweets as a test set\n",
    "# The remaining 90% will be used for training\n",
    "test_split = total_tweets*0.1\n",
    "\n",
    "train_tweets = []\n",
    "test_tweets = []\n",
    "\n",
    "count = 0\n",
    "for tweet_entry in tweet_bodies.collect():\n",
    "    tweet_text = RawText(stable_id=tweet_entry.tweet_id, name=tweet_entry.tweet_id, text=tweet_entry.tweet_body)\n",
    "    if count > test_split:\n",
    "        tweet = Tweet(tweet_body=tweet_text, split=0)\n",
    "        session.add(tweet)\n",
    "        train_tweets.append(tweet_entry.tweet_id)\n",
    "    else:\n",
    "        tweet = Tweet(tweet_body=tweet_text, split=1)\n",
    "        session.add(tweet)\n",
    "        test_tweets.append(tweet_entry.tweet_id)\n",
    "    count += 1\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train An ML Model with Snorkel\n",
    "\n",
    "Now we show how to train an end-to-end model with `Snorkel` where crowdsourced labels are used to generate training data. \n",
    "\n",
    "First we need to train `Snorkel's` generative model over the tweets in the training set. These are the ones for which noisy crowd labels are available.\n",
    "\n",
    "To this end, we generate the labeling matrix for Snorkel and train the corresponding generative model. Details on these steps are provided in [Part I](Crowdsourced_Sentiment_Analysis_Part2.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The labeling matrix is represented\n",
    "# as a sparse scipy array\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "# Initialize dimensions of labeling matrix\n",
    "objects = len(train_tweets)\n",
    "LFs = worker_labels.select(\"worker_id\").distinct().count()\n",
    "\n",
    "# Initialize empty labeling matrix\n",
    "L_train = sparse.lil_matrix((objects, LFs), dtype=np.int64)\n",
    "\n",
    "# Iterate over crowdsourced labels and populate labeling matrix\n",
    "for assigned_label in worker_labels.select(\"worker_id\", \"task_id\", \"label\").collect():\n",
    "    if assigned_label.task_id in train_tweets:\n",
    "        oid = task2ObjMap[assigned_label.task_id] - len(test_tweets)\n",
    "        LFid = worker2LFMap[assigned_label.worker_id]\n",
    "        label = taskLabelsMap[assigned_label.label]\n",
    "        L_train[oid, LFid] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the generated labeling matrix to train the generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/matplotlib/__init__.py:1401: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from snorkel.learning.gen_learning import GenerativeModel\n",
    "\n",
    "# Initialize Snorkel's generative model for\n",
    "# learning the different worker accuracies.\n",
    "gen_model = GenerativeModel(lf_propensity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred cardinality: 5\n"
     ]
    }
   ],
   "source": [
    "# Train the generative model\n",
    "gen_model.train(\n",
    "    L_train,\n",
    "    reg_type=2,\n",
    "    reg_param=0.1,\n",
    "    epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to train a Deep Network (an LSTM) using the output of `Snorkel's` generative model as training data. The LSTM will be used to predict the sentiment of tweets for which no crowdlabels are available. The corresponding model takes as input: (i) the generated features, and (ii) the marginals estimated by `Snorkel's` generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[textRNN] Dimension=100  LR=0.01\n",
      "[textRNN] Begin preprocessing\n",
      "899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thodoris/Documents/Research/snorkel/snorkel/contrib/rnn/rnn_base.py:53: UserWarning: Candidate 16 has argument past max length for model:\t[arg ends at index 30; max len 30]\n",
      "  warnings.warn('\\t'.join([w.format(i), info]))\n",
      "/Users/thodoris/Documents/Research/snorkel/snorkel/contrib/rnn/rnn_base.py:53: UserWarning: Candidate 683 has argument past max length for model:\t[arg ends at index 30; max len 30]\n",
      "  warnings.warn('\\t'.join([w.format(i), info]))\n",
      "/Users/thodoris/Documents/Research/snorkel/snorkel/contrib/rnn/rnn_base.py:53: UserWarning: Candidate 809 has argument past max length for model:\t[arg ends at index 30; max len 30]\n",
      "  warnings.warn('\\t'.join([w.format(i), info]))\n",
      "/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[textRNN] Preprocessing done (4.55s)\n",
      "[textRNN] Training model\n",
      "[textRNN] #examples=899  #epochs=200  batch size=256\n",
      "[textRNN] Epoch 0 (2.41s)\tAverage loss=1.527904\n",
      "[textRNN] Epoch 5 (10.57s)\tAverage loss=0.255238\n",
      "[textRNN] Epoch 10 (19.79s)\tAverage loss=0.122817\n"
     ]
    }
   ],
   "source": [
    "train_marginals = gen_model.marginals(L_train)\n",
    "\n",
    "from snorkel.contrib.rnn import textRNN\n",
    "\n",
    "train_kwargs = {\n",
    "    'lr':         0.01,\n",
    "    'dim':        100,\n",
    "    'n_epochs':   200,\n",
    "    'dropout':    0.2,\n",
    "    'rebalance':  0.01,\n",
    "    'print_freq': 5\n",
    "}\n",
    "\n",
    "lstm = textRNN(seed=1701, n_threads=None)\n",
    "train_cands = session.query(Tweet).filter(Tweet.split == 0).order_by(Tweet.id).all()\n",
    "\n",
    "lstm.train(train_cands, train_marginals, **train_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluation\n",
    "\n",
    "Finally, we evaluate the performance of the trained LSTM `Snorkel` model against the groundtruth labels of the tweets in the test set. We assign the final label of each tweet to be the MAP assignment given the marginal distribution returned by the Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get MAP assignment for each task\n",
    "#test_marginals = disc_model_sparse.marginals(F_test)\n",
    "\n",
    "\n",
    "test_cands = session.query(Tweet).filter(Tweet.split == 1).order_by(Tweet.id).all()\n",
    "test_marginals = lstm.marginals(test_cands)\n",
    "\n",
    "task_map_assignment = np.argmax(test_marginals, axis=1)\n",
    "inferedLabels = {}\n",
    "for i in range(len(task_map_assignment)):\n",
    "    inferedLabels[obj2TaskMap[i]] =  taskLabels[task_map_assignment[i]+1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "errors = 0\n",
    "total = 0\n",
    "verbose = False\n",
    "for trueLabel in gold_answers.select(\"tweet_id\",\"sentiment\",\"tweet_body\").collect():\n",
    "    if trueLabel.tweet_id in inferedLabels:\n",
    "        total += 1.0\n",
    "        if trueLabel.sentiment != inferedLabels[trueLabel.tweet_id]:\n",
    "            errors += 1\n",
    "            if verbose:\n",
    "                print '*** Error ***'\n",
    "                print 'Original tweet: '+trueLabel.tweet_body\n",
    "                print 'Groundtruth label: '+trueLabel.sentiment\n",
    "                print 'Snorkel label: '+inferedLabels[trueLabel.tweet_id]\n",
    "                print '\\n'\n",
    "print '\\n*** Overall Performance Statistics ***'\n",
    "print 'Wrongly infered labels: '+str(errors)+' out of '+str(total)\n",
    "print 'Accuracy of Snorkel''s model = ', (total-errors)/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Take-away**: The trained discriminative model can be used to infer the sentiment of tweets without obtaining labels from human contributors. \n",
    "**Disclaimer**: The LSTM used above is one of the simplest models and is trained over relatively few training examples. Nonetheless, we see that its accuracy is comparable to that of humans, which is [expected to be around 80% for binary sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
