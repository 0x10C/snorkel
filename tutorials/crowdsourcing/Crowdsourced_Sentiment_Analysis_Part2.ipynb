{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Crowdsourced Sentiment Analysis with Snorkel - Training an ML Model with Snorkel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [Part I](Crowdsourced_Sentiment_Analysis_Part1.ipynb) of the tutorial we saw how `Snorkel's` generative model can be used to resolve conflicts in crowdsourced answers for a sentiment analysis task. In this second part, we will show how the the output of `Snorkel's` generative model can be used to provide the necessary labeled data for training a Logistic Regression model that takes as input a tweet and predicts the associated sentiment. The following tutorial is broken up into four parts, each covering a step in the pipeline:\n",
    "1. Load files from Part I\n",
    "2. Train Snorkel's generative model\n",
    "3. Featurize tweets and train a Logistic regression model with Snorkel\n",
    "4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Files from Part I\n",
    "\n",
    "We first load certain dataframes and pickled files from Part I. These files are required in the subsequent steps. For more details on how the files were generated please check [Part I](Crowdsourced_Sentiment_Analysis_Part1.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize Spark Environment and Spark SQL\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Snorkel Crowdsourcing Demo\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load dataframes from parquet files\n",
    "worker_labels = spark.read.parquet(\"data/worker_labels.parquet\")\n",
    "gold_answers = spark.read.parquet(\"data/gold_answers.parquet\")\n",
    "\n",
    "# Load maps\n",
    "import pickle\n",
    "task2ObjMap = pickle.load( open( \"data/task2ObjMap.pkl\", \"rb\" ) )\n",
    "obj2TaskMap = pickle.load( open( \"data/obj2TaskMap.pkl\", \"rb\" ) )\n",
    "worker2LFMap = pickle.load( open( \"data/worker2LFMap.pkl\", \"rb\" ) )\n",
    "lf2WorkerMap = pickle.load( open( \"data/lf2WorkerMap.pkl\", \"rb\" ) )\n",
    "taskLabels = pickle.load( open( \"data/taskLabels.pkl\", \"rb\" ) )\n",
    "taskLabelsMap = pickle.load( open( \"data/taskLabelsMap.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train Snorkel's Generative Model\n",
    "\n",
    "We now generate the labeling matrix for Snorkel and train the corresponding generative model. Details on these steps are provided in [Part I](Crowdsourced_Sentiment_Analysis_Part2.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The labeling matrix is represented\n",
    "# as a sparse scipy array\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "# Initialize dimensions of labeling matrix\n",
    "objects = worker_labels.select(\"task_id\").distinct().count()\n",
    "LFs = worker_labels.select(\"worker_id\").distinct().count()\n",
    "\n",
    "# Initialize empty labeling matrix\n",
    "L = sparse.lil_matrix((objects, LFs), dtype=np.int64)\n",
    "\n",
    "# Iterate over crowdsourced labels and populate labeling matrix\n",
    "for assigned_label in worker_labels.select(\"worker_id\", \"task_id\", \"label\").collect():\n",
    "    oid = task2ObjMap[assigned_label.task_id]\n",
    "    LFid = worker2LFMap[assigned_label.worker_id]\n",
    "    label = taskLabelsMap[assigned_label.label]\n",
    "    L[oid, LFid] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/matplotlib/__init__.py:1401: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from snorkel.learning.gen_learning import GenerativeModel\n",
    "\n",
    "# Initialize Snorkel's generative model for\n",
    "# learning the different worker accuracies.\n",
    "gen_model = GenerativeModel(lf_propensity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred cardinality: 5\n"
     ]
    }
   ],
   "source": [
    "# Train the generative model\n",
    "gen_model.train(\n",
    "    L,\n",
    "    reg_type=2,\n",
    "    reg_param=0.1,\n",
    "    epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Featurize tweets and train a Logistic regression model with Snorkel\n",
    "\n",
    "In the part of the tutorial we show how to use the output of `Snorkel's` generative model to train a discriminative model (here a Logistic Regression model) to classify the sentiment of the available tweets.\n",
    "\n",
    "The first task we need to perfom is load the raw tweet bodies into Snorkel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load tweet bodies in a dataframe\n",
    "raw_crowd_answers = spark.read.format(\"csv\").option(\"header\", \"true\").csv(\"data/weather-non-agg-DFE.csv\")\n",
    "tweet_bodies = raw_crowd_answers.select(\"tweet_id\", \"tweet_body\").orderBy(\"tweet_id\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/sqlalchemy/ext/declarative/clsregistry.py:120: SAWarning: This declarative base already contains a class with the same class name and module name as snorkel.models.candidate.Tweet, and will be replaced in the string-lookup table.\n",
      "  item.__name__\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "Table 'tweet' is already defined for this MetaData instance.  Specify 'extend_existing=True' to redefine options and columns on an existing Table object.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-a7407028f6a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Define a tweet candidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mTweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcandidate_subclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tweet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'tweet_body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtaskLabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Generate and store the tweet candidates to be classified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/thodoris/Documents/Research/snorkel/snorkel/models/candidate.pyc\u001b[0m in \u001b[0;36mcandidate_subclass\u001b[0;34m(class_name, args, table_name, cardinality, values)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;31m# Create class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mCandidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_attribs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;31m# Create table in DB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/sqlalchemy/ext/declarative/api.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(cls, classname, bases, dict_)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_decl_class_registry'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0m_as_declarative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/sqlalchemy/ext/declarative/base.pyc\u001b[0m in \u001b[0;36m_as_declarative\u001b[0;34m(cls, classname, dict_)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0m_MapperConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/sqlalchemy/ext/declarative/base.pyc\u001b[0m in \u001b[0;36msetup_mapping\u001b[0;34m(cls, cls_, classname, dict_)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mcfg_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_MapperConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mcfg_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/sqlalchemy/ext/declarative/base.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cls_, classname, dict_)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_declared_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_inheritance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/sqlalchemy/ext/declarative/base.pyc\u001b[0m in \u001b[0;36m_setup_table\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m                     \u001b[0mtablename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                     \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeclared_columns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m                     **table_kw)\n\u001b[0m\u001b[1;32m    396\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__table__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/sqlalchemy/sql/schema.pyc\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, *args, **kw)\u001b[0m\n\u001b[1;32m    416\u001b[0m                     \u001b[0;34m\"to redefine \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                     \u001b[0;34m\"options and columns on an \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m                     \"existing Table object.\" % key)\n\u001b[0m\u001b[1;32m    419\u001b[0m             \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextend_existing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: Table 'tweet' is already defined for this MetaData instance.  Specify 'extend_existing=True' to redefine options and columns on an existing Table object."
     ]
    }
   ],
   "source": [
    "# Initialize a Snorkel session\n",
    "from snorkel import SnorkelSession\n",
    "from snorkel.models import candidate_subclass\n",
    "from snorkel.contrib.models.context import RawText\n",
    "\n",
    "session = SnorkelSession()\n",
    "\n",
    "# Define a tweet candidate\n",
    "Tweet = candidate_subclass('Tweet', ['tweet_body'], values=taskLabels)\n",
    "\n",
    "# Generate and store the tweet candidates to be classified\n",
    "for tweet in tweet_bodies.collect():\n",
    "    tweet_text = RawText(stable_id=tweet.tweet_id, name=tweet.tweet_id, text=tweet.tweet_body)\n",
    "    tweet = Tweet(tweet_body=tweet_text, split=0)\n",
    "    session.add(tweet)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will generate simple bag-of-word features that will be used by the Logistic Regression model. To do this we will use Snorkel's `FeatureAnnotator` class. All we need to provide as input to that class is a simple user-defined function (UDF) that takes as input a candidate and returns the bag-of-word features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We define a UDF that parses the body of a candidate (a tweet here)\n",
    "# and returns as features the token of the tweet body\n",
    "def bow_feature_generator(c):\n",
    "    for tok in c.get_contexts()[0].text.split():\n",
    "        yield tok, 1\n",
    "\n",
    "# We now use the FeatureAnnotaror provided by Snorkel \n",
    "# to generate features for all candidates.\n",
    "from snorkel.annotations import FeatureAnnotator\n",
    "featurizer = FeatureAnnotator(f=bow_feature_generator)\n",
    "\n",
    "%time F_train = featurizer.apply(split=0)\n",
    "F_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to train a Logistic Regression model to predict the sentiment of each tweet. The corresponding model takes as input: (i) the generated features, and (ii) the marginals estimated by `Snorkel's` generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import SparseLogisticRegression\n",
    "disc_model_sparse = SparseLogisticRegression()\n",
    "train_marginals = gen_model.marginals(L)\n",
    "#disc_model_sparse.train(F_train, train_marginals, n_epochs=2000, lr=0.001,\n",
    "#        batch_size=800, l2_penalty=0.1, print_freq=100)\n",
    "\n",
    "disc_model_sparse.train(F_train, train_marginals, n_epochs=4000, lr=0.0001,\n",
    "        batch_size=500, l2_penalty=0.001, print_freq=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluation\n",
    "\n",
    "Finally, we evaluate the performance of the end-to-end `Snorkel` model against the groundtruth labels. As with [Part I](Crowdsourced_Sentiment_Analysis_Part2.ipynb), we assign the final label of each tweet to be the MAP assignment given the marginal distribution returned by the Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get MAP assignment for each task\n",
    "test_marginals = disc_model_sparse.marginals(F_train)\n",
    "task_map_assignment = np.argmax(test_marginals, axis=1)\n",
    "inferedLabels = {}\n",
    "for i in range(len(task_map_assignment)):\n",
    "    inferedLabels[obj2TaskMap[i]] =  taskLabels[task_map_assignment[i]+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "errors = 0\n",
    "total = float(gold_answers.count())\n",
    "for trueLabel in gold_answers.select(\"tweet_id\",\"sentiment\",\"tweet_body\").collect():\n",
    "    if trueLabel.sentiment != inferedLabels[trueLabel.tweet_id]:\n",
    "        errors += 1\n",
    "        print '*** Error ***'\n",
    "        print 'Original tweet: '+trueLabel.tweet_body\n",
    "        print 'Groundtruth label: '+trueLabel.sentiment\n",
    "        print 'Snorkel label: '+inferedLabels[trueLabel.tweet_id]\n",
    "        print '\\n'\n",
    "print '\\n*** Overall Performance Statistics ***'\n",
    "print 'Wrongly infered labels: '+str(errors)+' out of '+str(total)\n",
    "print 'Accuracy of Snorkel''s model = ', (total-errors)/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
