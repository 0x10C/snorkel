{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Crowdsourced Sentiment Analysis with Snorkel - Resolving Conflicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the tutorial, we will walk through the process of using `Snorkel` to resolve conflicts in crowdsourced answers for a sentiment analysis task. The following tutorial is broken up into four core parts and a bonus part. Each part covers a step in the pipeline:\n",
    "1. Preprocessing\n",
    "2. Construction of a Snorkel Labeling Matrix\n",
    "3. Conflict Resolution\n",
    "4. Evaluation\n",
    "5. Bonus: Comparison against Majority Vote\n",
    "\n",
    "In this notebook, we preprocess the data collected by the crowd contributors using [Spark SQL and Dataframes](https://spark.apache.org/docs/latest/sql-programming-guide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Sentiment Analysis of Tweets\n",
    "\n",
    "In this tutorial we focus on the [Weather sentiment](https://www.crowdflower.com/data/weather-sentiment/) task from [Crowdflower](https://www.crowdflower.com/).\n",
    "\n",
    "In this task, contributors were asked to grade the sentiment of a particular tweet relating to the weather. The catch is that 20 contributors graded each tweet. We then ran an additional job (the one below) where we asked 10 contributors to grade the original sentiment evaluation.\n",
    "\n",
    "In this task, contributors were asked to grade the sentiment of a particular tweet relating to the weather. Contributors could choose among the following categories:\n",
    "1. Positive\n",
    "2. Negative\n",
    "3. I can't tell\n",
    "4. Neutral / author is just sharing information\n",
    "5. Tweet not related to weather condition\n",
    "\n",
    "The catch is that 20 contributors graded each tweet. Thus, in many cases contributors assigned conflicting sentiment labels to the same tweet. \n",
    "\n",
    "\n",
    "The task comes with two data files (to be found in the `data` directory of the tutorial:\n",
    "1. [weather-non-agg-DFE.csv](data/weather-non-agg-DFE.csv) contains the raw contributor answers for each of the 1,000 tweets.\n",
    "2. [weather-evaluated-agg-DFE.csv](data/weather-evaluated-agg-DFE.csv) contains gold sentiment labels by trusted workers for each of the 1,000 tweets.\n",
    "\n",
    "**GOAL:** The goal of this tutorial is to demonstrate how `Snorkel` can be used to accurately infer a single sentiment label for each tweet, thus, denoising the collected contributor answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Conflict Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now we have converted the raw crowdsourced data into a labeling matrix that can be provided as input to `Snorkel`. We will now show how to:\n",
    "\n",
    "1. Use `Snorkel's` generative model to learn the accuracy of each crowd contributor.\n",
    "2. Use the learned model to estimate a marginal distribution over the domain of possible labels for each task.\n",
    "3. Use the estimated marginal distribution to obtain the maximum a posteriori probability estimate for the label that each task takes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reloading the label matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<568x102 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 11360 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.annotations import load_label_matrix\n",
    "L_train = load_label_matrix(session, split=0)\n",
    "L_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and training a Snorkel generative model\n",
    "\n",
    "First we import and initialize `Snorkel's` generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from snorkel.learning.gen_learning import GenerativeModel\n",
    "\n",
    "# Initialize Snorkel's generative model for\n",
    "# learning the different worker accuracies.\n",
    "gen_model = GenerativeModel(lf_propensity=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train `Snorkel's` generative model by passing as input the labeling matrix that corresponds to the crowdsourced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred cardinality: 5.0\n"
     ]
    }
   ],
   "source": [
    "# Train the generative model\n",
    "gen_model.train(\n",
    "    L_train,\n",
    "    reg_type=2,\n",
    "    reg_param=0.1,\n",
    "    epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infering the marginal distribution\n",
    "The following command uses the labeling matrix and the learned generative model to estimate the marginal distribution over the domain of possible labels for each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "task_marginals = gen_model.marginals(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infering the MAP assignment for each task\n",
    "Each task corresponds to an indipendent random variable. Thus, we can simply associate each task with the most probably label based on the estimated marginal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get MAP assignment for each task\n",
    "task_map_assignment = np.argmax(task_marginals, axis=1)\n",
    "inferedLabels = {}\n",
    "for i in range(len(task_map_assignment)):\n",
    "    inferedLabels[obj2TaskMap[i]] =  taskLabels[task_map_assignment[i]+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluation\n",
    "\n",
    "We now evaluate the accuracy of `Snorkel's` model at identifying the correct label for each task by fusing the labels provided by differnet crowd contributors. For this we compare the MAP label assigned to tasks against the provided groundtruth data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract ground truth per tweet_id\n",
    "gold_crowd_answers = spark.read.format(\"csv\").option(\"header\", \"true\").csv(\"data/weather-evaluated-agg-DFE.csv\")\n",
    "gold_crowd_answers.createOrReplaceTempView(\"gold_crowd_answers\")\n",
    "gold_answers = spark.sql(\"SELECT tweet_id, sentiment, tweet_body FROM gold_crowd_answers WHERE correct_category ='Yes' and correct_category_conf = 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "errors = 0\n",
    "total = float(gold_answers.count())\n",
    "for trueLabel in gold_answers.select(\"tweet_id\",\"sentiment\",\"tweet_body\").collect():\n",
    "    if trueLabel.sentiment != inferedLabels[trueLabel.tweet_id]:\n",
    "        errors += 1\n",
    "        print '*** Error ***'\n",
    "        print 'Original tweet: '+trueLabel.tweet_body\n",
    "        print 'Groundtruth label: '+trueLabel.sentiment\n",
    "        print 'Snorkel label: '+inferedLabels[trueLabel.tweet_id]\n",
    "        print '\\n'\n",
    "print '\\n*** Overall Performance Statistics ***'\n",
    "print 'Wrongly infered labels: '+str(errors)+' out of '+str(total)\n",
    "print 'Accuracy of Snorkel''s model = ', (total-errors)/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store certain dataframes and maps generated during the previous steps to persistent files. These will be used in [Part 2](Crowdsourced_Sentiment_Analysis_Part2.ipynb) of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save dataframe as parquet files\n",
    "worker_labels.write.parquet(\"data/worker_labels.parquet\",mode=\"overwrite\")\n",
    "gold_answers.write.parquet(\"data/gold_answers.parquet\",mode=\"overwrite\")\n",
    "\n",
    "# Save maps as pickle files\n",
    "import pickle\n",
    "pickle.dump( task2ObjMap, open( \"data/task2ObjMap.pkl\", \"wb\" ) )\n",
    "pickle.dump( obj2TaskMap, open( \"data/obj2TaskMap.pkl\", \"wb\" ) )\n",
    "pickle.dump( worker2LFMap, open( \"data/worker2LFMap.pkl\", \"wb\" ) )\n",
    "pickle.dump( lf2WorkerMap, open( \"data/lf2WorkerMap.pkl\", \"wb\" ) )\n",
    "pickle.dump( taskLabels, open( \"data/taskLabels.pkl\", \"wb\" ) )\n",
    "pickle.dump( taskLabelsMap, open( \"data/taskLabelsMap.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Comparison against Majority Vote\n",
    "\n",
    "As a bonus we evaluate majority voting against `Snorkel`. Given that we have 20 contributors per task (and most of them are better than chance) **we expect majority voting to perform extremely well**. However, as shown below, **Snorkel's model, which estimates the accuracy of each worker, makes fewer mistakes and achieves a higher accuracy.** Specifically, Majority Vote makes 9 mistakes versus the 3 mistakes that Snorkel makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Majority vote evaluation\n",
    "import operator\n",
    "\n",
    "taskMVassignment = {}\n",
    "obj2TaskMap\n",
    "objects, LFs\n",
    "for i in range(objects):\n",
    "    objectVotes = {}\n",
    "    for j in range(LFs):\n",
    "        label = L[i,j]\n",
    "        if label != 0:\n",
    "            if label not in objectVotes:\n",
    "                objectVotes[label] = 0\n",
    "            objectVotes[label] += 1\n",
    "    maxValue = -1\n",
    "    assignedLabel = ''\n",
    "    for key in objectVotes:\n",
    "        if objectVotes[key] > maxValue:\n",
    "            maxValue = objectVotes[key]\n",
    "            assignedLabel = key\n",
    "    taskMVassignment[obj2TaskMap[i]] = taskLabels[assignedLabel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "errors = 0\n",
    "total = float(gold_answers.count())\n",
    "for trueLabel in gold_answers.select(\"tweet_id\",\"sentiment\",\"tweet_body\").collect():\n",
    "    if str(trueLabel.sentiment) != str(taskMVassignment[trueLabel.tweet_id]):\n",
    "        errors += 1\n",
    "        print '*** Error ***'\n",
    "        print 'Original tweet: '+trueLabel.tweet_body\n",
    "        print 'Groundtruth label: '+trueLabel.sentiment\n",
    "        print 'MV label: '+taskMVassignment[trueLabel.tweet_id]\n",
    "        print '\\n'\n",
    "print '\\n*** Overall Performance Statistics ***'\n",
    "print 'Wrongly infered labels: '+str(errors)+' out of '+str(total)\n",
    "print 'Accuracy of Majority Voting model = ', (total-errors)/total"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
