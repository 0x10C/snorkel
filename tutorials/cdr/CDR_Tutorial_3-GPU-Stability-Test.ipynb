{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chemical-Disease Relation (CDR) Tutorial\n",
    "\n",
    "In this example, we'll be writing an application to extract *mentions of* **chemical-induced-disease relationships** from Pubmed abstracts, as per the [BioCreative CDR Challenge](http://www.biocreative.org/resources/corpora/biocreative-v-cdr-corpus/).  This tutorial will show off some of the more advanced features of Snorkel, so we'll assume you've followed the Intro tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by reloading from the last notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from snorkel import SnorkelSession\n",
    "\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\t8272 candidates\n",
      "Dev set:\t888 candidates\n",
      "Test set:\t4620 candidates\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "ChemicalDisease = candidate_subclass('ChemicalDisease', ['chemical', 'disease'])\n",
    "\n",
    "train = session.query(ChemicalDisease).filter(ChemicalDisease.split == 0).all()\n",
    "dev = session.query(ChemicalDisease).filter(ChemicalDisease.split == 1).all()\n",
    "test = session.query(ChemicalDisease).filter(ChemicalDisease.split == 2).all()\n",
    "\n",
    "print('Training set:\\t{0} candidates'.format(len(train)))\n",
    "print('Dev set:\\t{0} candidates'.format(len(dev)))\n",
    "print('Test set:\\t{0} candidates'.format(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing features\n",
    "\n",
    "For `SparseLogReg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 7min 58s, sys: 3.49 s, total: 8min 1s\n",
      "Wall time: 8min 1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<8272x122840 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 448906 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.annotations import FeatureAnnotator\n",
    "featurizer = FeatureAnnotator()\n",
    "\n",
    "try:\n",
    "    F_train = featurizer.load_matrix(session, split=0)\n",
    "    F_dev = featurizer.load_matrix(session, split=1)\n",
    "    assert F_train.nonzero() > 0\n",
    "    assert F_dev.nonzero() > 0\n",
    "except:\n",
    "    %time F_train = featurizer.apply(split=0)\n",
    "    %time F_dev = featurizer.apply_existing(split=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load marginals and dev labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_marginals\n",
    "train_marginals = load_marginals(session, F_train, split=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<888x1 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 888 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "L_gold_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing stability of `SparseLogisticRegression` on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparseLogisticRegression] Training model\n",
      "[SparseLogisticRegression] n_train=4836  #epochs=50  batch size=256\n",
      "[SparseLogisticRegression] Epoch 0 (0.36s)\tAverage loss=0.730845\n",
      "[SparseLogisticRegression] Epoch 5 (2.26s)\tAverage loss=0.658236\n",
      "[SparseLogisticRegression] Epoch 10 (4.13s)\tAverage loss=0.658342\n",
      "[SparseLogisticRegression] Epoch 15 (6.08s)\tAverage loss=0.667906\n",
      "[SparseLogisticRegression] Epoch 20 (7.80s)\tAverage loss=0.668389\n",
      "[SparseLogisticRegression] Epoch 25 (9.69s)\tAverage loss=0.671954\n",
      "[SparseLogisticRegression] Epoch 30 (11.56s)\tAverage loss=0.667266\n",
      "[SparseLogisticRegression] Epoch 35 (13.61s)\tAverage loss=0.666604\n",
      "[SparseLogisticRegression] Epoch 40 (15.61s)\tAverage loss=0.671293\n",
      "[SparseLogisticRegression] Epoch 45 (17.35s)\tAverage loss=0.670419\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning import SparseLogisticRegression\n",
    "\n",
    "model_hyperparams = {\n",
    "    'n_epochs' : 50,\n",
    "    'rebalance' : 0.5,\n",
    "    'print_freq' : 5\n",
    "}\n",
    "\n",
    "log_reg = SparseLogisticRegression(seed=123, deterministic=True)\n",
    "log_reg.train(F_train, train_marginals, **model_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"{0:.5f}\".format(log_reg.score(F_dev, L_gold_dev)[-1]))\n",
    "w1, b1 = log_reg.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.51647\n",
      "2.99769\n",
      "0.00625005\n"
     ]
    }
   ],
   "source": [
    "print(\"{0:.5f}\".format(log_reg.score(F_dev, L_gold_dev)[-1]))\n",
    "w2, b2 = log_reg.get_weights()\n",
    "print(np.linalg.norm(w1-w2))\n",
    "print(np.linalg.norm(b1-b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No seed, det=False\n",
    "* 0.52891, 0.52878\n",
    "* 50.2766\n",
    "* 0.00731753\n",
    "\n",
    "### Seed=123, det=False\n",
    "* 0.52066, 0.51962\n",
    "* 2.69907\n",
    "* 0.00837359\n",
    "* t = 19.7 sec\n",
    "\n",
    "\n",
    "### Seed=123, det=True\n",
    "* 0.52708, 0.51647\n",
    "* 2.99769\n",
    "* 0.00625005\n",
    "* t = 19.7 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V: Training an LSTM extraction model\n",
    "\n",
    "In the intro tutorial, we automatically featurized the candidates and trained a linear model over these features. Here, we'll train a more complicated model for relation extraction: an LSTM network. You can read more about LSTMs [here](https://en.wikipedia.org/wiki/Long_short-term_memory) or [here](http://colah.github.io/posts/2015-08-Understanding-LSTMs/). An LSTM is a type of recurrent neural network and automatically generates a numerical representation for the candidate based on the sentence text, so no need for featurizing explicitly as in the intro tutorial. LSTMs take longer to train, and Snorkel doesn't currently support hyperparameter searches for them. We'll train a single model here, but feel free to try out other parameter sets. Just make sure to use the development set - and not the test set - for model selection.\n",
    "\n",
    "**Note: Again, training for more epochs than below will greatly improve performance- try it out!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import reRNN\n",
    "\n",
    "train_kwargs = {\n",
    "    'lr':         0.01,\n",
    "    'dim':        50,\n",
    "    'n_epochs':   20,\n",
    "    'dropout':    0.5,\n",
    "    'rebalance':  0.25,\n",
    "    'print_freq': 5,\n",
    "    'batch_size': 50,\n",
    "}\n",
    "\n",
    "lstm = reRNN(seed=1701, n_threads=None)\n",
    "lstm.train(train, train_marginals, X_dev=dev, Y_dev=L_gold_dev, **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lstm.score(test, L_gold_test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
