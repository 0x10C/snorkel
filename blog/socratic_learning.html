<!DOCTYPE html><html>

<head>
<meta charset="utf-8">
<title>Socratic Learning</title>
<style type="text/css">
body {
  font-family: Helvetica, arial, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  padding-top: 10px;
  padding-bottom: 10px;
  background-color: white;
  padding: 30px; }

body > *:first-child {
  margin-top: 0 !important; }
body > *:last-child {
  margin-bottom: 0 !important; }

a {
  color: #4183C4; }
a.absent {
  color: #cc0000; }
a.anchor {
  display: block;
  padding-left: 30px;
  margin-left: -30px;
  cursor: pointer;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0; }

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
  cursor: text;
  position: relative; }

h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA09pVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMy1jMDExIDY2LjE0NTY2MSwgMjAxMi8wMi8wNi0xNDo1NjoyNyAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNiAoMTMuMCAyMDEyMDMwNS5tLjQxNSAyMDEyLzAzLzA1OjIxOjAwOjAwKSAgKE1hY2ludG9zaCkiIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OUM2NjlDQjI4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OUM2NjlDQjM4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo5QzY2OUNCMDg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo5QzY2OUNCMTg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PsQhXeAAAABfSURBVHjaYvz//z8DJYCRUgMYQAbAMBQIAvEqkBQWXI6sHqwHiwG70TTBxGaiWwjCTGgOUgJiF1J8wMRAIUA34B4Q76HUBelAfJYSA0CuMIEaRP8wGIkGMA54bgQIMACAmkXJi0hKJQAAAABJRU5ErkJggg==) no-repeat 10px center;
  text-decoration: none; }

h1 tt, h1 code {
  font-size: inherit; }

h2 tt, h2 code {
  font-size: inherit; }

h3 tt, h3 code {
  font-size: inherit; }

h4 tt, h4 code {
  font-size: inherit; }

h5 tt, h5 code {
  font-size: inherit; }

h6 tt, h6 code {
  font-size: inherit; }

h1 {
  font-size: 28px;
  color: black; }

h2 {
  font-size: 24px;
  border-bottom: 1px solid #cccccc;
  color: black; }

h3 {
  font-size: 18px; }

h4 {
  font-size: 16px; }

h5 {
  font-size: 14px; }

h6 {
  color: #777777;
  font-size: 14px; }

p, blockquote, ul, ol, dl, li, table, pre {
  margin: 15px 0; }

hr {
  background: transparent url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC) repeat-x 0 0;
  border: 0 none;
  color: #cccccc;
  height: 4px;
  padding: 0;
}

body > h2:first-child {
  margin-top: 0;
  padding-top: 0; }
body > h1:first-child {
  margin-top: 0;
  padding-top: 0; }
  body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0; }
body > h3:first-child, body > h4:first-child, body > h5:first-child, body > h6:first-child {
  margin-top: 0;
  padding-top: 0; }

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0; }

h1 p, h2 p, h3 p, h4 p, h5 p, h6 p {
  margin-top: 0; }

li p.first {
  display: inline-block; }
li {
  margin: 0; }
ul, ol {
  padding-left: 30px; }

ul :first-child, ol :first-child {
  margin-top: 0; }

dl {
  padding: 0; }
  dl dt {
    font-size: 14px;
    font-weight: bold;
    font-style: italic;
    padding: 0;
    margin: 15px 0 5px; }
    dl dt:first-child {
      padding: 0; }
    dl dt > :first-child {
      margin-top: 0; }
    dl dt > :last-child {
      margin-bottom: 0; }
  dl dd {
    margin: 0 0 15px;
    padding: 0 15px; }
    dl dd > :first-child {
      margin-top: 0; }
    dl dd > :last-child {
      margin-bottom: 0; }

blockquote {
  border-left: 4px solid #dddddd;
  padding: 0 15px;
  color: #777777; }
  blockquote > :first-child {
    margin-top: 0; }
  blockquote > :last-child {
    margin-bottom: 0; }

table {
  padding: 0;border-collapse: collapse; }
  table tr {
    border-top: 1px solid #cccccc;
    background-color: white;
    margin: 0;
    padding: 0; }
    table tr:nth-child(2n) {
      background-color: #f8f8f8; }
    table tr th {
      font-weight: bold;
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr td {
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr th :first-child, table tr td :first-child {
      margin-top: 0; }
    table tr th :last-child, table tr td :last-child {
      margin-bottom: 0; }

img {
  max-width: 100%; }

span.frame {
  display: block;
  overflow: hidden; }
  span.frame > span {
    border: 1px solid #dddddd;
    display: block;
    float: left;
    overflow: hidden;
    margin: 13px 0 0;
    padding: 7px;
    width: auto; }
  span.frame span img {
    display: block;
    float: left; }
  span.frame span span {
    clear: both;
    color: #333333;
    display: block;
    padding: 5px 0 0; }
span.align-center {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-center > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: center; }
  span.align-center span img {
    margin: 0 auto;
    text-align: center; }
span.align-right {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-right > span {
    display: block;
    overflow: hidden;
    margin: 13px 0 0;
    text-align: right; }
  span.align-right span img {
    margin: 0;
    text-align: right; }
span.float-left {
  display: block;
  margin-right: 13px;
  overflow: hidden;
  float: left; }
  span.float-left span {
    margin: 13px 0 0; }
span.float-right {
  display: block;
  margin-left: 13px;
  overflow: hidden;
  float: right; }
  span.float-right > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: right; }

code, tt {
  margin: 0 2px;
  padding: 0 5px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px; }

pre code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent; }

.highlight pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }

pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }
  pre code, pre tt {
    background-color: transparent;
    border: none; }

sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}
* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:0 auto;
    }
}
@media print {
	table, pre {
		page-break-inside: avoid;
	}
	pre {
		word-wrap: break-word;
	}
}

.top-banner {
    position: absolute;
    top: 0;
    left: 0;
    z-index: 0;
}

#top-banner-img {
    opacity: 0.5;
    max-width: none;
    max-height: 350px;
}

#main-title {
    position: relative;
    margin-top: 100px;
    margin-bottom: 100px;
    padding: 10px;
    font-size: 40px;
    background: #333333;
    color: #f8f8f8;
    z-index: 10;
}

blockquote {
    font-size: large;
    font-weight: 300;
}

p.img {
    text-align: center;
}
</style>
</head>
<body>
<span class="top-banner">
    <img id="top-banner-img" src="socrates_banner1.jpg" alt="image alt text">
</span>

<p id="main-title">Socratic Learning:<br />Debugging Machine Learning Models</p>

<p>Post by Paroma Varma, Rose Yu, Dan Iter, Chris De Sa and Chris Ré</p>

<p><em>And referencing work by many other <a href="http://cs.stanford.edu/people/chrismre/#students">members of Hazy Research</a>. Especially the research (and template) <a href="http://hazyresearch.github.io/snorkel/blog/weak_supervision.html">here</a></em>.</p>

<blockquote>
<p>Modern machine learning techniques like deep learning often require large amounts of training data, which are not readily available for all applications. Weak supervision attempts to alleviate this problem by using heuristics and “weak” classifiers to provide noisy training data. Previous work from the lab, <a href="https://arxiv.org/abs/1605.07723"><strong>data programming</strong></a>, encodes this weak supervision signal with user-defined labeling functions (as part of a generative model) to noisily label unlabeled data. With data programming and other weak supervision methodologies, these issues still remain:
</p>

<ul>
<li><p>The generative model may be misspecified, which can affect the performance of the discriminative training that follows.</p></li>
<li><p>The errors in the generative model do not provide users any intuition about why there were errors and how they can be fixed.</p></li>
</ul>

<p>We describe our novel framework <a href="socratic-learning.pdf"><strong>Socratic learning</strong></a>, a systematic process that improves the expressiveness of the generative model by using information from the discriminative model. 
</p>
</blockquote>

<!-- ><p><img src="new-new-banner.png" alt="image alt text"></p> -->

<h1 id="toc_0">Case Study: Who has Taylor Swift Dated?</h1>

<p><img src="taylor1.jpg" alt="image alt text"></p> 


<blockquote>
<p>We will use a toy example to explore Socratic learning. We ran Socratic learning on real relation extraction datasets in other domains and reported results in the paper.</p>
</blockquote>


<h2 id="toc_0">Extraction Phase</h2>

<p> Imagine a relation extraction task that attempts to learn whether Taylor Swift has dated a particular person. In this scenario, a data point is any news article that includes the word &quot;Taylor Swift&quot; in the title. A natural language processing pipeline further analyzes the title to extract the entity that Taylor Swift is supposedly dating. For example, from an article titled &quot;<a href="https://www.thesun.co.uk/tvandshowbiz/1447387/tom-hiddleston-defends-his-relationship-with-taylor-swift-as-the-real-deal/">Tom Hiddleston defends his relationship with Taylor Swift as the real deal</a>&quot;, the data point extracted would be &quot;Tom Hiddleston&quot;. In this case, the ground truth label for this data point is <em>True</em>, since they did date at some point. </p>

<p style="text-align:center;"><img src="extraction.png" width=500px></p> 

<h2 id="toc_0">Generative Model</h2>

<p> Following the data programming paradigm, we write the following simple labeling functions: (1) if there are too many words between “Taylor Swift” and the data point, we mark it as False. (2) If another person is mentioned between the name of the data point and the singer, the data point is marked False. (3) if the phrase “is dating” or “...relationship with” appears between the names in the same sentence, it’s marked as True. If none of the labeling functions apply, the data point receives no label at the generative model level. The process described in data programming learns some accuracy for each of these labeling functions and weights them accordingly to generate a set of imperfect training labels. </p>

<p style="text-align:center;"><img src="generative.png" width=400px></p> 

<h1 id="toc_0">The Problem of Model Misspecification</h1>

<blockquote>
<p>Domain experts often prefer generative models since they “tell a story” about the data. However, these models can be incomplete or incorrect in some cases.</p>
</blockquote>

<p> We take a break from our case study to look at an underlying issue with generative models. These models are usually easier to write and interpret for domain experts. In the context of data programming, users can write labeling functions that encode domain knowledge to noisily label a certain portion of the unlabeled training data. However, some labeling functions might be too general.</p>
	
<p style="text-align:center;"><img src="misspecification2.png" width=700px></p> 

<p><em>Example of a labeling function that marks any data point with the word "love" or "like" as True and with  "never" as False. It would work well for some data points and not others.</em></p>
	
<p> Others can work well for some subset of the data and not the other. Because of this, designing these generative models is an arduous and iterative process where experts spend much of their time considering the trade-offs between high accuracy and high coverage models. Moreover, it is not trivial to pick which function to tweak and how. Since a strong generative model is key to the subsequent learning task, debugging at this stage becomes the most time-consuming and challenging part of this process. </p>

<p> With our approach, we attempt to <em>programatically</em> debug the generative model without user intervention. This is certainly not a complete solution to the problem of misspecified models, but a step towards making generative models more expressive and interpretable. </p>

<h1 id="toc_0">Case Study continued...</h1>

<h2 id="toc_0">Discriminative Model</h2>

<p> Back to our example! Now we have a labeled training set and features for each data point such as word count, the newspaper/magazine it came from, presence of phrases like &quot;dating&quot;, &quot;love&quot;, &quot;working with&quot; etc. that may help determine whether the relation is indeed a romantic one or not. We can train any discriminative machine learning algorithm on this data to predict a new set of labels.</p>

<p style="text-align:center;"><img src="discriminative.png" width=400px></p> 

<p>The discriminative model uses the noisily labeled training set from the generative model to learn a new model that relates the features associated with each article to the true label.</p>

<h2 id="toc_0">Difference Model</h2>

<p>We find the generative model labels disagree with the labels from the discriminative model about 10% of the time — a sign that there is some underlying disagreement between the two models. Socratic learning finds the cause behind this disagreement <strong>by identifying a particular feature</strong> that best explains the discrepancy. 
</p>
<p style="text-align:center;"><img src="difference.png" width=500px></p> 


<p>This feature turns out to be the one that fires if the article comes from &quot;<a href="http://www.theonion.com/">The Onion</a>&quot;. As expected, there are various articles along the lines of &quot;<a href="http://www.theonion.com/article/taylor-swift-now-in-long-distance-relationship-wit-33385">Taylor Swift is currently in a long-distance relationship with NASA’s Curiosity Rover</a>&quot; and &quot;<a href="http://www.theonion.com/article/taylor-swift-now-dating-senator-joseph-mccarthy-31861">Taylor Swift Now Dating Senator Joseph McCarthy</a>&quot; that are published by &quot;The Onion&quot;. These sarcasm-ridden articles were identified by the discriminative model as suggesting fake relationships, but the limited generative model was not able to make this distinction.</p>

<p>Socratic learning passes this newfound information to the generative model, which adjusts the accuracy of labeling function (3) (since that was causing false relationships to be marked true) so that it has a low accuracy when the article it is analyzing is from “The Onion” and a high accuracy when it is not. Including this feature in the generative model helps the it become stronger and more expressive. It also gives the users a chance to write specialized labeling functions for similar troll articles. </p>

<h1 id="toc_0">Socratic Learning: A Cooperatic Dialog</h1>

<blockquote>
<p>In many cases, the more powerful discriminative model has underlying information about the data that the generative model would benefit from knowing. We automatically recognize and use this knowledge to correct generative model misspecification.</p>
</blockquote>

<p style="text-align:center;"><img src="pipeline.png" width=700px></p> 

<p>Just like in the case study, there might be the opposite case where a user is tempted to throw out or rewrite labeling functions with low accuracies. However, these labeling functions might work very well for a subset of the data that the user is unaware of, even though it performs poorly on average. Recognizing these <em>latent classes</em> in the data that labeling function performances are related to can be essential to improving the generative model.</p>

<p>With Socratic learning, we attempt to automatically recognize these latent classes via a <strong>cooperative dialog </strong>between the discriminative and generative models. This dialog consists of certain important features from the discriminative model. By comparing the predictions from the two models, we can detect which features best explain the difference in predictions and map to latent classes. The advantage of identifying these features is twofold: first, they can be incorporated into the generative model to make it more expressive. Second, these features can provide users with interpretable feedback about how they can manually write more effective labeling functions.</p>

<p>Since our process is iterative, it is able to recognize multiple features that the generative model would benefit from knowing. Users are able to track how well the generative model does with the inclusion of these features and stop adding more features when the accuracy starts dropping on a held-out set. This ensures that the generative model does not overfit and adds a “human in the loop” aspect to the pipeline.</p>

<!-- ><p><img src="lfs.png" alt="image alt text" title="Example CID labeling functions"></p>

<p><em>Two simple example labeling functions (written in Python) for extracting mentions of chemical-induced disease relations from the scientific literature; <code>lf1</code> leverages an existing curated ontology, while <code>lf2</code> uses a heuristic pattern.</em></p> -->

<h2 id="toc_5">Next-Step Goals</h2>

<p>A few things we are currently working on:</p>

<ul>
<li><p>We want to extend Socratic learning to multimodal data like text, images, time-series etc.</p></li>
<li><p>We hope to find other avenues where Socratic learning can make a difference, including deep learning and crowdsourcing.</p></li>
</ul>


</body>

</html>
