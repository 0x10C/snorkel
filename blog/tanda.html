<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Alex Ratner, Henry Ehrenberg, Zeshan Hussain, Jared Dunnmon, Chris Ré" />
  <title>Learning to Compose Domain-Specific Transformations for Data Augmentation</title>
  <link rel="stylesheet" href="blog.css">
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<div id="header">
<h1 class="title">Learning to Compose Domain-Specific Transformations for Data Augmentation</h1>
Alex Ratner, Henry Ehrenberg, Zeshan Hussain, Jared Dunnmon, Chris Ré<br />
<em>And referencing work by many other members of Hazy Research</em><br />
<a href="https://snorkel.stanford.edu">Back to the Snorkel Blog</a>
</div>

<div class="figure">
<img src="tanda_figs/data_aug_basic.png" alt="Basic data augmentation."/>
</div>

<blockquote>
<p><b>Data augmentation</b> is a technique for increasing the size of labeled training sets by applying class-preserving transformations.
In the image domain, it is a crucial factor in <i>almost every</i> state-of-the-art result today.
However, the particular types, parameterizations, and composition of transformations applied can have a <i>large</i> effect on performance, and are tricky and time-consuming to tune by hand for a new dataset or task.</p>

<p>In this blog post we describe our new automated method for data augmentation:</p>

<ul>
	<li>We start by representing transformations as <b>sequences of incremental black-box operations</b></li>
	<li>We then learn a <b>generative sequence model</b> that produces realistic, class-preserving augmentations using unlabeled data and adversarial techniques.</li>
	<li>We get <b>strong gains over heuristic approaches</b>--3.9 pts. improvement on CIFAR-10, 1.4 F1 pts. on an ACE relation extraction task, and 3.4 pts. on a mammograpy task--and demonstrate <b>robustness to user misspecification.</b></li>
</ul>

<p><b>You can also check out our <a href="#">paper</a> for detail and references, and our <a href="#">code</a> to give it a spin!</b></p>
</blockquote>

<h2 id="intro">Automating the Art of Data Augmentation</h2>

<p>Modern machine learning models, such as deep neural networks, may have billions of free parameters and accordingly require massive labeled training sets--which are not usually available.
The technique of artificially expanding labeled training sets by transforming data points in ways which preserve class labels--known as <i>data augmentation</i>--has quickly become a critical and effective tool for combatting this labeled data scarcity problem.
And indeed, data augmentation is cited as essential to nearly every state-of-the-art result in image classification (see below), and is becoming increasingly common in other modalities as well.</p>

<p>These gains are remarkable for such a simple technique. But like everything in machine learning, there's a hidden cost: the time required to develop data augmentation pipelines.
For while it's often simple to formulate individual transformation operations, it's generally time-consuming and difficult to find the right parameterizations and compositions of them.
In particular, many transformation operations will have vastly different effects based on parameterization, the set of other transformations they are applied with, and even their particular order of composition.
For example, a brightness operation might produce realistic images when applied with a small rotation, but produce a garbage image when applied along with a saturation enhancement.
This problem is only exacerbated for a new task or domain, where optimal data augmentation strategies have not been worked out by the community over time.
In general, practitioners generally revert to random application of heuristically-tuned transformations, which while helpful, is far from optimal--and occasionally disastrous!</p>

<p>So coming up with an optimal data augmentation pipeline is tricky... but how often do we need to do this anyway?
In our view, data augmentation can be seen as an important form of <i><a href="https://hazyresearch.github.io/snorkel/blog/ws_blog_post.html">weak supervision</a></i>, providing a way for subject matter experts (SMEs) to leverage their knowledge of invariances in a task or domain to improve model performance even given limited labeled training data.
As such, our goal is to make it easy enough to deploy for any new, real-world task with its own specific types of invariances and transformation operations--without requiring days or weeks of tuning and tweaking.
Moreover, an ideal data augmentation system should permit arbitrary, black-box transformation operations--thus serving as a flexible, model-agnostic way for SMEs to inject domain knowledge into machine learning pipelines.
</p>

<p>In <b><a href="#">our proposed system</a></b>, users provide a set of arbitrary, black-box transformation functions (TFs)--representing <i>incremental</i> transformation operations--which need not be differentiable or deterministic, and an unlabeled dataset.
We the automatically learn a generative sequence model over the TFs, using the unlabeled data and adversarial techniques, so that the generated transformation sequences produce realistic, class-preserving transformations.
Finally, these can be used to improve the performance of any end discriminative model!</p>

<p>In this blog post, we'll start by reviewing the prevalence of heuristic data augmentation in practice, then outline our proposed approach, and finally review our empirical results.</p>


<h2 id="background-in-practice">Heuristic Data Augmentation in Practice</h2>

<p>Data augmentation is the secret sauce in today's state-of-the-art pipelines for benchmark image recognition tasks: it's not the star of the show, but your results won't be nearly as good without it. Here's an overview of top scoring models for CIFAR-10 and CIFAR-100, with and without data augmentation:</p>

<table class="dense-table" style="width:100%; font-size: 10px;">
  	<tr>
	    <th>Dataset</th>
		<th>Pos</th>
		<th>Name</th>
		<th>Err.w/ DA</th>
		<th>Err. w/oDA</th>
		<th>DA Types / notes</th>
	</tr>
	<tr>
		<td rowspan="10">CIFAR-10</td>
		<td>1*</td>
		<td>DenseNet</td>
		<td>3.46</td>
		<td>-</td>
		<td>Random shifts, flips; note, for their other next-best result, gap from DA is 1.43</td>
	</tr>
	<tr>
		<td>2</td>
		<td>Fractional Max-Pooling</td>
		<td>3.47</td>
		<td>-</td>
		<td>Randomized mix of translations, rotations, reflections, stretching, and shearing operations, and random RGB color shifts</td>
	</tr>
	<tr>
		<td>3*</td>
		<td>Wide ResNet</td>
		<td>4.17</td>
		<td>-</td>
		<td>Random shifts, flips</td>
	</tr>
	<tr>
		<td>4</td>
		<td>Striving for Simplicity: The All Convolutional Net</td>
		<td>4.41</td>
		<td>9.08</td>
		<td>They also get 7.25% using just hor. Flips, random shifts, other simple ones. They then used “heavy” augmentation: images expanded, then scaled, rotated, color shifted.</td>
	</tr>
	<tr>
		<td>5*</td>
		<td>FractalNet</td>
		<td>4.60</td>
		<td>7.33</td>
		<td>Random shifts, flips</td>
	</tr>
	<tr>
		<td>6*</td>
		<td>ResNet (1001-Layer)</td>
		<td>4.62</td>
		<td>10.56</td>
		<td>Random shifts, flips</td>
	</tr>
	<tr>
		<td>7*</td>
		<td>ResNet with Stochastic Depth (1202-Layer)</td>
		<td>4.91</td>
		<td>-</td>
		<td>Random shifts, flips</td>
	</tr>
	<tr>
		<td>8</td>
		<td>All You Need is a Good Init</td>
		<td>5.84</td>
		<td>-</td>
		<td>They get their best result using “out of the box” ResNets, which uses random crops, horizontal flips.</td>
	</tr>
	<tr>
		<td>9</td>
		<td>Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree</td>
		<td>6.05</td>
		<td>7.62</td>
		<td>Flips, random shifts, other simple ones.</td>
	</tr>
	<tr>
		<td>10</td>
		<td>Spatially-Sparse Convolutional Neural Networks</td>
		<td>6.28</td>
		<td>-</td>
		<td>Affine transformations</td>
	</tr>
	<tr>
		<td rowspan="10">CIFAR-100</td>
		<td>1*</td>
		<td>DenseNet</td>
		<td>17.18</td>
		<td>-</td>
		<td>Random shifts, flips; note, for their other next-best result, gap from DA is 2.04</td>
	</tr>
	<tr>
		<td>2*</td>
		<td>Wide ResNets</td>
		<td>20.50</td>
		<td>-</td>
		<td>Random shifts, flips</td>
	</tr>
	<tr>
		<td>3*</td>
		<td>ResNet (1001-Layer)</td>
		<td>22.71</td>
		<td>33.47</td>
		<td>Random shifts, flips</td>
	</tr>
	<tr>
		<td>4*</td>
		<td>FractalNet</td>
		<td>23.30</td>
		<td>35.34</td>
		<td>Random shifts, flips</td>
	</tr>
	<tr>
		<td>5</td>
		<td>Fast and Accurate Deep Network Learning by Exponential Linear Units</td>
		<td>-</td>
		<td>24.28</td>
		<td></td>
	</tr>
	<tr>
		<td>6</td>
		<td>Spatially-Sparse Convolutional Neural Networks</td>
		<td>24.3</td>
		<td>-</td>
		<td>Affine transformations</td>
	</tr>
	<tr>
		<td>7*</td>
		<td>ResNet with Stochastic Depth (1202-Layer)</td>
		<td>24.58</td>
		<td>37.80</td>
		<td>Random shifts, flips</td>
	</tr>
	<tr>
		<td>8</td>
		<td>Fractional Max-Pooling</td>
		<td>26.39</td>
		<td>-</td>
		<td>Randomized mix of translations, rotations, reflections, stretching, and shearing operations, and random RGB color shifts.</td>
	</tr>
	<tr>
		<td>9*</td>
		<td>ResNet (110-Layer)</td>
		<td>27.22</td>
		<td>44.74</td>
		<td>Random shifts, flips.</td>
	</tr>
	<tr>
		<td>10</td>
		<td>Scalable Bayesian Optimization Using Deep Neural Networks</td>
		<td>27.4</td>
		<td>-</td>
		<td>Hue, saturation, scalings, horizontal flips.</td>
	</tr>
</table>



<h1 id="learning-how-to-augment-data">Learning how to augment data</h1>

<p>We model the process of data augmentation as applying a series of TFs, each representing an incremental transformation (such as a small image rotation). This is illustrated for image recognition, natural language processing, and computer-assisted detection in the figure below. Our goal is to automatically compose and parameterize a set of TFs in ways that produce realistic and diverse augmented data points.</p>

<div class="figure">
<img src="tanda_figs/example_tfs.png" alt="Example TFs."/>
<p class="caption"><i>Figure 1: Examples of transformation functions for different domains.</i></p>
</div>

<p>The core assumption behind standard data augmentation is <em>any</em> sequence of transformation operations applied to <em>any</em> data point will produce an augmented point in the same class. Of course, this is unrealistic and many real-world data augmentation pipelines violate this assumption. Instead, we make a weaker modeling assumption: a sequence of TFs applied to a data point will produce an augmented point either in the same class or in a null class outside the distribution of interest. That is, we can reasonably assume that we won't turn an image of a plane into one of a dog, but we might turn an image of a plane into an indistiguishable garbage image. This critical assumption allows us to use unlabeled data to train our augmentation model.</p>

<div class="figure">
<img src="tanda_figs/fig2.png" alt="Figure 2"/>
</div>

<p>Our modeling setup is summarized in Figure 2. The objective is to learn a model <span class="math inline">\(G_{\theta}\)</span> which generates sequences of TF indices <span class="math inline">\(\tau\in\{1,K\}^L\)</span> with fixed length <span class="math inline">\(L\)</span> so that the augmented data point <span class="math inline">\(h_{\tau_{L}} \circ ... \circ h_{\tau_{1}}(x)\)</span> is realistic, i.e. not in the null class. In order to estimate whether or not the augmented point is in the null class, we use a generative adversarial network (GAN) setup and simultaneously train a discriminator <span class="math inline">\(D_{\phi}^{\emptyset}\)</span>. The discriminator's job is to produce values close to 1 for data points in the original training set and values close to 0 for augmented data points. We can write out our objective term as</p>

<span class="math">$$J_{\emptyset} = \mathbb{E}_{\tau\sim G_{\theta}} \mathbb{E}_{x\sim\mathcal{U}}\left[ \log(1 - D_\phi^\emptyset(h_{\tau_L}\circ ...\circ h_{\tau_1}(x)))\right] + \mathbb{E}_{x'\sim\mathcal{U}}\left[ \log(D_\phi^\emptyset(x')) \right]$$</span>

<p>where <span class="math inline">\(\mathcal{U}\)</span> is a distribution of unlabeled data (our unlabeled training set). We use an alternating optimization scheme, minimizing <span class="math inline">\(J_{\emptyset}\)</span> with respect to <span class="math inline">\(\theta\)</span> and maximizing with respect to <span class="math inline">\(\phi\)</span>. We also include a diversity term in the objective to ensure that the original data point and augmented data point aren't too similar. Since the TFs can be non-differentiable and/or non-deterministic, we cannot backpropagate through all of the parameters of <span class="math inline">\(G_{\theta}\)</span> as normal and instead use a recurrent policy gradient.</p>

<p>We evaluated two model classes for <span class="math inline">\(G_{\theta}\)</span>:</p>

<ul>
	<li><strong>Mean field</strong>: each sequential TF is chosen independently, reducing the task to learning the <span class="math inline">\(K\)</span> sampling frequencies of the TFs</li>
	<li><strong>Long short-term memory network (LSTM)</strong>: the input to each cell is a one-hot vector of the previously sampled TF, and the output from each cell of the network is a sampling distribution for the next TF. Making state-based decisions is critical when TFs are lossy when applied together, or are non-commutative.</li>
</ul>

<p>The training procedure for <span class="math inline">\(G_{\theta}\)</span> is completely decoupled from the end task. As a result, we can use <span class="math inline">\(G_{\theta}\)</span> to augment data for use with <em>any</em> end classifier <span class="math inline">\(D^{f}\)</span>.

<h1 id="our-results-with-tanda">Our results with TANDA</h1>

<p>Our experiments with TANDA thus far have been focused on pragmatics. Does learning an augmentation model produce better end classifier results than heuristic data augmentation approaches? To tackle this question, we evaluated on <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a>, <a href="yann.lecun.com/exdb/mnist/">MNIST</a>, and a subset of <a href="http://marathon.csee.usf.edu/Mammography/Database.html">DDSM</a> with mass segmentations. Here are the TFs we used for each of these tasks:</p>

<table style="width:100%">
  <tr>
    <th>CIFAR-10</th>
    <th>MNIST</th> 
    <th>DDSM</th>
  </tr>
  <tr>
    <td>Rotation (&plusmn;2.5&deg;, &plusmn;5&deg;)</td>
    <td>Rotation (&plusmn;2.5&deg;, &plusmn;5&deg;)</td>
    <td>Rotation (&plusmn;2.5&deg;, &plusmn;5&deg;)</td>
  </tr>
  <tr>
    <td>Shear (&plusmn;0.1 rad., &plusmn;0.25 rad.)</td>
    <td>Shear (&plusmn;0.1 rad., &plusmn;0.25 rad.)</td>
    <td>Contrast enhance (1.05, 1.15)</td>
  </tr>
  <tr>
  	<td>Swirl (&plusmn;0.1, &plusmn;0.25)</td>
  	<td>Swirl (&plusmn;0.1, &plusmn;0.25)</td>
  	<td>Mammographic brightness enhance (1.25, 1.5)</td>
  </tr>
  <tr>
  	<td>Elastic deformation (&alpha;=1.0,1.25,1.5)</td>
  	<td>Elastic deformation (&alpha;=1.0,1.25,1.5)</td>
  	<td>Translate segmented structure</td>
  </tr>
  <tr>
  	<td>Hue shift (&plusmn;0.1, &plusmn;0.25)</td>
  	<td>Erode (1 pixel)</td>
  	<td></td>
  </tr>
  <tr>
  	<td>Saturation enhance (0.5, 0.75, 1.25, 1.5)</td>
  	<td>Dilate (1 pixel)</td>
  	<td></td>
  </tr>
  <tr>
  	<td>Contrast enhance (0.5, 0.75, 1.25, 1.5)</td>
  	<td></td>
  	<td></td>
  </tr>
  <tr>
  	<td>Brightness enhance (0.5, 0.75, 1.25, 1.5)</td>
  	<td></td>
  	<td></td>
  </tr>
  <tr>
  	<td>Horizontal flip</td>
  	<td></td>
  	<td></td>
  </tr>
</table>

<p>The final two TFs for DDSM are highly domain-specific, developed by radiological experts. The brightness enhancer only shifts brightness levels to those attainable by the imaging process, and the structure translator moves segmented masses around the background tissue, filling in gaps using Poisson blending. Due to the intricacy of these TFs, many random augmentation sequences resulted in non-realistic images, punctuating the need for a learned augmentation model.</p>

<p>We also ventured outside of the imaging domain into natural language processing, where data augmentation recieves less attention. We augmented sentences in the <a href="https://www.ldc.upenn.edu/collaborations/past-projects/ace">ACE corpus</a> for a relation classification task. The TFs were based on swapping out words via sampling replacements from a trigram language model, specifying parts-of-speech and/or position with relation to the entities. For example, one TF swapped verbs in between the entity mentions.</p>

<p>Table 1 contains our primary results. We used off-the-shelf models as our end classifiers in order to focus on relative gains from learning composition models. We used a standard 56-layer ResNet for CIFAR-10, and much simpler convolutional neural networks for MNIST and DDSM. For ACE, we used a bidirectional long short-term memory network with word-level attention. Table 2 compares our performance to two GAN-based methods which augment training sets by generating out-of-class points.</p>

<p>We also investigated the robustness of our method to buggy or poorly specified TFs by intentionally including some in the MNIST pipeline. The frequencies for these TFs learned by the mean field model as training progresses are shown in Figure 4a. Finally, we validated our claim that longer TF sequences lead to better results (especially when we learn an augmentation model) in Figure 4b.</p>

<div class="figure">
<img src="tanda_figs/table12.png" alt="Tables 1 and 2"/>
</div>

<div class="figure">
<img src="tanda_figs/fig4.png" alt="Figure 4"/>
</div>

<h1 id="tanda">The TANDA library</h1>

<p>Does it sound like learning data augmentation models could help your machine learning pipeline? Get started with the TensorFlow-based <a href="https://github.com/HazyResearch/tanda">TANDA library on GitHub</a>, and let us know what you think!</p>

<h1 id="tanda">What's next for data augmentation?</h1>

<p>Learning a model for injecting weak supervision via data augmentation opens up tons of doors for future research. Here are a few directions that we're particularly excited about:</p>

<ul>
<li><p><em>Theoretical guarantees for data augmentation</em>:</p></li>
<li><p><em>Exploiting data structure directly</em>:</p></li>
</ul>



<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://snorkel-stanford-edu.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
                            
</body>
</html>
