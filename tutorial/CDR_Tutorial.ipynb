{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial, Part I: Candidate Extraction\n",
    "\n",
    "In this example, we'll be writing an application to extract **chemical-induced-disease relationships** from Pubmed abstracts, as per the [BioCreative CDR Challenge](http://www.biocreative.org/resources/corpora/biocreative-v-cdr-corpus/).  At core, we will be constructing a model to classify _candidate chemical-disease (C-D) relation mentions_ as either true or false.  To do this, we first need a set of such candidates- in this notebook, we'll use `DDLite` utilities to extract these candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Corpus\n",
    "\n",
    "First, we will load and pre-process the corpus, storing it for convenience in a `Corpus` object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring a document parser\n",
    "\n",
    "We'll start by defining a `DocParser` class to read in Pubmed abstracts from [Pubtator]([Pubtator](http://www.ncbi.nlm.nih.gov/CBBresearch/Lu/Demo/PubTator/index.cgi)), where they are stored along with \"gold\" (i.e. hand-annotated by experts) *chemical* and *disease mention* annotations. We'll use the `XMLDocParser` class, which allows us to use [XPath queries](https://en.wikipedia.org/wiki/XPath) to specify the relevant sections of the XML format.\n",
    "\n",
    "_Note that we are newline-concatenating text from the title and abstract together for simplicity, but if we wanted to, we could easily extend the `DocParser` classes to preserve information about document structure._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ddlite_parser import XMLDocParser\n",
    "xml_parser = XMLDocParser(\n",
    "    path='data/CDR_DevelopmentSet.xml',\n",
    "    doc='.//document',\n",
    "    text='.//passage/text/text()',\n",
    "    id='.//id/text()',\n",
    "    keep_xml_tree=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a sentence parser\n",
    "\n",
    "Next, we'll use an NLP preprocessing tool to split the `Document` objects into sentences, tokens, and provide annotations--part-of-speech tags, dependency parse structure, lemmatized word forms, etc.--for these sentences.  Here we use the default `SentenceParser` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ddlite_parser import SentenceParser\n",
    "sent_parser = SentenceParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing & loading the corpus\n",
    "\n",
    "Finally, we'll put this all together using a `Corpus` object, which will execute the parsers and store the results as an iterator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing documents...\n",
      "Parsing sentences...\n",
      "CPU times: user 4.15 s, sys: 122 ms, total: 4.27 s\n",
      "Wall time: 33.2 s\n"
     ]
    }
   ],
   "source": [
    "from ddlite_parser import Corpus\n",
    "%time corpus = Corpus(xml_parser, sent_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document(id='1445986', file='CDR_DevelopmentSet.xml', text=\"Cefotetan-induced immune hemolytic anemia.\\nImmune hemolytic anemia due to a drug-adsorption mechanism has been described primarily in patients receiving penicillins and first-generation cephalosporins. We describe a patient who developed anemia while receiving intravenous cefotetan. Cefotetan-dependent antibodies were detected in the patient's serum and in an eluate prepared from his red blood cells. The eluate also reacted weakly with red blood cells in the absence of cefotetan, suggesting the concomitant formation of warm-reactive autoantibodies. These observations, in conjunction with clinical and laboratory evidence of extravascular hemolysis, are consistent with drug-induced hemolytic anemia, possibly involving both drug-adsorption and autoantibody formation mechanisms. This case emphasizes the need for increased awareness of hemolytic reactions to all cephalosporins.\", attribs={'root': <Element document at 0x11210dcd0>})\n"
     ]
    }
   ],
   "source": [
    "doc  = corpus.get_docs()[2]\n",
    "print doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence(id='1445986-0', words=[u'Cefotetan-induced', u'immune', u'hemolytic', u'anemia', u'.'], lemmas=[u'cefotetan-induced', u'immune', u'hemolytic', u'anemia', u'.'], poses=[u'JJ', u'JJ', u'JJ', u'NN', u'.'], dep_parents=[4, 4, 4, 0, 4], dep_labels=[u'amod', u'amod', u'amod', u'ROOT', u'punct'], sent_id=0, doc_id='1445986', text=u'Cefotetan-induced immune hemolytic anemia.', char_offsets=[0, 18, 25, 35, 41], doc_name='CDR_DevelopmentSet.xml')\n"
     ]
    }
   ],
   "source": [
    "sent = corpus.get_sentences_in(doc.id)[0]\n",
    "print sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a basic candidate extractor\n",
    "\n",
    "Next, we'll write a basic function to extract **candidate disease mentions** from the corpus.  For this first attempt, we'll just write a function that checks for matches against a list (or _\"dictionary\"_) of disease phrases, constructed using some pre-compiled ontologies ([UMLS](https://www.nlm.nih.gov/research/umls/), [ORDO](http://www.orphadata.org/cgi-bin/inc/ordo_orphanet.inc.php), [DOID](http://www.obofoundry.org/ontology/doid.html), [NCBI Diseases](http://www.ncbi.nlm.nih.gov/CBBresearch/Dogan/DISEASE/); see `tutorial/data/diseases.py`).\n",
    "\n",
    "We'll do this using a `CandidateSpace` object--which defines the basic candidates we consider, in this case n-grams up to a certain length--and a `Matcher` object, which filters this candidate space down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 507899 disease phrases!\n"
     ]
    }
   ],
   "source": [
    "from load_dictionaries import load_disease_dictionary\n",
    "from ddlite_candidates import Ngrams\n",
    "from ddlite_matchers import DictionaryMatch\n",
    "\n",
    "# Load the disease phrase dictionary\n",
    "diseases = load_disease_dictionary()\n",
    "print \"Loaded %s disease phrases!\" % len(diseases)\n",
    "\n",
    "# Define a candidate space\n",
    "ngrams = Ngrams(n_max=3)\n",
    "\n",
    "# Define a matcher\n",
    "matcher = DictionaryMatch(d=diseases, longest_match_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we set `longest_match_only=False`, which means that we _will_ consider subsequences of phrases that match our dictionary.\n",
    "\n",
    "The `Ngrams` operator is applied over our `Sentence` objects and returns `Ngram` objects, and the `Matcher` then filters these, so we apply our operators over the sentences in the corpus, storing the results in a `Candidates` object for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting candidates...\n",
      "CPU times: user 5.37 s, sys: 122 ms, total: 5.49 s\n",
      "Wall time: 5.41 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Ngram(\"depression\", id=6794356-5:662-671, chars=[662,671], words=[4,4]),\n",
       " <Ngram(\"fulminant hepatitis\", id=3411101-5:504-522, chars=[504,522], words=[8,9]),\n",
       " <Ngram(\"examined\", id=16132524-4:674-681, chars=[674,681], words=[29,29]),\n",
       " <Ngram(\"hypertension\", id=16820346-4:652-663, chars=[652,663], words=[3,3]),\n",
       " <Ngram(\"artery disease\", id=15531665-5:829-842, chars=[829,842], words=[16,17])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ddlite_candidates import Candidates\n",
    "%time c = Candidates(ngrams, matcher, corpus.get_sentences())\n",
    "c.get_candidates()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our candidate recall on gold annotations\n",
    "\n",
    "Next, we'll test our _candidate recall_--in other words, how many of the true disease mentions we picked up in our candidate set--using the gold annotations in our dataset.\n",
    "\n",
    "The XML documents that we loaded using the `XMLDocParser` also contained annotations (this is why we kept the full xml tree using `keep_xml_tree=True`).  We'll load these annotations and map them to `Ngram` objects over our parsed sentences, that way we can easily compare our extracted candidate set with the gold annotations.  The code is fairly simple (see `tutorial/util.py`); note that we filter to only keep _disease_ annotations, and that the candidates should be uniquely identified by their `id` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import collect_pubtator_annotations\n",
    "gold = []\n",
    "for doc, sents in corpus:\n",
    "    gold += [a for a in collect_pubtator_annotations(doc, sents) if a.metadata['type'] == 'Disease']\n",
    "gold = frozenset(gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a set of gold annotations of the same type as our candidates (`Ngram`), and can use set operations (where candidate objects are hashed by their `id` attribute), e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3039"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gold.intersection(c.get_candidates()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we'll use a basic helper method of the `Candidates` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of gold annotations\t= 4244\n",
      "# of candidates\t\t= 6944\n",
      "Candidate recall\t= 0.716\n",
      "Candidate precision\t= 0.438\n"
     ]
    }
   ],
   "source": [
    "c.gold_stats(gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that our focus in this stage is on **acheiving high candidate recall, without considering an impractically large candidate set**.  Our main focus after this stage will be on training a classifier to select which candidates are true; this will raise precision while hopefully keeping recall high.  _Note however that candidate recall is an upper bound for the recall of this classifier!_\n",
    "\n",
    "So, we have some work to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the `Viewer` to inspect data\n",
    "\n",
    "Next, we'll use the `Viewer` class--here, specifically, the `SentenceNgramViewer`--to inspect the data.\n",
    "\n",
    "To start, we'll assemble a random set of all the sentences where there are gold annotations _not in our candidate set_, i.e. where we missed something, and then inspect these in the `Viewer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from random import shuffle\n",
    "\n",
    "# Index the gold annotations by sentence id\n",
    "gold_by_sid = defaultdict(list)\n",
    "for g in gold:\n",
    "    gold_by_sid[g.sent_id].append(g)\n",
    "\n",
    "# Get sentences\n",
    "view_sents = [s for s in corpus.get_sentences() if len(c.get_candidates_in(s.id)) < len(gold_by_sid[s.id])]\n",
    "shuffle(view_sents)\n",
    "view_sents = view_sents[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we instantiate and render the `Viewer` object; note we're being a bit sloppy, passing in _all_ the candidates and gold labels, but the `Viewer` object will take care of indexing them by sentence, and will only render the sentences we pass in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", module=\"widgets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "require.undef('viewer');\n",
       "\n",
       "define('viewer', [\"jupyter-js-widgets\"], function(widgets) {\n",
       "    var ViewerView = widgets.DOMWidgetView.extend({\n",
       "        render: function() {\n",
       "            this.nPages = this.model.get('n_pages');\n",
       "            this.pid = 0;\n",
       "            this.cid = 0;\n",
       "\n",
       "            // Insert the html payload\n",
       "            this.$el.append(this.model.get('html'));\n",
       "\n",
       "            // Enable button functionality for navigation\n",
       "            var that = this;\n",
       "            this.$el.find(\"#next-cand\").click(function() {\n",
       "                that.switchCandidate(1);\n",
       "            });\n",
       "            this.$el.find(\"#prev-cand\").click(function() {\n",
       "                that.switchCandidate(-1);\n",
       "            });\n",
       "            this.$el.find(\"#next-page\").click(function() {\n",
       "                that.switchPage(1);\n",
       "            });\n",
       "            this.$el.find(\"#prev-page\").click(function() {\n",
       "                that.switchPage(-1);\n",
       "            });\n",
       "\n",
       "            // Show the first page and highlight the first candidate\n",
       "            // NOTE: all elements should be selected using this.$el.find to avoid collisions with other Viewers\n",
       "            this.$el.find(\"#viewer-page-0\").show();\n",
       "            this.switchCandidate(0);\n",
       "\n",
       "            // Enable tooltips\n",
       "            // TODO: Fix this?\n",
       "            //$(function () {\n",
       "            //      this.$el.find('[data-toggle=\"tooltip\"]').tooltip()\n",
       "            //})\n",
       "        },\n",
       "\n",
       "        // Cycle through candidates and highlight, by increment inc\n",
       "        switchCandidate: function(inc) {\n",
       "            var nC = parseInt(this.$el.find(\"#viewer-page-\"+this.pid).attr(\"data-nc\"));\n",
       "\n",
       "            // Clear highlighting and highlight new candidate\n",
       "            this.$el.find(\"span.candidate\").removeClass(\"highlighted-candidate\");\n",
       "            if (this.cid + inc < 0) {\n",
       "                this.cid = nC + (this.cid + inc);\n",
       "            } else if (this.cid + inc > nC - 1) {\n",
       "                this.cid = (this.cid + inc) - nC;\n",
       "            } else {\n",
       "                this.cid += inc;\n",
       "            }\n",
       "            this.$el.find(\"span.c-\"+this.pid+\"-\"+this.cid).addClass(\"highlighted-candidate\");\n",
       "\n",
       "            // Fill in caption\n",
       "            // TODO: Redo this with widget data?\n",
       "            this.$el.find(\"#candidate-caption\").html(this.$el.find(\"#cdata-\"+this.pid+\"-\"+this.cid).attr(\"caption\"));\n",
       "        },\n",
       "\n",
       "        // Switch through pages\n",
       "        switchPage: function(inc) {\n",
       "            this.$el.find(\".viewer-page\").hide();\n",
       "            if (this.pid + inc < 0) {\n",
       "                this.pid = 0;\n",
       "            } else if (this.pid + inc > this.nPages - 1) {\n",
       "                this.pid = this.nPages - 1;\n",
       "            } else {\n",
       "                this.pid += inc;\n",
       "            }\n",
       "            this.$el.find(\"#viewer-page-\"+this.pid).show();\n",
       "\n",
       "            // Show pagination\n",
       "            this.$el.find(\"#page\").html(this.pid);\n",
       "\n",
       "            // Reset cid and set to first candidate\n",
       "            this.cid = 0;\n",
       "            this.switchCandidate(0);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    return {\n",
       "        ViewerView: ViewerView\n",
       "    };\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ddlite_viewer import SentenceNgramViewer\n",
    "sv = SentenceNgramViewer(view_sents, c.get_candidates(), gold=gold)\n",
    "sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building new Viewer with Widgets\n",
    "\n",
    "### TO-DO:\n",
    "1. Get basic stuff working as Widget subclass\n",
    "2. Communicate id of selected candidate back to python in notebook!\n",
    "3. Basic +/- labeling\n",
    "4. CLEANUP & PACKAGE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composing a better candidate extractor\n",
    "\n",
    "Let's try to increase our candidate recall using more of the `Matcher` operators and their functionalities.  First, let's turn on **Porter stemming** in our dictionary matcher; Porter stemming is an aggressive rules-based method for normalizing word endings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a new matcher\n",
    "matcher = DictionaryMatch(d=diseases, longest_match_only=False, stemmer='porter')\n",
    "\n",
    "# Extract a new set of candidates\n",
    "%time c = Candidates(ngrams, matcher, corpus.get_sentences())\n",
    "c.gold_stats(gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, note that *`Matcher` objects are compositional*. Observing in the `Viewer` that we are missing all of the acronyms, let's start with the `Union` operator, to integrate a dictionary for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ddlite_matchers import Union\n",
    "from load_dictionaries import load_acronym_dictionary\n",
    "\n",
    "# Load the disease phrase dictionary\n",
    "acronyms = load_acronym_dictionary()\n",
    "print \"Loaded %s acronyms!\" % len(acronyms)\n",
    "\n",
    "# Define a new matcher\n",
    "matcher = Union(\n",
    "    DictionaryMatch(d=diseases, longest_match_only=False, stemmer='porter'),\n",
    "    DictionaryMatch(d=acronyms, ignore_case=False))\n",
    "\n",
    "# Extract a new set of candidates\n",
    "%time c = Candidates(ngrams, matcher, corpus.get_sentences())\n",
    "c.gold_stats(gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we try using the `Concat` and `RegexMatch` operators to find candidate mentions composed of an _adjective followed by a term matching our diseases dictionary_.  Note in particular that we set `left_required=False` so that exact matches to our dictionary (with no adjective prepended) will still work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ddlite_matchers import Concat, RegexMatchEach\n",
    "matcher = Union(\n",
    "    Concat(\n",
    "        RegexMatchEach(rgx=r'JJ*', attrib='poses'),\n",
    "        DictionaryMatch(d=diseases, longest_match_only=False, stemmer='porter'),\n",
    "        left_required=False),\n",
    "    DictionaryMatch(d=acronyms, ignore_case=False))\n",
    "\n",
    "# Extract a new set of candidates\n",
    "%time c = Candidates(ngrams, matcher, corpus.get_sentences())\n",
    "c.gold_stats(gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running candidate extraction in parallel\n",
    "\n",
    "Note that **the candidate extraction procedure can be parallelized across multiple cores using the `parallelism=N`** optional argument to the `Candidates` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More coming here...\n",
    "\n",
    "We've increased the candidate recall (on the development set) by 7.3% using some simple compositional `Matcher` operators.  We'll be adding more here soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting with the rest of the `DDLite` workflow\n",
    "\n",
    "We are in the process of a big code refactor!  For now, to connect the candidates derived as in above to the rest of the `DDLite` workflow, create a `CandidateExtractor` class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ddlite_matchers import CandidateExtractor\n",
    "ce = CandidateExtractor(ngrams, matcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This object can now be used in place of the candidate extractors in the other tutorials!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "widgets": {
   "state": {
    "013bc95536b64bf385dfaed47eb38d11": {
     "views": []
    },
    "02eab3e5ba8642ab9bd33580fa1d8b42": {
     "views": []
    },
    "0a7fe39c623a4788a9c6a1b3914e97b6": {
     "views": []
    },
    "0b705f8e91804045886e5a6f2bd68542": {
     "views": []
    },
    "0e2e0ab16d834317b58e9a73d6c3c38e": {
     "views": []
    },
    "11ddee36d6df45dcb3dd65aef9df84ff": {
     "views": []
    },
    "15542abd5f364db095fe1df93a76d296": {
     "views": []
    },
    "1ccae7c2bdd5420e84790854c1aedbed": {
     "views": []
    },
    "1e8095dec3a8428c99bde075b18da2a3": {
     "views": []
    },
    "203b1764982944e897d0ea43c32222e9": {
     "views": []
    },
    "224f8577bdf9453da64ca07ef1b8e114": {
     "views": []
    },
    "31109a120a2944cda1c8f341bc29af61": {
     "views": []
    },
    "34913287cdda400b99b78db9435a6323": {
     "views": []
    },
    "394798deb5ea4e4eab61e52804f3b266": {
     "views": []
    },
    "3b55951a2cea4546a5ce611a4b95f5c8": {
     "views": []
    },
    "3bff15c01b074b6d8212000e9fc49f8f": {
     "views": []
    },
    "4474a51c5f8a4305b6aa4660ed4b9f04": {
     "views": []
    },
    "44e577ed01bd45b49cfd9119de18f1a8": {
     "views": []
    },
    "4521c076b1d34aedac77030428d81dde": {
     "views": []
    },
    "473307a6de604ac2a071e595d3e4b6b6": {
     "views": []
    },
    "5c359c36cbbc49d7a07064d9abd6e8c2": {
     "views": []
    },
    "6a06d6ea339140f484f3f58ea3736cd9": {
     "views": []
    },
    "6e46459bb8874547bfc3b6a76612f17e": {
     "views": []
    },
    "74e180da421545318f79b5f451e980ad": {
     "views": []
    },
    "7875f16e837c4621aadde89d4fdda5be": {
     "views": []
    },
    "78d1d44208444b13949679cb5d2873ca": {
     "views": []
    },
    "7a805225769b416b8378319e70c15b27": {
     "views": []
    },
    "7affe794a5284c8c8ef1232da4e2e1a9": {
     "views": []
    },
    "823d0bcd024d4b73bf1af05ebc035951": {
     "views": []
    },
    "82d151e87bcf44989e1c4f6bcae50de3": {
     "views": []
    },
    "835f7ce1303e4cc680d83b1d4b546674": {
     "views": []
    },
    "8b7c8ce068ab4292bb59e71d4772dc88": {
     "views": []
    },
    "8c21c476cf184217afe5be2b8d390bfc": {
     "views": []
    },
    "8e9cfee928cf48eb8a6da8df48007fb2": {
     "views": []
    },
    "97e600d1a3dc4fce804f183db2e46b0c": {
     "views": []
    },
    "9d482c3b534f46f7bb51ada67c3e9577": {
     "views": []
    },
    "9dc5d0d9b2634e139f13aa5a743f74e8": {
     "views": []
    },
    "a98cd34777224d7f9bce3dc517d336df": {
     "views": []
    },
    "ab4413cd59294a35addb68653f4a3a76": {
     "views": [
      {
       "cell_index": 25
      }
     ]
    },
    "af2e6ba4b90e4902b493e36ec3a88e0a": {
     "views": []
    },
    "b172b7a70d5b47f0b61f2b9dac6731ee": {
     "views": []
    },
    "b5feab5cd9494343b50b77de8da2f7fc": {
     "views": []
    },
    "bbe111bf2e75409eb59a89c92af5f021": {
     "views": []
    },
    "d5ad0d84c30547adb403ebf873ead9ee": {
     "views": []
    },
    "d973943ba76244b489019c7a4d19c2f4": {
     "views": []
    },
    "e357d2075ef041eaae5a0a10e655b1d6": {
     "views": []
    },
    "ec25a5a6d06b4410927672e6801aaad8": {
     "views": []
    },
    "f04d7dec21804bcd9d89cab5dd5ad78b": {
     "views": []
    },
    "f6ab0ba4df96494e95d51c56ad8e0ce8": {
     "views": []
    },
    "f74f5d1d592c45fabdee9102c3cfb149": {
     "views": []
    }
   },
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
