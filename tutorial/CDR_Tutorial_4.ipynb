{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disease Tagging Tutorial\n",
    "\n",
    "In this example, we'll be writing an application to extract *mentions of* diseases from Pubmed abstracts, using annotations from the [BioCreative CDR Challenge](http://www.biocreative.org/resources/corpora/biocreative-v-cdr-corpus/).  This tutorial, which has 5 parts, walks through the process of constructing a model to classify _candidate_ disease mentions as either true (i.e., that it is truly a mention of a disease) or false."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV: Training a Model with Data Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the tutorial, we will train a statistical model to differentiate between true and false `Disease` mentions.\n",
    "\n",
    "We will train this model using _data programming_, and we will **ignore** the training labels provided with the training data. This is a more realistic scenario; in the wild, hand-labeled training data is rare and expensive. Data programming enables us to train a model using only a modest amount of hand-labeled data for validation and testing. For more information on data programming, see the [NIPS 2016 paper](https://arxiv.org/abs/1605.07723)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "# Note: We run automated tests on this tutorial to make sure that it is always up to date! \n",
    "# However, certain interactive components cannot currently be tested automatically, and will \n",
    "# be skipped with if-then statements using the variable below\n",
    "AUTOMATED_TESTING = os.environ.get('TESTING') is not None\n",
    "\n",
    "import numpy as np\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat our definition of the `Disease` `Candidate` subclass from Parts II and III."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "ChemicalDisease = candidate_subclass('Disease', ['disease'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading `CandidateSet` objects\n",
    "\n",
    "We reload the training and development `CandidateSet` objects from the previous parts of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.models import CandidateSet\n",
    "\n",
    "train = session.query(CandidateSet).filter(CandidateSet.name == 'CDR Training Candidates').one()\n",
    "dev = session.query(CandidateSet).filter(CandidateSet.name == 'CDR Development Candidates').one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically Creating Features\n",
    "Recall that our goal is to distinguish between true and false mentions of chemical-disease relations. To train a model for this task, we first embed our `ChemicalDisease` candidates in a feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import FeatureManager\n",
    "\n",
    "feature_manager = FeatureManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a new feature set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time F_train = feature_manager.create(session, train, 'Train Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OR** if we've already created one, we can simply load as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time F_train = feature_manager.load(session, train, 'Train Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the returned matrix is a special subclass of the `scipy.sparse.csr_matrix` class, with some special features which we demonstrate below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "F_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "F_train.get_candidate(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "F_train.get_key(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Labeling Functions\n",
    "Labeling functions are a core tool of data programming. They are heuristic functions that aim to classify candidates correctly. Their outputs will be automatically combined and denoised to estimate the probabilities of training labels for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_ROOT = os.environ['SNORKELHOME'] + '/tutorial/data/'\n",
    "\n",
    "CTD_lower = set()\n",
    "with open(DATA_ROOT + 'dicts/CTD_chemicals_diseases.tsv', 'rb') as f:\n",
    "    for line in f:\n",
    "        if not line.startswith(\"#\"):\n",
    "            chem, chem_id, _, disease, disease_id, evidence, _, _, _, _ = line.split('\\t')\n",
    "            CTD_lower.add((chem.lower(), disease.lower()))\n",
    "print len(CTD_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import random\n",
    "import re\n",
    "from snorkel.lf_helpers import get_text_between\n",
    "from snorkel.models import split_stable_id\n",
    "from snorkel.lf_helpers import get_tagged_text\n",
    "\n",
    "def LF_in_CDT(c, p_neg=0.2):\n",
    "    \"\"\"Match against the CDT KB, with random negative supervision as well\"\"\"\n",
    "    chem    = c.chemical.get_span().lower()\n",
    "    disease = c.disease.get_span().lower()\n",
    "    if (chem, disease) in CTD_lower:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1 if random() < p_neg else 0\n",
    "\n",
    "WEAK_PHRASES = ['none', 'although', 'was carried out', 'was conducted',\n",
    "                'seems', 'suggests', 'risk', 'associated with', 'implicated',\n",
    "               'the aim', 'to (investigate|assess|study)']\n",
    "\n",
    "WEAK_RGX = r'|'.join(WEAK_PHRASES)\n",
    "\n",
    "def LF_weak_assertions(c):\n",
    "    return -1 if re.search(WEAK_RGX, get_tagged_text(c), flags=re.I) else 0\n",
    "\n",
    "ANIMAL_RGX = r'mouse|mice'\n",
    "\n",
    "def LF_animal(c):\n",
    "    return -1 if re.search(ANIMAL_RGX, get_tagged_text(c), flags=re.I) else 0\n",
    "\n",
    "def LF_cause(c):\n",
    "    return 1 if re.search(r'cause', get_text_between(c), flags=re.I) else 0\n",
    "\n",
    "def LF_in_CDT_causes(c):\n",
    "    \"\"\"Match against the CDT KB, with random negative supervision as well\"\"\"\n",
    "    chem    = c.chemical.get_span().lower()\n",
    "    disease = c.disease.get_span().lower()\n",
    "    if (chem, disease) in CTD_lower:\n",
    "        if re.search(r'causes', get_text_between(c), flags=re.I):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def LF_in_CDT_induced(c):\n",
    "    chem    = c.chemical.get_span().lower()\n",
    "    disease = c.disease.get_span().lower()\n",
    "    if (chem, disease) in CTD_lower and re.search(r'{{A}}.{0,20}induce.{0,20}{{B}}', get_tagged_text(c), flags=re.I):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def LF_in_CDT_exposure(c):\n",
    "    chem    = c.chemical.get_span().lower()\n",
    "    disease = c.disease.get_span().lower()\n",
    "    if (chem, disease) in CTD_lower and re.search(r'{{B}}.{0,20}expos.{0,20}{{A}}', get_tagged_text(c), flags=re.I):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def LF_in_CDT_caused_by(c):\n",
    "    chem    = c.chemical.get_span().lower()\n",
    "    disease = c.disease.get_span().lower()\n",
    "    if (chem, disease) in CTD_lower and re.search(r'{{B}}.{0,20}caused by.{0,20}{{A}}', get_tagged_text(c), flags=re.I):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def LF_in_CDT_caused_by_2(c):\n",
    "    chem    = c.chemical.get_span().lower()\n",
    "    disease = c.disease.get_span().lower()\n",
    "    if (chem, disease) in CTD_lower and re.search(r'{{B}}.*(caused|induced) by.*{{A}}', get_tagged_text(c), flags=re.I):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def LF_dash_induced(c):\n",
    "    return 1 if re.search(r'{{A}}-induced {{B}}', get_tagged_text(c), flags=re.I) else 0\n",
    "\n",
    "def LF_caused(c):\n",
    "    return 1 if re.search(r'{{A}}.{0,20}caused.{0,20}{{B}}', get_tagged_text(c), flags=re.I) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disease Tagging LFs Start Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document-Level Labeling Functions\n",
    "We start with some labeling functions that label candidates based on document-level features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence-Level Labeling Functions\n",
    "We also include some labeling functions that label candidates based on sentence-level features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mention-Level Labeling Functions\n",
    "We now define a number of labeling functions that label candidates based on attributes related to the mention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary Labeling Functions\n",
    "We can use existing dictionaries for distant supervision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative Labeling Functions\n",
    "When writing labeling functions, it is important to provide negative supervision in addition to positive supervision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We maintain a list of all LFs for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# document and sentence-level\n",
    "LFs_doc = [#LF_candidate_df,\n",
    "           LF_known_abbreviation,\n",
    "           LF_undefined_abbreviation\n",
    "#            LF_in_title\n",
    "]\n",
    "\n",
    "LFs_sent = [#LF_in_list,\n",
    "            LF_contiguous_mentions]\n",
    "\n",
    "# mention-level LFs\n",
    "LFs_true = [LF_tumors_growths,\n",
    "#             LF_dep_path,\n",
    "#             LF_acronym_def,\n",
    "#             LF_long_phrase,\n",
    "            LF_cancer,\n",
    "            LF_disease_syndrome,\n",
    "#             LF_deficiency,\n",
    "            LF_indicators,\n",
    "            LF_common_disease,\n",
    "            LF_common_disease_acronyms,\n",
    "            LF_deficiency_of,\n",
    "            LF_come_with,\n",
    "#             LF_contain_disease,\n",
    "#             LF_parentheses,\n",
    "#             LF_noun_phrases,\n",
    "#             LF_type_x,\n",
    "            LF_positive_indicator,\n",
    "            LF_left_positive_argument,\n",
    "#             LF_right_positive_argument,\n",
    "            LF_right_negative_argument,\n",
    "            LF_medical_afixes,\n",
    "#             LF_dropped_syndrome\n",
    "            LF_adj_diseases\n",
    "           ]\n",
    "\n",
    "LFs_dicts =  [#LF_DOID_human_disease_ontology,\n",
    "#              LF_cell_molecular_dysfunction,\n",
    "             LF_SNOWMED_CT_sign_or_symptom,\n",
    "             LF_SNOWMED_CT_disease_or_syndrome,\n",
    "             LF_MESH_disease_or_syndrome,\n",
    "             LF_MESH_sign_or_symptom\n",
    "            ]\n",
    "\n",
    "LFs_false = [#LF_dangling_phrase,\n",
    "             LF_chemical_name,\n",
    "             LF_organs,\n",
    "             LF_bodysym,\n",
    "             LF_protein_chemical_abbrv,\n",
    "             LF_has_punctuation,\n",
    "             LF_gene_abbrv,\n",
    "             LF_base_pair_seq, \n",
    "             LF_too_vague,\n",
    "#              LF_weakness,\n",
    "             LF_negation,\n",
    "#              LF_activity,\n",
    "#              LF_neg_prefix,\n",
    "             LF_neg_surfix,\n",
    "             LF_non_common_disease,\n",
    "             LF_non_disease_acronyms,\n",
    "#              LF_intensity_modifier,\n",
    "             LF_pos_in,\n",
    "             LF_gene_chromosome_link,\n",
    "             LF_right_window_incomplete,\n",
    "#              LF_left_negative_argument,\n",
    "#              LF_not_disease\n",
    "             LF_negative_indicator\n",
    "            ]\n",
    "\n",
    "LFs = LFs_true + LFs_false + LFs_dicts + LFs_doc + LFs_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Labeling Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we construct a `CandidateLabeler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import LabelManager\n",
    "\n",
    "label_manager = LabelManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we run the `CandidateLabeler` to to apply the labeling functions to the training `CandidateSet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time L_train = label_manager.create(session, train, 'LF Labels', f=LFs)\n",
    "L_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OR** load if we've already created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time L_train = label_manager.load(session, train, 'LF Labels')\n",
    "L_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now say we want to add a new labeling function to our matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LF_test_3(candidate):\n",
    "    return -1 if random.random() < 0.2 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add or rerun a single labeling function (or more!) with the below command. Note that we set the argument `expand_key_set` to `True` to indicate that the set of matrix columns should be allowed to expand. \n",
    "\n",
    "Do this to test changes to the labeling functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_train = label_manager.update(session, train, 'LF Labels', True, f=[LF_test_3])\n",
    "L_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view statistics about the resulting label matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_train.lf_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Generative Model\n",
    "We estimate the accuracies of the labeling functions without supervision. Specifically, we estimate the parameters of a `NaiveBayes` generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import NaiveBayes\n",
    "\n",
    "gen_model = NaiveBayes()\n",
    "gen_model.train(L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen_model.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen_model.save(session, 'Generative Params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen_model.load(session, 'Generative Params')\n",
    "gen_model.w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the generative model to the training candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_marginals = gen_model.marginals(L_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Discriminative Model\n",
    "We use the estimated probabilites to train a discriminative model that classifies each `Candidate` as a true or false mention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.learning import LogReg\n",
    "\n",
    "disc_model = LogReg()\n",
    "disc_model.train(F_train, train_marginals, n_iter=1500, rate=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disc_model.w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time disc_model.save(session, \"Discriminative Params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_prev = disc_model.w\n",
    "%time disc_model.load(session, \"Discriminative Params\")\n",
    "np.all(disc_model.w == w_prev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating on the Development `CandidateSet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create features for the development set.\n",
    "\n",
    "Note that we use the training features feature set, because those are the only features for which we have learned parameters. Features that were not encountered during training, e.g., a token that does not appear in the training set, are ignored, because we do not have any information about them.\n",
    "\n",
    "To do so with the `FeatureManager`, we call update with the new `CandidateSet`, the name of the training `AnnotationKeySet`, and the value `False` for the parameter `extend_key_set` to indicate that the `AnnotationKeySet` should not be expanded with new `Feature` keys encountered during processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time F_dev = feature_manager.update(session, dev, 'Train Features', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OR** if we've already created one, we can simply load as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time F_dev = feature_manager.load(session, dev, 'Train Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the development set labels and gold candidates we made in Part III."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L_dev = label_manager.load(session, dev, \"CDR Development Labels -- Gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gold_dev_set = session.query(CandidateSet).filter(CandidateSet.name == 'CDR Development Candidates -- Gold').one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate the discriminative model on the development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = disc_model.score(F_dev, L_dev, gold_dev_set, b=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Examples\n",
    "After evaluating on the development `CandidateSet`, the labeling functions can be modified. Try changing the labeling functions to improve performance. You can view the true positives, false positives, true negatives, and false negatives using the `Viewer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# NOTE: This if-then statement is only to avoid opening the viewer during automated testing of this notebook\n",
    "# You should ignore this!\n",
    "if not AUTOMATED_TESTING:\n",
    "    sv = SentenceNgramViewer(tp, session, annotator_name=\"Tutorial Part IV User\")\n",
    "else:\n",
    "    sv = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sv.get_selected()[0].parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sv.g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, in Part V, we will test our model on the test `CandidateSet`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {
    "b42aa7ec064e441babe2eb854a914541": {
     "views": [
      {
       "cell_index": 61
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
