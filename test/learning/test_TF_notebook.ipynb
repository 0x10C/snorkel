{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing `TFNoiseAwareModel` in Jupyter Notebook\n",
    "\n",
    "We'll start by testing the `textRNN` model on a categorical problem from `tutorials/crowdsourcing`.  In particular we'll test for (a) basic performance and (b) proper construction / re-construction of the TF computation graph both after (i) repeated notebook calls, and (ii) with `GridSearch` in particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ['SNORKELDB'] = 'sqlite:///{0}{1}crowdsourcing.db'.format(os.getcwd(), os.sep)\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load candidates and training marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "from snorkel.contrib.models.text import RawText\n",
    "Tweet = candidate_subclass('Tweet', ['tweet'], cardinality=5)\n",
    "train_tweets = session.query(Tweet).filter(Tweet.split == 0).order_by(Tweet.id).all()\n",
    "len(train_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.annotations import load_marginals\n",
    "train_marginals = load_marginals(session, train_tweets, split=0)\n",
    "train_marginals.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train basic LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[textRNN] Dimension=100  LR=0.01\n",
      "[textRNN] Begin preprocessing\n",
      "[textRNN] Preprocessing done (1.43s)\n",
      "[textRNN] Training model\n",
      "[textRNN] #examples=568  #epochs=50  batch size=256\n",
      "[textRNN] Epoch 0 (1.24s)\tAverage loss=1.543260\n",
      "[textRNN] Epoch 5 (3.00s)\tAverage loss=0.209871\n",
      "[textRNN] Epoch 10 (4.74s)\tAverage loss=0.045068\n",
      "[textRNN] Epoch 15 (6.50s)\tAverage loss=0.034457\n",
      "[textRNN] Epoch 20 (8.67s)\tAverage loss=0.041505\n",
      "[textRNN] Epoch 25 (10.53s)\tAverage loss=0.030682\n",
      "[textRNN] Epoch 30 (12.24s)\tAverage loss=0.029904\n",
      "[textRNN] Epoch 35 (14.00s)\tAverage loss=0.028201\n",
      "[textRNN] Epoch 40 (15.74s)\tAverage loss=0.026489\n",
      "[textRNN] Epoch 45 (17.58s)\tAverage loss=0.029394\n",
      "[textRNN] Epoch 49 (19.09s)\tAverage loss=0.025847\n",
      "[textRNN] Training done (19.09s)\n"
     ]
    }
   ],
   "source": [
    "from snorkel.contrib.rnn import TextRNN\n",
    "\n",
    "train_kwargs = {\n",
    "    'lr':         0.01,\n",
    "    'dim':        100,\n",
    "    'n_epochs':   50,\n",
    "    'dropout':    0.2,\n",
    "    'print_freq': 5\n",
    "}\n",
    "lstm = TextRNN(seed=1701, n_threads=None)\n",
    "lstm.train(train_tweets, train_marginals, **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6875\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "test_tweets = session.query(Tweet).filter(Tweet.split == 1).order_by(Tweet.id).all()\n",
    "test_labels = np.load('crowdsourcing_test_labels.npy')\n",
    "correct, incorrect = lstm.score(session, test_tweets, test_labels)\n",
    "acc = len(correct) / float(len(correct) + len(incorrect))\n",
    "assert acc > 0.60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run `GridSearch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
