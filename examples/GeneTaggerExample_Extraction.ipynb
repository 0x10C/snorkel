{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging genes with ddlite: candidate extraction\n",
    "\n",
    "## Introduction\n",
    "In this example **ddlite** app, we'll build a gene tagger from scratch. Domain-specific tagging systems take months or years to develop. They use hand-crafted model circuitry and accurate, hand-labeled training data. We'll start to build a pretty good one in a few minutes with none of those things. The generalized extraction and learning utilities provided by ddlite will allow us to turn a sampling of article abstracts and some basic domain knowledge into an automated tagging system. Specifically, we want an accurate tagger for genes in academic articles. We have comprehensive dictionaries of genes, but applying a simple matching rule might yield a lot of false positives. For example, \"p53\" might get tagged as a gene if it refers to a page number. Our goal is to use distant supervision to improve precision.\n",
    "\n",
    "Here's the pipeline we'll follow:\n",
    "\n",
    "1. Obtain and parse input data (relevant article abstracts from PubMed)\n",
    "2. Extract candidates for tagging\n",
    "3. Generate features\n",
    "4. Create a test set\n",
    "5. Write labeling functions\n",
    "6. Learn the tagging model\n",
    "7. Iterate on labeling functions\n",
    "\n",
    "Parts 1 and 2 are covered in this notebook, and parts 3 through 7 are covered in `GeneTaggerExample_Learning.ipynb`. Let's get to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import cPickle, os, sys\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "from sqlalchemy import desc\n",
    "\n",
    "from snorkel.snorkel import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connects to database backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the input data\n",
    "We already downloaded the raw HTML for 150 gene-related article pages from PubMed using the `pubmed_gene_html.py` script. These can be found in the `data` folder. We can use ddlite's `DocParser` to read in the article text. There's a general HTML parser which finds visible text, but we can do better by writing a more specific version to just grab the abstract text.\n",
    "\n",
    "We'll use CoreNLP via ddlite's `SentenceParser` to parse each sentence. `DocParser` can handle this too; we didn't really need that call above. This can take a little while, so if the example has already been run, we'll reload it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PubMedAbstractParser(HTMLDocParser):\n",
    "    def _cleaner(self, s):\n",
    "        return (s.parent.name == 'abstracttext')\n",
    "\n",
    "dp = PubMedAbstractParser('gene_tag_example/data/')\n",
    "cp = CorpusParser(dp, SentenceParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = cp.parse()\n",
    "corpus.id = 'Pubmed Abstracts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document('10101268', None, '10101268.html', {})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print len(corpus.documents)\n",
    "corpus.documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sentence('sent-10101268-0', None, 0, u'By constructing DNA probes we have identified and cloned a human PtdIns 4-kinase, PI4K230, corresponding to a mRNA of 7.0 kb.', [u'By', u'constructing', u'DNA', u'probes', u'we', u'have', u'identified', u'and', u'cloned', u'a', u'human', u'PtdIns', u'4-kinase', u',', u'PI4K230', u',', u'corresponding', u'to', u'a', u'mRNA', u'of', u'7.0', u'kb', u'.'], [u'by', u'construct', u'dna', u'probe', u'we', u'have', u'identify', u'and', u'clone', u'a', u'human', u'ptdins', u'4-kinase', u',', u'pi4k230', u',', u'correspond', u'to', u'a', u'mrna', u'of', u'7.0', u'kb', u'.'], [u'IN', u'VBG', u'NN', u'NNS', u'PRP', u'VBP', u'VBN', u'CC', u'VBN', u'DT', u'JJ', u'NN', u'NN', u',', u'NN', u',', u'VBG', u'TO', u'DT', u'NN', u'IN', u'CD', u'NN', u'.'], [2, 7, 4, 2, 7, 7, 0, 7, 7, 13, 13, 13, 9, 13, 13, 13, 13, 20, 20, 17, 23, 23, 20, 7], [u'mark', u'advcl', u'compound', u'dobj', u'nsubj', u'aux', u'ROOT', u'cc', u'conj', u'det', u'amod', u'compound', u'dobj', u'punct', u'appos', u'punct', u'acl', u'case', u'det', u'nmod', u'case', u'nummod', u'nmod', u'punct'], [0, 3, 16, 20, 27, 30, 35, 46, 50, 57, 59, 65, 72, 80, 82, 89, 91, 105, 108, 110, 115, 118, 122, 124])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = list(corpus.get_contexts())\n",
    "print len(sents)\n",
    "sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persists corpus in database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session.add(corpus)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reloads corpus from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = session.query(Corpus).get('Pubmed Abstracts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting candidates with matchers\n",
    "Extracting candidates for mentions (or relations) in ddlite is done with `Matcher` objects. First, we'll use a `DictionaryMatcher`. We have access to a pretty comprehensive gene dictionary. Let's load it in and create the `DictionaryMatcher`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.matchers import DictionaryMatch\n",
    "\n",
    "# Schema is: ENSEMBL_ID | NAME | TYPE (refseq, canonical, non-canonical)\n",
    "genes = [line.rstrip().split('\\t')[1] for line in open('gene_tag_example/dicts/ensembl_genes.tsv')]\n",
    "genes = filter(lambda g : len(g) > 2, genes)\n",
    "\n",
    "gene_dm = DictionaryMatch(d=genes, ignore_case=False, longest_match_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary match should provide fairly high recall, but we may still miss some candidates. We know that gene names are named nouns and are often all uppercase. Let's use DDLite's *compositional* matcher operations to handle this. First, we'll write a matcher to find all nouns using the parts-of-speech tags. Then, we'll use a filter to find uppercase sequences. Finally, we'll use a filter to make sure each match has at least 3 characters. We pass `noun_rm` to `up_rm`, and `up_rm` to the final filter to compose them with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.matchers import RegexMatchSpan, RegexMatchEach\n",
    "\n",
    "noun_regex  = RegexMatchEach(rgx=r'NN.*', attrib='poses', longest_match_only=True)\n",
    "up_regex    = RegexMatchSpan(noun_regex, rgx=r'[A-Z]+([0-9]+)?([A-Z]+)?([0-9]+)?$', ignore_case=False, attrib='words', longest_match_only=True)\n",
    "multi_regex = RegexMatchSpan(up_regex, rgx=r'[a-z0-9]{3,}', ignore_case=True, attrib='words', longest_match_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we put it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "from snorkel.matchers import Union\n",
    "\n",
    "ngrams = Ngrams(n_max=3)\n",
    "\n",
    "CE = CandidateExtractor(ngrams, Union(gene_dm, multi_regex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the candidates\n",
    "We'll use our unioned candidate extractor to extract our candidate entities from the sentences into a `Candidates` object. Using both matchers together will provide very high recall, but may have poor precision. In the next demo notebook (`GeneTaggerExample_Learning.ipynb`), we'll write distant supervision rules and learn a model to improve precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "candidates = CE.extract(corpus.get_contexts())\n",
    "candidates.id = 'Pubmed Candidates'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1349"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll persist the extracted candidates for use in `GeneTaggerExample_Learning.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session.add(candidates)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
